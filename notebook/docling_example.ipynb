{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docling example\n",
    "\n",
    "- https://ds4sd.github.io/docling/examples/minimal/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', message='Could not load the custom kernel for multi-scale deformable attention')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 함수 사용 예시\u001b[39;00m\n\u001b[1;32m     25\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/Hands-On Large Language Models Language.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 26\u001b[0m cleaned_text \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_pdf_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_text[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m100\u001b[39m])\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mconvert_pdf_to_text\u001b[0;34m(source_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert_pdf_to_text\u001b[39m(source_path):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# PDF를 텍스트로 변환\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     converter \u001b[38;5;241m=\u001b[39m DocumentConverter()\n\u001b[0;32m----> 7\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     text \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mdocument\u001b[38;5;241m.\u001b[39mexport_to_text()\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# 정규표현식을 사용하여 HTML 주석 제거\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/pydantic/_internal/_validate_call.py:38\u001b[0m, in \u001b[0;36mupdate_wrapper_attributes.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(wrapped)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper_function\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/pydantic/_internal/_validate_call.py:111\u001b[0m, in \u001b[0;36mValidateCallWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 111\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpydantic_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mArgsKwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__return_pydantic_validator__:\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__return_pydantic_validator__(res)\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/docling/document_converter.py:203\u001b[0m, in \u001b[0;36mDocumentConverter.convert\u001b[0;34m(self, source, headers, raises_on_error, max_num_pages, max_file_size, page_range)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;129m@validate_call\u001b[39m(config\u001b[38;5;241m=\u001b[39mConfigDict(strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m     page_range: PageRange \u001b[38;5;241m=\u001b[39m DEFAULT_PAGE_RANGE,\n\u001b[1;32m    194\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ConversionResult:\n\u001b[1;32m    195\u001b[0m     all_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_all(\n\u001b[1;32m    196\u001b[0m         source\u001b[38;5;241m=\u001b[39m[source],\n\u001b[1;32m    197\u001b[0m         raises_on_error\u001b[38;5;241m=\u001b[39mraises_on_error,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m         page_range\u001b[38;5;241m=\u001b[39mpage_range,\n\u001b[1;32m    202\u001b[0m     )\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_res\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/docling/document_converter.py:226\u001b[0m, in \u001b[0;36mDocumentConverter.convert_all\u001b[0;34m(self, source, headers, raises_on_error, max_num_pages, max_file_size, page_range)\u001b[0m\n\u001b[1;32m    223\u001b[0m conv_res_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert(conv_input, raises_on_error\u001b[38;5;241m=\u001b[39mraises_on_error)\n\u001b[1;32m    225\u001b[0m had_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconv_res\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconv_res_iter\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhad_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraises_on_error\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconv_res\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mConversionStatus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSUCCESS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mConversionStatus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPARTIAL_SUCCESS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/docling/document_converter.py:261\u001b[0m, in \u001b[0;36mDocumentConverter._convert\u001b[0;34m(self, conv_input, raises_on_error)\u001b[0m\n\u001b[1;32m    252\u001b[0m _log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoing to convert document batch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# parallel processing only within input_batch\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# with ThreadPoolExecutor(\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m#    max_workers=settings.perf.doc_batch_concurrency\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# ) as pool:\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m#   yield from pool.map(self.process_document, input_batch)\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Note: PDF backends are not thread-safe, thread pool usage was disabled.\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_document\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraises_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraises_on_error\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43melapsed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/docling/document_converter.py:302\u001b[0m, in \u001b[0;36mDocumentConverter._process_document\u001b[0;34m(self, in_doc, raises_on_error)\u001b[0m\n\u001b[1;32m    298\u001b[0m valid \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallowed_formats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m in_doc\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallowed_formats\n\u001b[1;32m    300\u001b[0m )\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 302\u001b[0m     conv_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraises_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraises_on_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not allowed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_doc\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/docling/document_converter.py:323\u001b[0m, in \u001b[0;36mDocumentConverter._execute_pipeline\u001b[0;34m(self, in_doc, raises_on_error)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_execute_pipeline\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m, in_doc: InputDocument, raises_on_error: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m    321\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ConversionResult:\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m in_doc\u001b[38;5;241m.\u001b[39mvalid:\n\u001b[0;32m--> 323\u001b[0m         pipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_doc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pipeline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m             conv_res \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mexecute(in_doc, raises_on_error\u001b[38;5;241m=\u001b[39mraises_on_error)\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/docling/document_converter.py:289\u001b[0m, in \u001b[0;36mDocumentConverter._get_pipeline\u001b[0;34m(self, doc_format)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# TODO this will ignore if different options have been defined for the same pipeline class.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    285\u001b[0m     pipeline_class \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized_pipelines\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized_pipelines[pipeline_class]\u001b[38;5;241m.\u001b[39mpipeline_options\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;241m!=\u001b[39m pipeline_options\n\u001b[1;32m    288\u001b[0m ):\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized_pipelines[pipeline_class] \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_options\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialized_pipelines[pipeline_class]\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/docling/pipeline/standard_pdf_pipeline.py:93\u001b[0m, in \u001b[0;36mStandardPdfPipeline.__init__\u001b[0;34m(self, pipeline_options)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (ocr_model \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_ocr_model(artifacts_path\u001b[38;5;241m=\u001b[39martifacts_path)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe specified OCR kind is not supported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_options\u001b[38;5;241m.\u001b[39mocr_options\u001b[38;5;241m.\u001b[39mkind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_pipe \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Pre-processing\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     PagePreprocessingModel(\n\u001b[1;32m     81\u001b[0m         options\u001b[38;5;241m=\u001b[39mPagePreprocessingOptions(\n\u001b[1;32m     82\u001b[0m             images_scale\u001b[38;5;241m=\u001b[39mpipeline_options\u001b[38;5;241m.\u001b[39mimages_scale\n\u001b[1;32m     83\u001b[0m         )\n\u001b[1;32m     84\u001b[0m     ),\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# OCR\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     ocr_model,\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Layout model\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     LayoutModel(\n\u001b[1;32m     89\u001b[0m         artifacts_path\u001b[38;5;241m=\u001b[39martifacts_path,\n\u001b[1;32m     90\u001b[0m         accelerator_options\u001b[38;5;241m=\u001b[39mpipeline_options\u001b[38;5;241m.\u001b[39maccelerator_options,\n\u001b[1;32m     91\u001b[0m     ),\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Table structure model\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     \u001b[43mTableStructureModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43menabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43martifacts_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifacts_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable_structure_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# Page assemble\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     PageAssembleModel(options\u001b[38;5;241m=\u001b[39mPageAssembleOptions()),\n\u001b[1;32m    101\u001b[0m ]\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Picture description model\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    105\u001b[0m     picture_description_model \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_picture_description_model(\n\u001b[1;32m    106\u001b[0m         artifacts_path\u001b[38;5;241m=\u001b[39martifacts_path\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m ) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/docling/models/table_structure_model.py:44\u001b[0m, in \u001b[0;36mTableStructureModel.__init__\u001b[0;34m(self, enabled, artifacts_path, options, accelerator_options)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artifacts_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         artifacts_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_path\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;66;03m# will become the default in the future\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (artifacts_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_repo_folder)\u001b[38;5;241m.\u001b[39mexists():\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/docling/models/table_structure_model.py:94\u001b[0m, in \u001b[0;36mTableStructureModel.download_models\u001b[0;34m(local_dir, force, progress)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m progress:\n\u001b[1;32m     93\u001b[0m     disable_progress_bars()\n\u001b[0;32m---> 94\u001b[0m download_path \u001b[38;5;241m=\u001b[39m \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mds4sd/docling-models\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mv2.1.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Path(download_path)\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/huggingface_hub/_snapshot_download.py:155\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# if we have internet connection we want to list files to download\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     api \u001b[38;5;241m=\u001b[39m HfApi(\n\u001b[1;32m    149\u001b[0m         library_name\u001b[38;5;241m=\u001b[39mlibrary_name,\n\u001b[1;32m    150\u001b[0m         library_version\u001b[38;5;241m=\u001b[39mlibrary_version,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    154\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     repo_info \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mSSLError, requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mProxyError):\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Actually raise for those subclasses of ConnectionError\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py:2704\u001b[0m, in \u001b[0;36mHfApi.repo_info\u001b[0;34m(self, repo_id, revision, repo_type, timeout, files_metadata, expand, token)\u001b[0m\n\u001b[1;32m   2702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported repo type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2704\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   2710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py:2488\u001b[0m, in \u001b[0;36mHfApi.model_info\u001b[0;34m(self, repo_id, revision, timeout, securityStatus, files_metadata, expand, token)\u001b[0m\n\u001b[1;32m   2486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expand:\n\u001b[1;32m   2487\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m expand\n\u001b[0;32m-> 2488\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2489\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   2490\u001b[0m data \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:93\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     95\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/mnt/c4a1c56d-0589-48cd-9717-f6c33a86fa3b/Tools/tts-kokoro-app/.venv/lib/python3.11/site-packages/urllib3/connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/http/client.py:1395\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1394\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/socket.py:718\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/lib/python3.11/ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "def convert_pdf_to_text(source_path):\n",
    "    # PDF를 텍스트로 변환\n",
    "    converter = DocumentConverter()\n",
    "    result = converter.convert(source_path)\n",
    "    text = result.document.export_to_text()\n",
    "\n",
    "    # 정규표현식을 사용하여 HTML 주석 제거\n",
    "    text = re.sub(r'<!--.*?-->', '', text)\n",
    "\n",
    "    # 페이지 숫자 제거 (예: [Page 1], [1], 1 등)\n",
    "    text = re.sub(r'\\[Page \\d+\\]|\\[\\d+\\]|\\b\\d+\\b(?=\\s*$)', '', text)\n",
    "\n",
    "    # 읽을 수 없는 특수 기호 제거 (유니코드 문장 부호는 유지)\n",
    "    # text = re.sub(r'[^\\w\\s\\p{P}]', '', text, flags=re.UNICODE)\n",
    "\n",
    "    # 연속된 공백 제거 및 앞뒤 공백 제거\n",
    "    # text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# 함수 사용 예시\n",
    "source = \"../data/Hands-On Large Language Models Language.pdf\"\n",
    "cleaned_text = convert_pdf_to_text(source)\n",
    "print(cleaned_text[1:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OREILLY\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hands-On Large Language Models\n",
      "\n",
      "AI has acquired startling new language capabilities in just the past few years. Driven by rapid advances in deep learning, language AI systems are able to write and understand text better than ever before. This trend is enabling new features, products, and entire industries. Through this book's visually educational nature, readers will learn practical tools and concepts they need to use these capabilities today.\n",
      "\n",
      "You'll understand how to use pretrained large language models for use cases like copywriting and summarization; create semantic search systems that go beyond keyword matching; and use existing libraries and pretrained models for text classification, search, and clusterings.\n",
      "\n",
      "This book also helps you:\n",
      "\n",
      " · Understand the architecture of Transformer language models that excel at text generation and representation\n",
      "\n",
      " · Build advanced LLM pipelines to cluster text documents and explore the topics they cover\n",
      "\n",
      " · Build semantic search engines that go beyond keyword search, using methods like dense retrieval and rerankers\n",
      "\n",
      " · Explore how generative models can be used, from prompt engineering all the way to retrieval-augmented generation\n",
      "\n",
      " · Gain a deeper understanding of how to train LLMs and optimize them for specific applications using generative model fine-tuning, contrastive fine-tuning, and in-context learning\n",
      "\n",
      "DATA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "' Jay and Maarten have continued their tradition of providing beautifully illustrated and insightful descriptions of complex topics. Their book is a valuable resource for anyone looking to understand the main techniques behind how large language models are built.'\n",
      "\n",
      "-Andrew Ng\n",
      "\n",
      "founder of DeepLearning.AI\n",
      "\n",
      " 'I can't think of another book that is more important to read right now. On every single page, I learned something that is critical to success in this era of language models.'\n",
      "\n",
      "-Josh Starmer,\n",
      "\n",
      "StatQuest\n",
      "\n",
      "Jay Alammar is director and engineering fellow at Cohere.\n",
      "\n",
      "Maarten Grootendorst is senior clinical data scientist at the Netherlands Comprehensive Cancer Organization (IKNL).\n",
      "\n",
      "linkedin.com/company/oreilly-media youtube.com/oreillymedia\n",
      "\n",
      "Praise for Hands-On Large Language Models\n",
      "\n",
      "This is an exceptional guide to the world of language models and their practical applications in industry. Its highly-visual coverage of generative, representational, and retrieval applications of language models empowers readers to quickly understand, use, and refine LLMs. Highly recommended!\n",
      "\n",
      "-Nils Reimers, Director of Machine Learning at Cohere | creator of sentence-transformers\n",
      "\n",
      "Jay and Maarten have continued their tradition of providing beautifully illustrated and insightful descriptions of complex topics in their new book. Bolstered with working code, timelines, and references to key papers, their book is a valuable resource for anyone looking to understand the main techniques behind how Large Language Models are built.\n",
      "\n",
      "-Andrew Ng, founder of DeepLearning.AI\n",
      "\n",
      "I can't think of another book that is more important to read right now. On every single page, I learned something that is critical to success in this era of language models. -Josh Starmer, StatQuest\n",
      "\n",
      "If you're looking to get up to speed in everything regarding LLMs, look no further! In this wonderful book, Jay and Maarten will take you from zero to expert in the history and latest advances in large language models. With very intuitive explanations, great real-life examples, clear illustrations, and comprehensive code labs, this book lifts the curtain on the complexities of transformer models, tokenizers, semantic search, RAG, and many other cutting-edge technologies. A must read for anyone interested in the latest AI technology!\n",
      "\n",
      "-Luis Serrano, PhD, Founder and CEO of Serrano Academy\n",
      "\n",
      "This book is a must-read for anyone interested in the rapidly-evolving field of generative AI. With a focus on both text and visual embeddings, it's a great blend of algorithmic evolution, theoretical rigor, and practical guidance. Whether you are a student, researcher, or industry professional, this book will equip you with the use cases and solutions needed to level-up your knowledge of generative AI. Well done!\n",
      "\n",
      "-Chris Fregly, Principal Solution Architect, Generative AI at AWS\n",
      "\n",
      "In the heart of the GenAI revolution, this indispensable guide masterfully balances theory and practice, navigating the vast landscape of large language models to equip readers with the knowledge needed for immediate and transformative impact in the field of AI.\n",
      "\n",
      "-Tarun Narayanan Venkatachalam, AI Researcher, University of Washington\n",
      "\n",
      "Timely reading to get hands-on experience with language models. -Emir Muñoz, Genesys\n",
      "\n",
      "Hands-On Large Language Models brings clarity and practical examples to cut through the hype of AI. It provides a wealth of great diagrams and visual aids to supplement the clear explanations. The worked examples and code make concrete what other books leave abstract. The book starts with simple introductory beginnings, and steadily builds in scope. By the final chapters, you will be fine-tuning and building your own large language models with confidence.\n",
      "\n",
      "-Leland McInnes, Researcher at the Tutte Institute for Mathematics and Computing\n",
      "\n",
      "Finally, a book that not only avoids superficial coverage of large language models but also thoroughly explores the background in a way that is both accessible and engaging. The authors have masterfully created a definitive guide that will remain essential reading despite the fast-paced advancements in the field.\n",
      "\n",
      "-Prof. DDr. Roman Egger, CEO of Smartvisions.at and Modul University Vienna\n",
      "\n",
      "Hands-On Large Language Models\n",
      "\n",
      "Language Understanding and Generation\n",
      "\n",
      "Jay Alammar and Maarten Grootendorst\n",
      "\n",
      "\n",
      "\n",
      "Hands-On Large Language Models\n",
      "\n",
      "by Jay Alammar and Maarten Grootendorst\n",
      "\n",
      "Copyright © 2024 Jay Alammar and Maarten Pieter Grootendorst. All rights reserved.\n",
      "\n",
      "Printed in the United States of America.\n",
      "\n",
      "Published by O'Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "\n",
      "O'Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com .\n",
      "\n",
      "Acquisitions Editor: Nicole Butterfield Development Editor: Michele Cronin Production Editor: Ashley Stussy Copyeditor: Charles Roumeliotis Proofreader: Kim Cofer\n",
      "\n",
      "Indexer: BIM Creatives, LLC Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Kate Dullea\n",
      "\n",
      "September 2024:\n",
      "\n",
      "First Edition\n",
      "\n",
      "Revision History for the First Edition\n",
      "\n",
      "2024-09-10: First Release\n",
      "\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781098150969 for release details.\n",
      "\n",
      "The O'Reilly logo is a registered trademark of O'Reilly Media, Inc. Hands-On Large Language Models , the cover image, and related trade dress are trademarks of O'Reilly Media, Inc.\n",
      "\n",
      "The views expressed in this  work  are  those  of  the  authors  and  do  not  represent  the  publisher's  views. While  the  publisher  and  the  authors  have  used  good  faith  efforts  to  ensure  that  the  information  and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for  errors  or  omissions, including without limitation responsibility for damages resulting from the use of  or  reliance  on  this  work.  Use  of  the  information  and  instructions  contained  in  this  work  is  at  your own risk.  If  any  code  samples  or  other  technology  this  work  contains  or  describes  is  subject  to  open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Model Selection                                                                                                            115\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Summary                                                                                                                       198\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Part III. Training and Fine-Tuning Language Models\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Preface\n",
      "\n",
      "Large language models (LLMs) have had a profound and far-reaching impact on the world. By enabling machines to better understand and generate human-like language, LLMs have opened new possibilities in the field of AI and impacted entire industries.\n",
      "\n",
      "This book provides a comprehensive and highly visual introduction to the world of LLMs,  covering  both  the  conceptual  foundations  and  practical  applications.  From word  representations  that  preceded  deep  learning  to  the  cutting-edge  (at  the  time of this writing) Transformer architecture, we will explore the history and evolution of  LLMs. We delve into the inner workings of LLMs, exploring their architectures, training methods, and fine-tuning techniques. We also examine various applications of  LLMs  in  text  classification,  clustering,  topic  modeling,  chatbots,  search  engines, and more.\n",
      "\n",
      "With  its  unique  blend  of  intuition-building,  applications,  and  illustrative  style,  we hope that this  book  provides  the  ideal  foundation  for  those  looking  to  explore  the exciting world of LLMs. Whether you are a beginner or an expert, we invite you to join us on this journey to start building with LLMs.\n",
      "\n",
      "An Intuition-First Philosophy\n",
      "\n",
      "The main goal of this book is to provide an intuition into the field of LLMs. The pace of development in the Language AI field is incredibly fast and frustration can build trying to keep up with the latest technologies. Instead, we focus on the fundamentals of LLMs and intend to provide a fun and easy learning process.\n",
      "\n",
      "To  achieve  this intuition-first  philosophy we  liberally  make  use  of  visual  language. Illustrations will help give a visual identity to major concepts and processes involved\n",
      "\n",
      "in  the  learning  process  of  LLMs.   With  our  illustrative  method  of  storytelling,  we 1 want to take you on a journey to this exciting and potentially world-changing field.\n",
      "\n",
      "Throughout the book, we make a clear distinction between representation and generative language models. Representation models are LLMs that do not generate text but are commonly used for task-specific use cases, like classification, whereas generation models are LLMs that generate text, like GPT models. Although generative models are typically the first thing that comes to mind when thinking about LLMs, there is still much use for representation models. We are also loosely using the word 'large' in large language models and often elect to simply call them language models as size descriptions are often rather arbitrary and not always indicative of capability.\n",
      "\n",
      "Prerequisites\n",
      "\n",
      "This book assumes that you have some experience programming in Python and are familiar with the fundamentals of machine learning. The focus will be on building a strong intuition rather than deriving mathematical equations. As such, illustrations combined with hands-on examples will drive the examples and learning through this book. This book assumes no prior knowledge of popular deep learning frameworks such as PyTorch or TensorFlow nor any prior knowledge of generative modeling.\n",
      "\n",
      "If you are not familiar with Python, a great place to start is Learn Python, where you will  find  many  tutorials  on  the  basics  of  the  language.  To  further  ease  the  learning process, we made all the code available on Google Colab, a platform where you can run all of the code without the need to install anything locally.\n",
      "\n",
      "Book Structure\n",
      "\n",
      "The book is broadly divided into three parts. They are illustrated in Figure P-1 to give you a full view of the book. Note that each chapter can be read independently, so feel free to skim chapters you are already familiar with.\n",
      "\n",
      "Part I: Understanding Language Models\n",
      "\n",
      "In Part I of the book, we explore the inner workings of language models both small and large. We start with an overview of the field and common techniques (see Chapter 1) before moving over to two central components of these models, tokenization and  embeddings  (see  Chapter  2).  We  finish  this  part  of  the  book  with  an  updated and expanded version of Jay's well-known Illustrated Transformer, which dives into\n",
      "\n",
      "|\n",
      "\n",
      "the architecture of these models (see Chapter 3). Many terms and definitions will be introduced that are used throughout the book.\n",
      "\n",
      "\n",
      "\n",
      "Part II: Using Pretrained Language Models\n",
      "\n",
      "In Part II of the book, we explore how LLMs can be used through common use cases. We  use  pretrained  models  and  demonstrate  their  capabilities  without  the  need  to fine-tune them.\n",
      "\n",
      "You learn how to use language models for supervised classification (see Chapter 4), text  clustering  and  topic  modeling  (see  Chapter  5),  leveraging  embedding  models for  semantic  search  (see  Chapter  6),  generating  text  (see  Chapters  7  and  8),  and extending the capabilities of text generation to the visual domain (see Chapter 9).\n",
      "\n",
      "|\n",
      "\n",
      "Learning  these  individual  language  model  capabilities  will  equip  you  with  the  skill set  to  problem-solve  with  LLMs  and  build  more  and  more  advanced  systems  and pipelines.\n",
      "\n",
      "Part III: Training and Fine-Tuning Language Models\n",
      "\n",
      "In  Part  III  of  the  book,  we  explore  advanced  concepts  through  training  and  finetuning all kinds of language models. We will explore how to create and fine-tune an embedding model (see Chapter 10), review how to fine-tune BERT for classification (see Chapter 11), and end the book with several methods for fine-tuning generation models (see Chapter 12).\n",
      "\n",
      "Hardware and Software Requirements\n",
      "\n",
      "Running generative models is generally a compute-intensive task that requires a computer with a strong GPU. Since those are not available to every reader, all examples in this book are made to run using an online platform, namely Google Colaboratory, often shortened to 'Google Colab. ' At the time of writing, this platform allows you to use an NVIDIA GPU (T4) for free to run your code. This GPU has 16 GB of VRAM (which is the memory of your GPU), which is the minimum amount of VRAM we expect for the examples throughout the book.\n",
      "\n",
      "\n",
      "\n",
      "Not all chapters require a minimum  of  16 GB  VRAM  as some examples, like training and fine-tuning, are more computeintensive  than  others,  such  as  prompt  engineering.  In  the  repository,  you  will  find  the  minimum  GPU  requirements  for  each chapter.\n",
      "\n",
      "All code, requirements, and additional tutorials are available in this book's repository. If you want to run the examples locally, we recommend access to an NVIDIA GPU with a minimum of 16 GB of VRAM. For a local installation, for example with conda, you can follow this setup to create your environment:\n",
      "\n",
      "conda create -n thellmbook python=3.10 conda activate thellmbook\n",
      "\n",
      "You can install all the necessary dependencies by forking or cloning the repository and then running the following in your newly created Python 3.10 environment:\n",
      "\n",
      "pip install -r requirements.txt\n",
      "\n",
      "|\n",
      "\n",
      "API Keys\n",
      "\n",
      "We  use  both  open  source  and  proprietary  models  throughout  the  examples  to demonstrate the advantages and disadvantages of both. For the proprietary models, using OpenAI and Cohere's offering, you will need to create a free account:\n",
      "\n",
      "OpenAI\n",
      "\n",
      "Click 'sign up' on the site to create a free account. This account allows you to create an API key, which can be used to access GPT-3.5. Then, go to ' API keys' to create a secret key.\n",
      "\n",
      "Cohere\n",
      "\n",
      "Register a free account on the website. Then, go to ' API keys' to create a secret key.\n",
      "\n",
      "Note that with both accounts, rate limits apply and that these free API keys only allow for  a  limited  number  of  calls  per  minute.  Throughout  all  examples,  we  have  taken that into account and provided local alternatives if necessary.\n",
      "\n",
      "For the open source models, you do not need to create an account with the exception of the Llama 2 model in Chapter 2. To use that model, you will need a Hugging Face account:\n",
      "\n",
      "Hugging Face\n",
      "\n",
      "Click 'sign up' on the Hugging Face website to create a free account. Then, in 'Settings' go to ' Access Tokens' to create a token that you can use to download certain LLMs.\n",
      "\n",
      "Conventions Used in This Book\n",
      "\n",
      "The following typographical conventions are used in this book:\n",
      "\n",
      "Italic\n",
      "\n",
      "Indicates new terms, URLs, email addresses, filenames, and file extensions.\n",
      "\n",
      "Constant width\n",
      "\n",
      "Used  for  program  listings,  as  well  as  within  paragraphs  to  refer  to  program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.\n",
      "\n",
      "Constant width bold\n",
      "\n",
      "Shows commands or other text that should be typed literally by the user.\n",
      "\n",
      "Constant width italic\n",
      "\n",
      "Shows text that should be replaced with user-supplied values or by values determined by context.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This element signifies a tip or suggestion.\n",
      "\n",
      "This element signifies a general note.\n",
      "\n",
      "Using Code Examples\n",
      "\n",
      "Supplemental  material  (code  examples,  exercises,  etc.)  is  available  for  download  at https://github.com/HandsOnLLM/Hands-On-Large-Language-Models .\n",
      "\n",
      "If you have a technical question or a problem using the code examples, please send email to support@oreilly.com .\n",
      "\n",
      "This  book  is  here  to  help  you  get  your  job  done.  In  general,  if  example  code  is offered  with  this  book,  you  may  use  it  in  your  programs  and  documentation.  You do  not  need  to  contact  us  for  permission  unless  you're  reproducing  a  significant portion of the code. For example, writing a program that uses several chunks of code from this  book  does  not  require  permission.  Selling  or  distributing  examples  from O'Reilly  books  does  require  permission.  Answering  a  question  by  citing  this  book and quoting example code does not require permission. Incorporating a significant amount  of  example  code  from  this  book  into  your  product's  documentation  does require permission.\n",
      "\n",
      "We  appreciate,  but  generally  do  not  require,  attribution.  An  attribution  usually includes the title, author, publisher, and ISBN. For example: ' Hands-On Large Language Models by Jay Alammar and Maarten Grootendorst (O'Reilly). Copyright 2024 Jay Alammar and Maarten Pieter Grootendorst, 978-1-098-15096-9.'\n",
      "\n",
      "If  you  feel  your  use  of  code  examples  falls  outside  fair  use  or  the  permission  given above, feel free to contact us at permissions@oreilly.com .\n",
      "\n",
      "O'Reilly Online Learning\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "For more than 40 years, O'Reilly Media has provided technology  and  business  training,  knowledge,  and  insight  to  help companies succeed.\n",
      "\n",
      "Our unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O'Reilly's online learning platform  gives  you  on-demand  access  to  live  training  courses,  in-depth  learning paths, interactive coding environments, and a vast collection of text and video from O'Reilly and 200+ other publishers. For more information, visit https://oreilly.com .\n",
      "\n",
      "How to Contact Us\n",
      "\n",
      "Please address comments and questions concerning this book to the publisher:\n",
      "\n",
      "O'Reilly Media, Inc. 1005 Gravenstein Highway North Sebastopol, CA 95472 800-889-8969 (in the United States or Canada) 707-827-7019 (international or local) 707-829-0104 (fax) support@oreilly.com https://www.oreilly.com/about/contact.html\n",
      "\n",
      "We have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/hands_on_LLMs_1e .\n",
      "\n",
      "For news and information about our books and courses, visit https://oreilly.com .\n",
      "\n",
      "Find us on LinkedIn: https://linkedin.com/company/oreilly-media .\n",
      "\n",
      "Watch us on YouTube: https://youtube.com/oreillymedia .\n",
      "\n",
      "Acknowledgments\n",
      "\n",
      "Writing  this  book  has  been  an  incredible  experience,  collaboration,  and  journey for us.\n",
      "\n",
      "The field of (large) language models is one of the most dynamic areas in technology today,  and  within  the  span  of  writing  this  book,  we  have  witnessed  extraordinary advancements.  Yet,  despite  the  rapid  pace  of  change,  the  fundamental  principles remain strikingly consistent which made the writing process particularly intriguing. We are grateful to have had the opportunity to explore this field in-depth at such a pivotal moment.\n",
      "\n",
      "Working with our O'Reilly team was incredible! Special thanks to Michele Cronin for her amazing feedback, support, and enthusiasm for this book from day one. We could not have asked for a better editor-you are amazing! Thank you, Nicole Butterfield, for kicking off this book and helping us maintain a structured approach throughout the writing. Thank you to Karen Montgomery for creating our wonderful cover, we\n",
      "\n",
      "|\n",
      "\n",
      "love the kangaroo! Big thanks to Kate Dullea for being so patient with us having to go through hundreds of illustrations many times over. The timely early releases by Clare Laylock helped us see our work grow which was a big motivator, thank you. Thanks to Ashley Stussy and Charles Roumeliotis for the development in the final stages of the book and everyone else at O'Reilly who contributed.\n",
      "\n",
      "Thanks to our amazing crew of technical reviewers. Invaluable feedback was given by Harm Buisman, Emir Muñoz, Luba Elliott, Guarav Chawla, Rafael V. Pierre, Luba Elliott, Tarun Narayanan, Nikhil Buduma, and Patrick Harrison.\n",
      "\n",
      "Jay\n",
      "\n",
      "I' d  love  to  extend  my  deepest  gratitude  to  my  family  for  their  unwavering  support and inspiration. I would like to specifically acknowledge my parents, Abdullah and Mishael, and my aunts, Hussah and Aljoharah.\n",
      "\n",
      "I'm grateful to the friends, colleagues, and collaborators who helped me understand and explain the tricky concepts covered in this book as well as to the Cohere folks who cultivate a supporting learning and sharing environment. Thank you to Adrien Morisot, Aidan Gomez, Andy Toulis, Anfal Alatawi, Arash Ahmadian, Bharat Venkitesh,  Edward  Grefenstette,  Ivan  Zhang,  Joao  Araújo,  Luis  Serrano,  Matthias  Gallé, Meor Amer, Nick Frosst, Patrick Lewis, Phil Blunsom, Sara Hooker, and Suhas Pai.\n",
      "\n",
      "I  couldn't  conceive  of  this  project  getting  accomplished  to  the  level  it  has  without the  extraordinary  talent  and  tireless  effort  of  Maarten,  my  coauthor.  Your  ability to  repeatedly  nail  the  technical  details  (from  the  pinned  version  of  the  nth  import dependency to the latest in LLM quantization) while weaving some of the world's best visual narratives is absolutely breathtaking.\n",
      "\n",
      "Lastly, a tip of the hat to the incredible coffee shop scene of Riyadh, Saudi Arabia for supplying me with caffeine and a good place to focus from dawn until midnight. It's where I read most of these papers and worked out my understanding (looking at you, Elixir Bunn).\n",
      "\n",
      "Maarten\n",
      "\n",
      "I  want  to  begin  by  expressing  my  heartfelt  appreciation  to  my  coauthor,  Jay.  Y our insights have made this not only possible but incredibly fulfilling. This journey has been nothing short of amazing and collaborating with you has been an absolute joy.\n",
      "\n",
      "I  want  to  sincerely  thank  my  wonderful  colleagues  at  IKNL  for  their  continued support  throughout  this  journey.  A  special  mention  goes  to  Harm-our  Monday morning coffee breaks discussing this book were a constant source of encouragement.\n",
      "\n",
      "Thank you to my family and friends for their unwavering support, and to my parents in  particular.  Pap,  despite  the  challenges  you  faced,  you  always  found  a  way  to  be\n",
      "\n",
      "|\n",
      "\n",
      "there  for  me  when  I  needed  it  most,  thank  you.  Mam,  the  conversations  we  had as  aspiring  writers  were  wonderful  and  motivated  me  more  than  you  could  ever imagine. Thank you both for your endless support and encouragement.\n",
      "\n",
      "Finally, I am at a loss for words to adequately express my gratitude to my wonderful wife,  Ilse.  Lieverd,  your  boundless  enthusiasm  and  patience  have  been  legendary, especially when I droned on about the latest LLM developments for hours on end. You are my greatest support. My apologies to my amazing daughter, Sarah. At just two years old, you already have listened to more about large language models than anyone  should  have  to  endure  in  a  lifetime!  I  promise  we'll  make  up  for  it  with endless playtime and adventures together.\n",
      "\n",
      "|\n",
      "\n",
      "Understanding Language Models\n",
      "\n",
      "An Introduction to Large Language Models\n",
      "\n",
      "Humanity is at an inflection point. From 2012 onwards, developments in building AI systems (using deep neural networks) accelerated so that by the end of the decade, they yielded the first software system able to write articles indiscernible from those written  by  humans.  This  system  was  an  AI  model  called  Generative  Pre-trained Transformer 2, or GPT-2. 2022 marked the release of ChatGPT, which demonstrated how profoundly  this  technology  was  poised  to  revolutionize  how  we  interact  with technology  and  information.  Reaching  one  million  active  users  in  five  days  and then one hundred million active users in two months, the new breed of AI models started  out  as  human-like  chatbots  but  quickly  evolved  into  a  monumental  shift  in our approach to common tasks, like translation, text generation, summarization, and more. It became an invaluable tool for programmers, educators, and researchers.\n",
      "\n",
      "The success of ChatGPT was unprecedented and popularized more research into the technology behind it, namely large language models (LLMs). Both proprietary and public  models  were  being  released  at  a  steady  pace,  closing  in  on,  and  eventually catching up to the performance of ChatGPT. It is not an exaggeration to state that almost all attention was on LLMs.\n",
      "\n",
      "As  a  result,  2023  will  always  be  known,  at  least  to  us,  as  the  year  that  drastically changed our field, Language Artificial Intelligence (Language AI), a field characterized by the development of systems capable of understanding and generating human language.\n",
      "\n",
      "However, LLMs have been around for a while now and smaller models are still relevant to this day. LLMs are much more than just a single model and there are many other techniques and models in the field of language AI that are worth exploring.\n",
      "\n",
      "In  this  book,  we  aim  to  give  readers  a  solid  understanding  of  the  fundamentals of  both  LLMs  and  the  field  of  Language  AI  in  general.  This  chapter  serves  as  the\n",
      "\n",
      "scaffolding for the rest of the book and will introduce concepts and terms that we will use throughout the chapters.\n",
      "\n",
      "But mostly, we intend to answer the following questions in this chapter:\n",
      "\n",
      " · What is Language AI?\n",
      "\n",
      " · What are large language models?\n",
      "\n",
      " · What are the common use cases and applications of large language models?\n",
      "\n",
      " · How can we use large language models ourselves?\n",
      "\n",
      "What Is Language AI?\n",
      "\n",
      "The term artificial intelligence (AI) is often used to describe computer systems dedicated to performing tasks close to human intelligence, such as speech recognition, language translation, and visual perception. It is the intelligence of software as opposed to the intelligence of humans.\n",
      "\n",
      "Here is a more formal definition by one of the founders of the artificial intelligence discipline:\n",
      "\n",
      "[Artificial intelligence is] the science and engineering of making intelligent machines, especially  intelligent  computer  programs.  It  is  related  to  the  similar  task  of  using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable.\n",
      "\n",
      " -John McCarthy, 2007 1\n",
      "\n",
      "Due to  the  ever-evolving  nature  of  AI,  the  term  has  been  used  to  describe  a  wide variety of systems, some of which might not truly embody intelligent behavior. For instance, characters in computer games (NPCs [nonplayable characters]) have often been referred to as AI even though many are nothing more than if-else statements.\n",
      "\n",
      "Language  AI  refers  to  a  subfield  of  AI  that  focuses  on  developing  technologies capable  of  understanding,  processing,  and  generating  human  language.  The  term Language  AI can  often  be  used  interchangeably  with natural  language  processing (NLP) with the continued success of machine learning methods in tackling language processing problems.\n",
      "\n",
      "|\n",
      "\n",
      "We use the term Language AI to  encompass technologies that technically might not be LLMs but still have a significant impact on the field, like how retrieval systems can give LLMs superpowers (see Chapter 8).\n",
      "\n",
      "Throughout this book, we want to focus on the models that have had a major role in shaping the field of Language AI. This means exploring more than just LLMs in isolation. That, however, brings us to the question: what are large language models? To  begin  answering  this  question  in  this  chapter,  let's  first  explore  the  history  of Language AI.\n",
      "\n",
      "A Recent History of Language AI\n",
      "\n",
      "The history of Language AI encompasses many developments and models aiming to represent and generate language, as illustrated in Figure 1-1.\n",
      "\n",
      "\n",
      "\n",
      "Language, however, is a tricky concept for computers. Text is unstructured in nature\n",
      "\n",
      "and loses  its  meaning  when  represented  by  zeros  and  ones  (individual  characters). As a result, throughout the history of Language AI, there has been a large focus on representing language in a structured manner so that it can more easily be used by computers. Examples of these Language AI tasks are provided in Figure 1-2.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Representing Language as a Bag-of-Words\n",
      "\n",
      "Our history of Language AI starts with a technique called bag-of-words, a method for representing unstructured text.  It was first mentioned around the 1950s but became 2 popular around the 2000s.\n",
      "\n",
      "Bag-of-words works as follows: let's assume that we have two sentences for which we want to create numerical representations. The first step of the bag-of-words model is tokenization ,  the  process  of  splitting  up  the  sentences  into  individual  words  or subwords ( tokens ), as illustrated in Figure 1-3.\n",
      "\n",
      "\n",
      "\n",
      "The most common method for tokenization is by splitting on a whitespace to create individual words. However, this has its disadvantages as some languages, like Mandarin, do not have whitespaces around individual words. In the next chapter, we will\n",
      "\n",
      "|\n",
      "\n",
      "go in depth about tokenization and how that technique influences language models. As illustrated  in  Figure  1-4,  after  tokenization,  we  combine  all  unique  words  from each sentence to create a vocabulary that we can use to represent the sentences.\n",
      "\n",
      "\n",
      "\n",
      "Using our vocabulary, we simply count how often a word in each sentence appears, quite  literally  creating  a  bag  of  words.  As  a  result,  a  bag-of-words  model  aims  to create  representations  of  text  in  the  form  of  numbers,  also  called  vectors  or  vector representations, observed in Figure 1-5. Throughout the book, we refer to these kinds of models as representation models .\n",
      "\n",
      "\n",
      "\n",
      "Although bag-of-words is a classic method, it is  by  no  means  completely  obsolete. In  Chapter  5,  we  will  explore  how  it  can  still  be  used  to  complement  more  recent language models.\n",
      "\n",
      "|\n",
      "\n",
      "Better Representations with Dense Vector Embeddings\n",
      "\n",
      "Bag-of-words, although an elegant approach, has a flaw. It considers language to be nothing more than an almost literal bag of words and ignores the semantic nature, or meaning, of text.\n",
      "\n",
      "Released in 2013, word2vec was one of the first successful attempts at capturing the meaning of text in embeddings . 3 Embeddings are vector representations of data that attempt to capture its meaning. To do so, word2vec learns semantic representations of words by training on vast amounts of textual data, like the entirety of Wikipedia.\n",
      "\n",
      "To  generate  these  semantic  representations,  word2vec  leverages neural  networks . These networks consist of interconnected layers of nodes that process information. As  illustrated  in  Figure  1-6,  neural  networks  can  have  many  layers  where  each connection  has  a  certain  weight  depending  on  the  input.  These  weights  are  often referred to as the parameters of the model.\n",
      "\n",
      "\n",
      "\n",
      "Using  these  neural  networks,  word2vec  generates  word  embeddings  by  looking  at which  other  words  they  tend  to  appear  next  to  in  a  given  sentence.  We  start  by assigning every word in our vocabulary with a vector embedding, say of 50 values for each word initialized with random values. Then in every training step, as illustrated in Figure 1-7, we take pairs of words from the training data and a model attempts to predict whether or not they are likely to be neighbors in a sentence.\n",
      "\n",
      "|\n",
      "\n",
      "During  this  training  process,  word2vec  learns  the  relationship  between  words  and distills  that  information  into  the  embedding.  If  the  two  words  tend  to  have  the same neighbors, their  embeddings  will  be  closer  to  one  another  and  vice  versa.  In Chapter 2, we will look closer at word2vec's training procedure.\n",
      "\n",
      "\n",
      "\n",
      "The resulting embeddings capture the meaning of words but what exactly does that mean? To illustrate  this  phenomenon,  let's  somewhat  oversimplify  and  imagine  we have embeddings of several words, namely 'apple' and 'baby.' Embeddings attempt to capture meaning by representing the properties of words. For instance, the word 'baby' might score high on the properties 'newborn' and 'human' while the word 'apple' scores low on these properties.\n",
      "\n",
      "As illustrated in Figure 1-8, embeddings can have many properties to represent the meaning of a word. Since the size of embeddings is fixed, their properties are chosen to create a mental representation of the word.\n",
      "\n",
      "\n",
      "\n",
      "In  practice,  these  properties  are  often  quite  obscure  and  seldom  relate  to  a  single entity  or  humanly  identifiable  concept.  However,  together,  these  properties  make sense  to  a  computer  and  serve  as  a  good  way  to  translate  human  language  into computer language.\n",
      "\n",
      "|\n",
      "\n",
      "Embeddings  are  tremendously  helpful  as  they  allow  us  to  measure  the  semantic similarity between two words. Using various distance metrics, we can judge how close one  word  is  to  another.  As  illustrated  in  Figure  1-9,  if  we  were  to  compress  these embeddings  into  a  two-dimensional  representation,  you  would  notice  that  words with similar meaning tend to be closer. In Chapter 5, we will explore how to compress these embeddings into n -dimensional space.\n",
      "\n",
      "\n",
      "\n",
      "Types of Embeddings\n",
      "\n",
      "There are many types of embeddings, like word embeddings and sentence embeddings that are used to indicate different levels of abstractions (word versus sentence), as illustrated in Figure 1-10.\n",
      "\n",
      "Bag-of-words,  for  instance,  creates  embeddings  at  a  document  level  since  it  represents  the  entire  document.  In  contrast,  word2vec  generates  embeddings  for  words only.\n",
      "\n",
      "Throughout the book, embeddings will take on a central role as they are utilized in many use cases, such as classification (see Chapter 4), clustering (see Chapter 5), and semantic search and retrieval-augmented generation (see Chapter 8). In Chapter 2, we will take our first deep dive into token embeddings.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Encoding and Decoding Context with Attention\n",
      "\n",
      "The  training  process  of  word2vec  creates  static,  downloadable  representations  of words. For instance, the word 'bank' will always have the same embedding regardless of the context in which it is used. However, 'bank' can refer to both a financial bank as  well  as  the  bank  of  a  river.  Its  meaning,  and  therefore  its  embeddings,  should change depending on the context.\n",
      "\n",
      "A step in encoding this text was achieved through recurrent neural networks (RNNs). These  are  variants  of  neural  networks  that  can  model  sequences  as  an  additional input.\n",
      "\n",
      "To  do  so,  these  RNNs  are  used  for  two  tasks, encoding or  representing  an  input sentence and decoding or  generating an output sentence. Figure 1-11 illustrates this concept by showing how a sentence like 'I love llamas' gets translated to the Dutch 'Ik hou van lama's. '\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Each  step  in  this  architecture  is autoregressive . When  generating  the  next  word, this  architecture  needs  to  consume  all  previously  generated  words,  as  shown  in Figure 1-12.\n",
      "\n",
      "\n",
      "\n",
      "The  encoding  step  aims  to  represent  the  input  as  well  as  possible,  generating  the context in the form of an embedding, which serves as the input for the decoder. To generate this representation, it takes embeddings as its inputs for words, which means we can use word2vec for the initial representations. In Figure 1-13, we can observe this process. Note how the inputs are processed sequentially, one at a time, as well as the output.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "This  context  embedding,  however,  makes  it  difficult  to  deal  with  longer  sentences since it is merely a single embedding representing the entire input. In 2014, a solution called attention was introduced that highly improved upon the original architecture. 4 Attention  allows  a  model  to  focus  on  parts  of  the  input  sequence  that  are  relevant to  one  another  ('attend'  to  each  other)  and  amplify  their  signal,  as  shown  in  Figure 1-14. Attention selectively determines which words are most important in a given sentence.\n",
      "\n",
      "For instance, the output word 'lama's' is Dutch for 'llamas, ' which is why the attention between both is high. Similarly, the words 'lama's' and 'I' have lower attention since they aren't as related. In Chapter 3, we will go more in depth on the attention mechanism.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "By  adding  these  attention  mechanisms  to  the  decoder  step,  the  RNN  can  generate signals for each input word in the sequence related to the potential output. Instead of passing only a context embedding to the decoder, the hidden states of all input words are passed. This process is demonstrated in Figure 1-15.\n",
      "\n",
      "\n",
      "\n",
      "As  a  result,  during  the  generation  of  'Ik  hou  van  lama's, '  the  RNN  keeps  track  of the words it mostly attends to perform the translation. Compared to word2vec, this architecture  allows  for  representing  the  sequential  nature  of  text  and  the  context in  which  it  appears  by  'attending'  to  the  entire  sentence.  This  sequential  nature, however, precludes parallelization during training of the model.\n",
      "\n",
      "|\n",
      "\n",
      "Attention Is All You Need\n",
      "\n",
      "The  true  power  of  attention,  and  what  drives  the  amazing  abilities  of  large  language  models,  was  first  explored  in  the  well-known  ' Attention  is  all  you  need' paper  released  in  2017.   The  authors  proposed  a  network  architecture  called  the 5 Transformer ,  which  was  solely  based  on  the  attention  mechanism and removed the recurrence network that we saw previously. Compared to the recurrence network, the Transformer could be trained in parallel, which tremendously sped up training.\n",
      "\n",
      "In  the  Transformer,  encoding  and  decoder  components are stacked on top of each other, as illustrated in Figure 1-16. This architecture remains autoregressive, needing to consume each generated word before creating a new word.\n",
      "\n",
      "\n",
      "\n",
      "Now, both the encoder and decoder blocks would revolve around attention instead of leveraging an RNN with attention features. The encoder block in the Transformer consists  of  two  parts, self-attention and  a feedforward  neural  network , which  are shown in Figure 1-17.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Compared  to  previous  methods  of  attention,  self-attention  can  attend  to  different positions within a single sequence, thereby more easily and accurately representing the input sequence as illustrated in Figure 1-18. Instead of processing one token at a time, it can be used to look at the entire sequence in one go.\n",
      "\n",
      "\n",
      "\n",
      "Compared to the encoder, the decoder has an additional layer that pays attention to the output of the encoder (to find the relevant parts of the input). As demonstrated in Figure 1-19, this process is similar to the RNN attention decoder that we discussed previously.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "As shown in Figure 1-20, the self-attention layer in the decoder masks future positions  so  it  only  attends  to  earlier  positions  to  prevent  leaking  information  when generating the output.\n",
      "\n",
      "\n",
      "\n",
      "Together, these building blocks create the Transformer architecture and are the foundation of many impactful models in Language AI, such as BERT and GPT-1, which we cover later in this chapter. Throughout this book, most models that we will use are Transformer-based models.\n",
      "\n",
      "|\n",
      "\n",
      "There  is  much  more  to  the  Transformer  architecture  than  what  we  explored  thus far.  In  Chapters  2  and  3,  we  will  go  through  the  many  reasons  why  Transformer models  work  so  well,  including  multi-head  attention,  positional  embeddings,  and layer normalization.\n",
      "\n",
      "Representation Models: Encoder-Only Models\n",
      "\n",
      "The original Transformer model is an encoder-decoder architecture that serves translation tasks well but cannot easily be used for other tasks, like text classification.\n",
      "\n",
      "In 2018, a new architecture called Bidirectional Encoder Representations from Transformers (BERT) was introduced that could be leveraged for a wide variety of tasks and  would  serve  as  the  foundation  of  Language  AI  for  years  to  come.   BERT  is 6 an  encoder-only  architecture  that  focuses  on  representing  language,  as  illustrated in  Figure  1-21.  This  means  that  it  only  uses  the  encoder  and  removes  the  decoder entirely.\n",
      "\n",
      "\n",
      "\n",
      "These  encoder  blocks  are  the  same  as  we  saw  before:  self-attention  followed  by feedforward neural networks. The input contains an additional token, the [CLS] or classification token, which is used as the representation for the entire input. Often, we use this [CLS] token as the input embedding for fine-tuning the model on specific tasks, like classification.\n",
      "\n",
      "|\n",
      "\n",
      "Training these encoder stacks can be a difficult task that BERT approaches by adopting a technique called masked language modeling (see Chapters 2 and 11). As shown in Figure 1-22, this method masks a part of the input for the model to predict. This prediction  task  is  difficult  but  allows  BERT  to  create  more  accurate  (intermediate) representations of the input.\n",
      "\n",
      "\n",
      "\n",
      "This  architecture  and  training  procedure  makes  BERT  and  related  architectures incredible at representing contextual language. BERT-like models are commonly used for transfer  learning, which  involves  first  pretraining  it  for  language  modeling  and then fine-tuning it for a specific task. For instance, by training BERT on the entirety of  Wikipedia,  it  learns  to  understand  the  semantic  and  contextual  nature  of  text. Then, as shown in Figure 1-23, we can use that pretrained model to fine-tune it for a specific task, like text classification.\n",
      "\n",
      "\n",
      "\n",
      "A huge benefit of pretrained models is that most of the training is already done for us. Fine-tuning on specific tasks is generally less compute-intensive and requires less data. Moreover, BERT-like models generate embeddings at almost every step in their\n",
      "\n",
      "|\n",
      "\n",
      "architecture. This also makes BERT models feature extraction machines without the need to fine-tune them on a specific task.\n",
      "\n",
      "Encoder-only models, like BERT, will be used in many parts of the book. For years, they have been and are still used for common tasks, including classification tasks (see Chapter 4), clustering tasks (see Chapter 5), and semantic search (see Chapter 8).\n",
      "\n",
      "Throughout the book, we will refer to encoder-only models as representation models to  differentiate  them  from  decoder-only,  which  we  refer  to  as generative  models . Note that the main distinction does not lie between the underlying architecture and the  way  these  models  work.  Representation  models  mainly  focus  on  representing language, for instance, by creating embeddings, and typically do not generate text. In contrast, generative models focus primarily on generating text and typically are not trained to generate embeddings.\n",
      "\n",
      "The distinction between representation and generative models and components will also  be  shown  in  most  images.  Representation  models  are  teal  with  a  small  vector icon (to indicate its focus on vectors and embeddings) whilst generative models are pink with a small chat icon (to indicate its generative capabilities).\n",
      "\n",
      "Generative Models: Decoder-Only Models\n",
      "\n",
      "Similar  to  the  encoder-only  architecture  of  BERT,  a  decoder-only  architecture  was proposed in 2018 to target generative tasks.  This architecture was called a Generative 7 Pre-trained  Transformer  (GPT)  for  its  generative  capabilities  (it's  now  known  as GPT-1  to  distinguish  it  from  later  versions).  As  shown  in  Figure  1-24,  it  stacks decoder blocks similar to the encoder-stacked architecture of BERT.\n",
      "\n",
      "GPT-1 was trained on a corpus of 7,000 books and Common Crawl, a large dataset of web pages. The resulting model consisted of 117 million parameters . Each parameter is a numerical value that represents the model's understanding of language.\n",
      "\n",
      "If everything remains the same, we expect more parameters to greatly influence the capabilities and performance of language models. Keeping this in mind, we saw larger and larger models being released at a steady pace. As illustrated in Figure 1-25, GPT-2 had 1.5 billion parameters  and GPT-3 used 175 billion parameters  quickly followed. 8 9\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "These generative decoder-only models, especially the 'larger' models, are commonly referred to as large language models (LLMs). As we will discuss later in this chapter, the  term  LLM  is  not  only  reserved  for  generative  models  (decoder-only)  but  also representation models (encoder-only).\n",
      "\n",
      "Generative LLMs, as sequence-to-sequence machines, take in some text and attempt to  autocomplete  it.  Although  a  handy  feature,  their  true  power  shone  from  being trained  as  a  chatbot.  Instead  of  completing  a  text,  what  if  they  could  be  trained  to answer questions? By fine-tuning these models, we can create instruct or chat models that can follow directions.\n",
      "\n",
      "As illustrated in Figure 1-26, the resulting model could take in a user query ( prompt ) and output a response that would most likely follow that prompt. As such, you will often hear that generative models are completion models.\n",
      "\n",
      "\n",
      "\n",
      "A  vital  part  of  these  completion  models  is  something  called  the context  length or context window .  The context length represents the maximum number of tokens the model can process, as shown in Figure 1-27. A large context window allows entire documents to be passed to the LLM. Note that due to the autoregressive nature of these models, the current context length will increase as new tokens are generated.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "The Year of Generative AI\n",
      "\n",
      "LLMs had a tremendous impact on the field and led some to call 2023 The Year of Generative AI with the release, adoption, and media coverage of ChatGPT (GPT-3.5). When we refer to ChatGPT, we are actually talking about the product and not the underlying model. When it was first released, it was powered by the GPT-3.5 LLM and  has  since  then  grown  to  include  several  more  performant  variants,  such  as GPT-4. 10\n",
      "\n",
      "GPT-3.5 was not the only model that made its impact in the Year of Generative AI. As  illustrated  in  Figure  1-28,  both  open  source  and  proprietary  LLMs  have  made their  way  to  the  people  at  an  incredible  pace.  These  open  source  base  models  are often referred to as foundation models and  can  be  fine-tuned for specific tasks, like following instructions.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Apart  from  the  widely  popular  Transformer  architecture,  new  promising  architectures  have  emerged  such  as  Mamba 11,12 and  RWKV. 13 These  novel  architectures attempt  to  reach  Transformer-level  performance  with  additional  advantages,  like larger context windows or faster inference.\n",
      "\n",
      "These developments exemplify the evolution of the field and showcase 2023 as a truly hectic year for AI. It took all we had to just keep up with the many developments, both within and outside of Language AI.\n",
      "\n",
      "As  such,  this  book  explores  more  than  just  the  latest  LLMs.  We  will  explore  how other  models,  such  as  embedding  models,  encoder-only  models,  and  even  bag-ofwords can be used to empower LLMs.\n",
      "\n",
      "|\n",
      "\n",
      "The Moving Definition of a 'Large Language Model'\n",
      "\n",
      "In our travels through the recent history of Language AI, we observed that primarily generative  decoder-only  (Transformer)  models  are  commonly  referred  to  as large language models . Especially if they are considered to be 'large. ' In practice, this seems like a rather constrained description!\n",
      "\n",
      "What if we create a model with the same capabilities as GPT-3 but 10 times smaller? Would such a model fall outside the 'large' language model categorization?\n",
      "\n",
      "Similarly,  what  if  we  released  a  model  as  big  as  GPT-4  that  can  perform  accurate text classification but does not have any generative capabilities? Would it still qualify as a large 'language model' if its primary function is not language generation, even though it still represents text?\n",
      "\n",
      "The problem with these kinds of definitions is that we exclude capable models. What name we give one model or the other does not change how it behaves.\n",
      "\n",
      "Since  the  definition  of  the  term  'large  language  model'  tends  to  evolve  with  the release of new models, we want to be explicit in what it means for this book. 'Large' is arbitrary and what might be considered a large model today could be small tomorrow. There are currently many names for the same thing and to us, 'large language models'  are  also  models  that  do  not  generate  text  and  can  be  run  on  consumer hardware.\n",
      "\n",
      "As such, aside from covering generative models, this book will also cover models with fewer than 1 billion parameters that do not generate text. We will explore how other models, such as embedding models, representation models, and even bag-of-words can be used to empower LLMs.\n",
      "\n",
      "The Training Paradigm of Large Language Models\n",
      "\n",
      "Traditional machine learning generally involves training a model for a specific task, like classification. As shown in Figure 1-29, we consider this to be a one-step process.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Creating LLMs, in contrast, typically consists of at least two steps:\n",
      "\n",
      "Language modeling\n",
      "\n",
      "The first step, called pretraining ,  takes the majority of computation and training time.  An  LLM  is  trained  on  a  vast  corpus  of  internet  text  allowing  the  model to  learn  grammar, context, and language patterns. This broad training phase is not yet directed toward specific tasks or applications beyond predicting the next word.  The  resulting  model  is  often  referred  to  as  a foundation  model or base model . These models generally do not follow instructions.\n",
      "\n",
      "Fine-tuning\n",
      "\n",
      "The second step, fine-tuning or sometimes post-training , involves using the previously trained model and further training it on a narrower task. This allows the LLM to adapt to specific tasks or to exhibit desired behavior. For example, we could fine-tune a base model to perform well on a classification task or to follow instructions. It saves massive amounts of resources because the pretraining phase is  quite  costly  and  generally  requires  data  and  computing  resources  that  are out  of  the  reach  of  most  people  and  organizations.  For  instance,  Llama  2  has been  trained  on  a  dataset  containing  2  trillion  tokens. 14 Imagine  the  compute necessary to create that model! In Chapter 12, we will go over several methods for fine-tuning foundation models on your dataset.\n",
      "\n",
      "Any  model  that  goes  through  the  first  step,  pretraining,  we  consider  a pretrained model, which also includes fine-tuned models. This two-step approach of training is visualized in Figure 1-30.\n",
      "\n",
      "\n",
      "\n",
      "Additional fine-tuning steps can be added to further align the model with the user's preferences, as we will explore in Chapter 12.\n",
      "\n",
      "|\n",
      "\n",
      "Large Language Model Applications: What Makes Them So Useful?\n",
      "\n",
      "The nature of LLMs makes them suitable for a wide range of tasks. With text generation and prompting, it almost seems as if your imagination is the limit. To illustrate, let's explore some common tasks and techniques:\n",
      "\n",
      "Detecting whether a review left by a customer is positive or negative\n",
      "\n",
      "This  is  (supervised)  classification  and  can  be  handled  with  both  encoder-  and decoder-only models either with pretrained models (see Chapter 4) or by finetuning models (see Chapter 11).\n",
      "\n",
      "Developing a system for finding common topics in ticket issues\n",
      "\n",
      "This  is  (unsupervised)  classification  for  which  we  have  no  predefined  labels. We  can  leverage  encoder-only  models  to  perform  the  classification  itself  and decoder-only models for labeling the topics (see Chapter 5).\n",
      "\n",
      "Building a system for retrieval and inspection of relevant documents\n",
      "\n",
      "A major component of language model systems is their ability to add external resources of information. Using semantic search, we can build systems that allow us  to  easily  access  and  find  information  for  an  LLM  to  use  (see  Chapter  8). Improve your system by creating or fine-tuning a custom embedding model (see Chapter 12).\n",
      "\n",
      "Constructing an LLM chatbot that can leverage external resources, such as tools and documents\n",
      "\n",
      "This  is  a  combination  of  techniques  that  demonstrates  how  the  true  power  of LLMs can be found through additional  components.  Methods  such  as  prompt engineering (see Chapter 6), retrieval-augmented generation (see Chapter 8), and fine-tuning an LLM (see Chapter 12) are all pieces of the LLM puzzle.\n",
      "\n",
      "Constructing an LLM capable of writing recipes based on a picture showing the products in your fridge\n",
      "\n",
      "This is a multimodal task where the LLM takes in an image and reasons about what it sees (see Chapter 9). LLMs are being adapted to other modalities, such as Vision, which opens a wide variety of interesting use cases.\n",
      "\n",
      "LLM applications are incredibly satisfying to create since they are partially bounded by the things you can imagine. As these models grow more accurate, using them in practice for creative use cases such as role-playing and writing children's books simply becomes more and more fun.\n",
      "\n",
      "|\n",
      "\n",
      "Responsible LLM Development and Usage\n",
      "\n",
      "The impact of LLMs has been and likely continues to be significant due to their widespread adoption. As we explore the incredible capabilities of LLMs it is important to keep their societal and ethical implications in mind. Several key points to consider:\n",
      "\n",
      "Bias and fairness\n",
      "\n",
      "LLMs  are  trained  on  large  amounts  of  data  that  might  contain  biases.  LLMs might learn from these biases, start to reproduce them, and potentially amplify them. Since the data on which LLMs are trained are seldom shared, it remains unclear what potential biases they might contain unless you try them out.\n",
      "\n",
      "Transparency and accountability\n",
      "\n",
      "Due to LLMs' incredible capabilities, it is not always clear when you are talking with  a  human  or  an  LLM.  As  such,  the  usage  of  LLMs  when  interacting  with humans  can  have  unintended  consequences  when  there  is  no  human  in  the loop.  For  instance,  LLM-based  applications  used  in  the  medical  field  might  be regulated as medical devices since they could affect a patient's well-being.\n",
      "\n",
      "Generating harmful content\n",
      "\n",
      "An LLM does not necessarily  generate  ground-truth  content  and  might  confidently output incorrect text. Moreover, they can be used to generate fake news, articles, and other misleading sources of information.\n",
      "\n",
      "Intellectual property\n",
      "\n",
      "Is the output of an LLM your intellectual property or that of the LLM's creator? When the output is similar to a phrase in the training data, does the intellectual property belong to the author of that phrase? Without access to the training data it remains unclear when copyrighted material is being used by the LLM.\n",
      "\n",
      "Regulation\n",
      "\n",
      "Due  to  the  enormous  impact  of  LLMs,  governments  are  starting  to  regulate commercial applications.  An  example  is  the  European  AI  Act,  which  regulates the development and deployment of foundation models including LLMs.\n",
      "\n",
      "As you develop and use LLMs, we want to stress the importance of ethical considerations and urge you to learn more about the safe and responsible use of LLMs and AI systems in general.\n",
      "\n",
      "Limited Resources Are All You Need\n",
      "\n",
      "The compute resources that we have referenced several times thus far generally relate to the GPU(s) you have available on your system. A powerful GPU (graphics card) will make both training and using LLMs much more efficient and faster.\n",
      "\n",
      "|\n",
      "\n",
      "In  choosing  a  GPU,  an  important  component  is  the  amount  of  VRAM  (video random-access memory) you have available. This refers to the amount of memory you have available on your GPU. In practice, the more VRAM you have the better. The reason for this is that some models simply cannot be used at all if you do not have sufficient VRAM.\n",
      "\n",
      "Because training and fine-tuning LLMs can be an expensive process, GPU-wise, those without a powerful GPU have often been referred to as the GPU-poor. This illustrates the battle for computing resources to train these huge models. To create the Llama 2 family of models, for example, Meta used A100-80 GB GPUs. Assuming renting such a  GPU  would  cost  $1.50/hr,  the  total  costs  of  creating  these  models  would  exceed $5,000,000! 15\n",
      "\n",
      "Unfortunately,  there  is  no  single  rule  to  determine  exactly  how  much  VRAM  you need for a specific model. It depends on the model's architecture and size, compression technique, context size, backend for running the model, etc.\n",
      "\n",
      "This book is for the GPU-poor! We will use models that users can run without the most expensive GPU(s) available or a big budget. To do so, we will make all the code available in Google Colab instances. At the time of writing, a free instance of Google Colab will net you a T4 GPU with 16 GB VRAM, which is the minimum amount of VRAM that we suggest.\n",
      "\n",
      "Interfacing with Large Language Models\n",
      "\n",
      "Interfacing with LLMs is a vital component of not only using them but also developing  an  understanding  of  their  inner  workings.  Due  to  the  many  developments  in the  field,  there  has  been  an  abundance  of  techniques,  methods,  and  packages  for communicating  with  LLMs.  Throughout  the  book,  we  intend  to  explore  the  most common techniques for doing so, including using both proprietary (closed source) and publicly available open models.\n",
      "\n",
      "Proprietary, Private Models\n",
      "\n",
      "Closed  source  LLMs  are  models  that  do  not  have  their  weights  and  architecture shared  with  the  public.  They  are  developed  by  specific  organizations  with  their underlying code being kept secret. Examples of such models include OpenAI's GPT-4 and Anthropic's Claude. These proprietary models are generally backed by significant commercial support and have been developed and integrated within their services.\n",
      "\n",
      "|\n",
      "\n",
      "You can access these models through an interface that communicates with the LLM, called an API (application programming interface), as illustrated in Figure 1-31. For instance, to use ChatGPT in Python you can use OpenAI's package to interface with the service without directly accessing it.\n",
      "\n",
      "\n",
      "\n",
      "A huge benefit of proprietary models is that the user does not need to have a strong GPU to use the LLM. The provider takes care of hosting and running the model and generally has more computing available. There is no expertise necessary concerning hosting and using the model, which lowers the barrier to entry significantly. Moreover, these models tend to be more performant than their open source counterparts due to the significant investment from these organizations.\n",
      "\n",
      "A downside to this is that it can be a costly service. The provider manages the risk and  costs  of  hosting  the  LLM,  which  often  translates  to  a  paid  service.  Moreover, since  there  is  no  direct  access  to  the  model,  there  is  no  method  to  fine-tune  it yourself. Lastly, your data is shared with the provider, which is not desirable in many common use cases, such as sharing patient data.\n",
      "\n",
      "Open Models\n",
      "\n",
      "Open LLMs are models that share their weights and architecture with the public to use. They are still developed by specific organizations but often share their code for creating or running the model locally-with varying levels of licensing that may or may not  allow  commercial  usage  of  the  model.  Cohere's  Command  R,  the  Mistral models, Microsoft's Phi, and Meta's Llama models are all examples of open models.\n",
      "\n",
      "\n",
      "\n",
      "There are ongoing discussions as to what truly represents an open source  model.  For  instance,  some  publicly  shared  models  have  a permissive commercial license, which means that the model cannot be  used  for  commercial  purposes.  For  many,  this  is  not  the  true definition  of  open  source,  which  states  that  using  these  models should  not  have  any  restrictions.  Similarly,  the  data  on  which  a model is trained as well as its source code are seldom shared.\n",
      "\n",
      "|\n",
      "\n",
      "You can download these models and use them on your device as long as you have a powerful GPU that can handle these kinds of models, as shown in Figure 1-32.\n",
      "\n",
      "\n",
      "\n",
      "A major advantage of these local models is that you, the user, have complete control over the model. You can use the model without depending on the API connection, fine-tune it, and run sensitive data through it. You are not dependent on any service and have complete transparency of the processes that lead to the output of the model. This benefit is enhanced by the large communities that enable these processes, such as Hugging Face, demonstrating the possibilities of collaborative efforts.\n",
      "\n",
      "A downside is that you need powerful hardware to run these models and even more when training or fine-tuning them. Moreover, it requires specific knowledge to set up and use these models (which we will cover throughout this book).\n",
      "\n",
      "We generally prefer using open source models wherever we can. The freedom this gives  to  play  around  with  options,  explore  the  inner  workings,  and  use  the  model locally arguably provides more benefits than using proprietary LLMs.\n",
      "\n",
      "Open Source Frameworks\n",
      "\n",
      "Compared to closed source LLMs, open source LLMs require you to use certain packages  to  run  them.  In  2023,  many  different  packages  and  frameworks  were  released that,  each  in  their  own  way,  interact  with  and  make  use  of  LLMs.  Wading  through hundreds  upon  hundreds  of  potentially  worthwhile  frameworks  is  not  the  most enjoyable experience.\n",
      "\n",
      "As a result, you might even miss your favorite framework in this book!\n",
      "\n",
      "Instead  of  attempting  to  cover  every  LLM  framework  in  existence  (there  are  too many, and they continue to grow in number), we aim to provide you with a solid foundation  for  leveraging  LLMs.  The  idea  is  that  after  reading  this  book,  you  can easily pick up most other frameworks as they all work in a very similar manner.\n",
      "\n",
      "The intuition  that  we  attempt  to  realize  is  an  important  component  of  this.  If  you have  an  intuitive  understanding  of  not  only  LLMs  but  also  using  them  in  practice with common frameworks, branching out to others should be a straightforward task.\n",
      "\n",
      "|\n",
      "\n",
      "More  specifically,  we  focus  on  backend  packages.  These  are  packages  without  a GUI (graphical  user  interface)  that  are  created  for  efficiently  loading  and  running any  LLM  on  your  device,  such  as  llama.cpp,  LangChain,  and  the  core  of  many frameworks, Hugging Face Transformers.\n",
      "\n",
      "\n",
      "\n",
      "We  will  mostly  cover  frameworks  for  interacting  with  large  language  models  through  code.  Although  it  helps  you  learn  the fundamentals  of  these  frameworks,  sometimes  you  just  want  a ChatGPT-like  interface  with  a  local  LLM.  Fortunately,  there  are many  incredible  frameworks  that  allow  for  this.  A  few  examples include text-generation-webui, KoboldCpp, and LM Studio.\n",
      "\n",
      "Generating Your First Text\n",
      "\n",
      "An  important  component  of  using  language  models  is  selecting  them.  The  main source for finding and downloading LLMs is the Hugging Face Hub. Hugging Face is  the  organization  behind  the  well-known  Transformers  package,  which  for  years has driven the development of language models in general. As the name implies, the package  was  built  on  top  of  the transformers framework  that  we  discussed  in  ' A Recent History of Language AI' on page 5.\n",
      "\n",
      "At the time of writing, you will find more than 800,000 models on Hugging Face's platform  for  many  different  purposes,  from  LLMs  and  computer  vision  models  to models that work with audio and tabular data. Here, you can find almost any open source LLM.\n",
      "\n",
      "Although we will explore all kinds of models throughout this book, let's start our first lines of code with a generative model. The main generative model we use throughout the book is Phi-3-mini, which is a relatively small (3.8 billion parameters) but quite performant model. 16 Due to its small size, the model can be run on devices with less than 8 GB of VRAM. If you perform quantization, a type of compression that we will further  discuss  in  Chapters  7  and  12,  you  can  use  even  less  than  6  GB  of  VRAM. Moreover, the model is licensed under the MIT license, which allows the model to be used for commercial purposes without constraints!\n",
      "\n",
      "Keep in mind that new and improved LLMs are frequently released. To ensure this book remains current, most examples are designed to work with any LLM. We'll also highlight different models in the repository associated with this book for you to try out.\n",
      "\n",
      "Let's get started! When you use an LLM, two models are loaded:\n",
      "\n",
      "|\n",
      "\n",
      " · The generative model itself\n",
      "\n",
      " · Its underlying tokenizer\n",
      "\n",
      "The tokenizer is in charge of splitting the input text into tokens before feeding it to the generative model. You can find the tokenizer and model on the Hugging Face site and only need the corresponding IDs to be passed. In this case, we use 'microsoft/ Phi-3-mini-4k-instruct' as the main path to the model.\n",
      "\n",
      "We can use transformers to load both the tokenizer and model. Note that we assume you  have  an  NVIDIA  GPU  ( device_map=\"cuda\" )  but  you  can  choose  a  different device instead. If you do not have access to a GPU you can use the free Google Colab notebooks we made available in the repository of this book:\n",
      "\n",
      "```\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained( \"microsoft/Phi-3-mini-4k-instruct\", device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code= True , ) tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
      "```\n",
      "\n",
      "Running the code will start downloading the model and depending on your internet connection can take a couple of minutes.\n",
      "\n",
      "Although we now have enough to start generating text, there is a nice trick in transformers that simplifies the process, namely transformers.pipeline .  It  encapsulates the model, tokenizer, and text generation process into a single function:\n",
      "\n",
      "```\n",
      "from transformers import pipeline # Create a pipeline generator = pipeline( \"text-generation\", model=model, tokenizer=tokenizer, return_full_text= False , max_new_tokens=500, do_sample= False ) The following parameters are worth mentioning: return_full_text By setting this to False , the prompt will not be returned but merely the output of the model.\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "max_new_tokens\n",
      "\n",
      "The maximum number of tokens the model will generate. By setting a limit, we prevent  long  and  unwieldy  output  as  some  models  might  continue  generating output until they reach their context window.\n",
      "\n",
      "do_sample\n",
      "\n",
      "Whether the model uses a sampling strategy to choose the next token. By setting this  to False ,  the  model  will  always  select  the  next  most  probable  token.  In Chapter 6, we explore several sampling parameters that invoke some creativity in the model's output.\n",
      "\n",
      "To generate our first text, let's instruct the model to tell a joke about chickens. To do so, we format the prompt in a list of dictionaries where each dictionary relates to an entity in the conversation. Our role is that of 'user' and we use the 'content' key to define our prompt:\n",
      "\n",
      "```\n",
      "# The prompt (user input / query) messages = [ {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"} ] # Generate output output = generator(messages) print(output[\"generated_text\"]) Why don't chickens like to go to the gym? Because they can't crack the eggsistence of it!\n",
      "```\n",
      "\n",
      "And that is it! The first text generated in this book was a decent joke about chickens.\n",
      "\n",
      "Summary\n",
      "\n",
      "In this first chapter of the book, we delved into the revolutionary impact LLMs have had on the Language AI field. It has significantly changed our approach to tasks such as translation, classification, summarization, and more. Through a recent history of Language AI, we explored the fundamentals of several types of LLMs, from a simple bag-of-words representation to more complex representations using neural networks.\n",
      "\n",
      "|\n",
      "\n",
      "We  discussed  the  attention  mechanism  as  a  step  toward  encoding  context  within models,  a  vital  component  of  what  makes  LLMs  so  capable.  We  touched  on  two main categories of models that use this incredible mechanism: representation models (encoder-only) like BERT and generative models (decoder-only) like the GPT family of  models.  Both  categories  are  considered  large  language  models  throughout  this book.\n",
      "\n",
      "Overall, the chapter provided an overview of the landscape of Language AI, including its  applications,  societal  and  ethical  implications,  and  the  resources  needed  to  run such models. We ended by generating our first text using Phi-3, a model that will be used throughout the book.\n",
      "\n",
      "In the next two chapters, you will learn about some underlying processes. We start by exploring tokenization and embeddings in Chapter 2, two often underestimated but vital components of the Language AI field. What follows in Chapter 3 is an in-depth look  into  language  models  where  you  will  discover  the  precise  methods  used  for generating text.\n",
      "\n",
      "|\n",
      "\n",
      "Tokens and Embeddings\n",
      "\n",
      "Tokens  and  embeddings  are  two  of  the  central  concepts  of  using  large  language models  (LLMs).  As  we've  seen  in  the  first  chapter,  they're  not  only  important  to understanding the history of Language AI, but we cannot have a clear sense of how LLMs work, how they're built, and where they will go in the future without a good sense of tokens and embeddings, as we can see in Figure 2-1.\n",
      "\n",
      "\n",
      "\n",
      "In this chapter, we look more closely at what tokens are and the tokenization methods used to power LLMs. We will then dive into the famous word2vec embedding method  that  preceded  modern-day  LLMs  and  see  how  it's  extending  the  concept of  token  embeddings  to  build  commercial  recommendation  systems  that  power  a lot  of  the  apps  you  use.  Finally,  we  go  from  token  embeddings  into sentence or\n",
      "\n",
      "text embeddings,  where  a  whole  sentence  or  document  can  have  one  vector  that represents it-enabling applications like semantic search and topic modeling that we see in Part II of this book.\n",
      "\n",
      "LLM Tokenization\n",
      "\n",
      "The  way  the  majority  of  people  interact  with  language  models,  at  the  time  of  this writing, is through a web playground that presents a chat interface between the user and  a  language  model.  You  may  notice  that  a  model  does  not  produce  its  output response all at once; it actually generates one token at a time.\n",
      "\n",
      "But tokens aren't only the output of a model, they're also the way in which the model sees its inputs. A text prompt sent to the model is first broken down into tokens, as we'll now see.\n",
      "\n",
      "How Tokenizers Prepare the Inputs to the Language Model\n",
      "\n",
      "Viewed  from  the  outside,  generative  LLMs  take  an  input  prompt  and  generate  a response, as we can see in Figure 2-2.\n",
      "\n",
      "\n",
      "\n",
      "Before  the  prompt  is  presented  to  the  language  model,  however,  it  first  has  to  go through a tokenizer that breaks it into pieces. You can find an example showing the tokenizer of GPT-4 on the OpenAI Platform. If we feed it the input text, it shows the output in Figure 2-3, where each token is shown in a different color.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Let's look at a code example and interact with these tokens ourselves. Here we'll be downloading an LLM and seeing how to tokenize the input before generating text with the LLM.\n",
      "\n",
      "Downloading and Running an LLM\n",
      "\n",
      "Let's start by loading our model and its tokenizer as we've done in Chapter 1:\n",
      "\n",
      "```\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained( \"microsoft/Phi-3-mini-4k-instruct\", device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code= True , ) tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "We  can  then  proceed  to  the  actual  generation.  We  first  declare  our  prompt,  then tokenize it, then pass those tokens to the model, which generates its output. In this case, we're asking the model to only generate 20 new tokens:\n",
      "\n",
      "```\n",
      "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>\" # Tokenize the input prompt input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\") # Generate the text generation_output = model.generate( input_ids=input_ids, max_new_tokens=20 ) # Print the output print(tokenizer.decode(generation_output)) Output: <s> Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|> Subject: My Sincere Apologies for the Gardening Mishap Dear\n",
      "```\n",
      "\n",
      "The text in bold is the 20 tokens generated by the model.\n",
      "\n",
      "Looking  at  the  code,  we  can  see  that  the  model  does  not  in  fact  receive  the  text prompt. Instead, the tokenizers processed the input prompt, and returned the information  the  model  needed  in  the  variable input_ids ,  which  the  model  used  as  its input.\n",
      "\n",
      "Let's print input_ids to see what it holds inside:\n",
      "\n",
      "```\n",
      "tensor([[ 1, 14350, 385, 4876, 27746, 5281, 304, 19235, 363, 278, 25305, 293, 16423, 292, 286, 728, 481, 29889, 12027, 7420, 920, 372, 9559, 29889, 32001]], device='cuda:0')\n",
      "```\n",
      "\n",
      "This  reveals  the  inputs  that  LLMs  respond  to,  a  series  of  integers  as  shown  in Figure 2-4. Each one is the unique ID for a specific token (character, word, or part of a word). These IDs reference a table inside the tokenizer containing all the tokens it knows.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "If we want to inspect those IDs, we can use the tokenizer's decode method to translate the IDs back into text that we can read:\n",
      "\n",
      "```\n",
      "for id in input_ids: print(tokenizer.decode(id))\n",
      "```\n",
      "\n",
      "This prints (each token is on a separate line):\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "ap . Exp lain how it happened . <|assistant|>\n",
      "```\n",
      "\n",
      "This is how the tokenizer broke down our input prompt. Notice the following:\n",
      "\n",
      " · The first token is ID 1 ( &lt;s&gt; ), a special token indicating the beginning of the text.\n",
      "\n",
      " · Some tokens are complete words (e.g., Write , an email , ).\n",
      "\n",
      " · Some tokens are parts of words (e.g., apolog , izing , trag , ic ).\n",
      "\n",
      " · Punctuation characters are their own token.\n",
      "\n",
      "Notice how the space character does not have its own token. Instead, partial tokens (like 'izing' and 'ic') have a special hidden character at their beginning that indicates that they're connected with the token that precedes them in the text. Tokens without that special character are assumed to have a space before them.\n",
      "\n",
      "On the output side, we can also inspect the tokens generated by the model by printing the generation_output variable.  This shows the input tokens as well as the output tokens (we'll highlight the new tokens in bold):\n",
      "\n",
      "```\n",
      "tensor([[ 1, 14350, 385, 4876, 27746, 5281, 304, 19235, 363, 278, 25305, 293, 16423, 292, 286, 728, 481, 29889, 12027, 7420, 920, 372, 9559, 29889, 32001, 3323 , 622 , 29901 , 1619 , 317, 3742 , 406 , 6225 , 11763 , 363 , 278 , 19906 , 292 , 341 , 728, 481 , 13 , 13 , 29928 , 799 ]], device='cuda:0')\n",
      "```\n",
      "\n",
      "This  shows  us  the  model  generated  the  token  3323, 'Sub' ,  followed  by  token  622, 'ject' .  Together  they  formed  the  word 'Subject' .  They  were  then  followed  by token 29901, which is the colon ':' ...and so on. Just like on the input side, we need the tokenizer on the output side to translate the token ID into the actual text. We do that using the tokenizer's decode method. We can pass it an individual token ID or a list of them:\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "print(tokenizer.decode(3323)) print(tokenizer.decode(622)) print(tokenizer.decode([3323, 622])) print(tokenizer.decode(29901))\n",
      "```\n",
      "\n",
      "This outputs:\n",
      "\n",
      "```\n",
      "Sub ject Subject :\n",
      "```\n",
      "\n",
      "How Does the Tokenizer Break Down Text?\n",
      "\n",
      "There  are  three  major  factors  that  dictate  how  a  tokenizer  breaks  down  an  input prompt.\n",
      "\n",
      "First, at model design time, the creator of the model chooses a tokenization method. Popular methods include byte pair encoding (BPE) (widely used by GPT models) and WordPiece (used by BERT). These methods are similar in that they aim to optimize an efficient set of tokens to represent a text dataset, but they arrive at it in different ways.\n",
      "\n",
      "Second, after choosing the method, we need to make a number of tokenizer design choices like vocabulary size and what special tokens to use. More on this in 'Comparing Trained LLM Tokenizers' on page 46.\n",
      "\n",
      "Third,  the  tokenizer  needs  to  be  trained  on  a  specific  dataset  to  establish  the  best vocabulary  it  can  use  to  represent  that  dataset.  Even  if  we  set  the  same  methods and parameters, a tokenizer trained on an English text dataset will be different from another trained on a code dataset or a multilingual text dataset.\n",
      "\n",
      "In addition to being used to process the input text into a language model, tokenizers are used on the output of the language model to turn the resulting token ID into the output word or token associated with it, as Figure 2-5 shows.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Word Versus Subword Versus Character Versus Byte Tokens\n",
      "\n",
      "The  tokenization  scheme  we  just  discussed  is  called subword  tokenization .  It' s  the most commonly used tokenization scheme but not the only one. The four notable ways to tokenize are shown in Figure 2-6. Let's go over them:\n",
      "\n",
      "Word tokens\n",
      "\n",
      "This  approach  was  common  with  earlier  methods  like  word2vec  but  is  being used less and less in NLP . Its usefulness, however, led it to be used outside of NLP for use cases such as recommendation systems, as we'll see later in the chapter.\n",
      "\n",
      "One  challenge  with  word  tokenization  is  that  the  tokenizer  may  be  unable  to deal with new words that enter the dataset after the tokenizer was trained. This also  results  in  a  vocabulary  that  has  a  lot  of  tokens  with  minimal  differences between  them  (e.g.,  apology,  apologize,  apologetic,  apologist).  This  latter  challenge is resolved by subword tokenization as it has a token for apolog, and then suffix tokens (e.g., -y , -ize , -etic ,  -ist ) that are common with many other tokens, resulting in a more expressive vocabulary.\n",
      "\n",
      "Subword tokens\n",
      "\n",
      "|\n",
      "\n",
      "This  method  contains  full  and  partial  words.  In  addition  to  the  vocabulary expressivity  mentioned  earlier,  another  benefit  of  the  approach  is  its  ability  to represent new words by breaking down the new token into smaller characters, which tend to be a part of the vocabulary.\n",
      "\n",
      "\n",
      "\n",
      "Character tokens\n",
      "\n",
      "This  is  another  method  that  can  deal  successfully  with  new  words  because  it has  the  raw  letters  to  fall  back  on.  While  that  makes  the  representation  easier to tokenize, it makes the modeling more difficult. Where a model with subword tokenization  can  represent  'play'  as  one  token,  a  model  using  character-level tokens  needs  to  model  the  information  to  spell  out  'p-l-a-y'  in  addition  to modeling the rest of the sequence.\n",
      "\n",
      "Subword tokens present an advantage over character tokens in the ability to fit more text within the limited context length of a Transformer model. So with a model with a context length of 1,024, you may be able to fit about three times as much text using subword tokenization than using character tokens (subword tokens often average three characters per token).\n",
      "\n",
      "Byte tokens\n",
      "\n",
      "One  additional  tokenization  method  breaks  down  tokens  into  the  individual bytes that are used to represent unicode characters. Papers like 'CANINE: Pretraining an efficient tokenization-free encoder for language representation' outline methods like this, which are also called 'tokenization-free encoding. ' Other works  like  'ByT5:  Towards  a  token-free  future  with  pre-trained  byte-to-byte\n",
      "\n",
      "|\n",
      "\n",
      "models' show that this can be a competitive method, especially in multilingual scenarios.\n",
      "\n",
      "One  distinction  to  highlight  here:  some  subword  tokenizers  also  include  bytes  as tokens  in  their  vocabulary  as  the  final  building  block  to  fall  back  to  when  they encounter characters they can't otherwise represent. The GPT-2 and RoBERTa tokenizers do this, for example. This doesn't make them tokenization-free byte-level tokenizers, because they don't use these bytes to represent everything, only a subset, as we'll see in the next section.\n",
      "\n",
      "If you want to go deeper into tokenizers, they are discussed in more detail in Designing Large Language Model Applications .\n",
      "\n",
      "Comparing Trained LLM Tokenizers\n",
      "\n",
      "We've  pointed  out  earlier  three  major  factors  that  dictate  the  tokens  that  appear within a tokenizer: the tokenization method, the parameters and special tokens we use  to  initialize  the  tokenizer,  and  the  dataset  the  tokenizer  is  trained  on.  Let's compare and contrast a number of actual, trained tokenizers to see how these choices change  their  behavior.  This  comparison  will  show  us  that  newer  tokenizers  have changed their behavior to improve model performance, and we'll also see how specialized  models  (like  code  generation  models,  for  example)  often  need  specialized tokenizers.\n",
      "\n",
      "We'll use a number of tokenizers to encode the following text:\n",
      "\n",
      "```\n",
      "text = \"\"\" English and CAPITALIZATION 🎵 鸟 show_tokens False None elif == >= else: two tabs:\" \" Three tabs: \"   \" 12.0*50=600 \"\"\"\n",
      "```\n",
      "\n",
      "This will allow us to see how each tokenizer deals with a number of different kinds of tokens:\n",
      "\n",
      " · Capitalization.\n",
      "\n",
      " · Languages other than English.\n",
      "\n",
      " · Emojis.\n",
      "\n",
      " · Programming code with keywords and whitespaces often used for indentation (in languages like Python for example).\n",
      "\n",
      "|\n",
      "\n",
      " · Numbers and digits.\n",
      "\n",
      " · Special tokens. These are unique tokens that have a role other than representing text. They include tokens that indicate the beginning of the text, or the end of the text (which is the way the model signals to the system that it has completed this generation), or other functions as we'll see.\n",
      "\n",
      "Let's go from older to newer tokenizers to see how they tokenize this text and what that might say about the language model. We'll tokenize the text, and then print each token with a color background color using this function:\n",
      "\n",
      "```\n",
      "colors_list = [ '102;194;165', '252;141;98', '141;160;203', '231;138;195', '166;216;84', '255;217;47' ] def show_tokens(sentence, tokenizer_name): tokenizer = AutoTokenizer.from_pretrained(tokenizer_name) token_ids = tokenizer(sentence).input_ids for idx, t in enumerate(token_ids): print( f' \\x1b [0;30;48;2;{colors_list[idx % len(colors_list)]}m' + tokenizer.decode(t) + ' \\x1b [0m', end=' ' )\n",
      "```\n",
      "\n",
      "BERT base model (uncased) (2018)\n",
      "\n",
      "Link to the model on the HuggingFace model hub\n",
      "\n",
      "Tokenization method: WordPiece, introduced in 'Japanese and Korean voice search':\n",
      "\n",
      "Vocabulary size: 30,522\n",
      "\n",
      "Special tokens:\n",
      "\n",
      "unk_token [UNK]\n",
      "\n",
      "An unknown token that the tokenizer has no specific encoding for.\n",
      "\n",
      "sep_token [SEP]\n",
      "\n",
      "A separator that enables certain tasks that require giving the model two texts (in these cases,  the  model  is  called  a  cross-encoder).  One  example  is  reranking,  as we'll see in Chapter 8.\n",
      "\n",
      "pad_token [PAD]\n",
      "\n",
      "A padding token used to pad unused positions in the model's input (as the model expects a certain length of input, its context-size).\n",
      "\n",
      "|\n",
      "\n",
      "cls_token [CLS]\n",
      "\n",
      "A special classification token for classification tasks, as we'll see in Chapter 4.\n",
      "\n",
      "mask_token [MASK]\n",
      "\n",
      "A masking token used to hide tokens during the training process.\n",
      "\n",
      "Tokenized text:\n",
      "\n",
      "[CLS] english and capital ##ization [UNK] [UNK] show _ token ##s false none eli ##f = = &gt; = else : two tab ##s : \" \" three tab ##s : \" \" 12 . 0 * 50 = 600 [SEP]\n",
      "\n",
      "BERT was released in two major flavors: cased (where the capitalization is kept) and uncased  (where  all  capital  letters  are  first  turned  into  small  cap  letters).  With  the uncased (and more popular) version of the BERT tokenizer, we notice the following:\n",
      "\n",
      " · The  newline  breaks  are  gone,  which  makes  the  model  blind  to  information encoded in newlines (e.g., a chat log when each turn is in a new line).\n",
      "\n",
      " · All the text is in lowercase.\n",
      "\n",
      " · The word 'capitalization' is encoded as two subtokens: capital ##ization . The ## characters are used to indicate this token is a partial token connected to the token that precedes it. This is also a method to indicate where the spaces are, as it is assumed tokens without ## in front have a space before them.\n",
      "\n",
      " · The emoji and Chinese characters are gone and replaced with the [UNK] special token indicating an 'unknown token.'\n",
      "\n",
      "BERT base model (cased) (2018)\n",
      "\n",
      "Link to the model on the HuggingFace model hub\n",
      "\n",
      "Tokenization method: WordPiece\n",
      "\n",
      "Vocabulary size: 28,996\n",
      "\n",
      "Special tokens: Same as the uncased version\n",
      "\n",
      "Tokenized text:\n",
      "\n",
      "[CLS] English and CA ##PI ##TA ##L ##I ##Z ##AT ##ION [UNK] [UNK] show _ token ##s F ##als ##e None el ##if = = &gt; = else : two ta ##bs : \" \" Three ta ##bs : \" \" 12 . 0 * 50 = 600 [SEP]\n",
      "\n",
      "The  cased  version  of  the  BERT  tokenizer  differs  mainly  in  including  uppercase tokens.\n",
      "\n",
      " · Notice  how  'CAPITALIZATION' is  now  represented  as  eight  tokens: CA ##PI ##TA ##L ##I ##Z ##AT ##ION .\n",
      "\n",
      "|\n",
      "\n",
      " · Both BERT tokenizers wrap the input within a starting [CLS] token and a closing [SEP] token. [CLS] and [SEP] are  utility  tokens  used  to  wrap  the  input  text and they serve their own purposes. [CLS] stands for classification as it's a token used at times for sentence classification. [SEP] stands for separator, as it's used to separate sentences in some applications that require passing two sentences to a model (For example, in Chapter 8, we will use a [SEP] token to separate the text of the query and a candidate result.)\n",
      "\n",
      "GPT-2 (2019)\n",
      "\n",
      "Link to the model on the HuggingFace model hub\n",
      "\n",
      "Tokenization  method:  Byte  pair  encoding  (BPE),  introduced  in  'Neural  machine translation of rare words with subword units'.\n",
      "\n",
      "Vocabulary size: 50,257\n",
      "\n",
      "Special tokens: &lt;|endoftext|&gt;\n",
      "\n",
      "English and CAP ITAL IZ ATION\n",
      "\n",
      "/uniFFFD /uniFFFD /uniFFFD /uniFFFD /uniFFFD /uniFFFD\n",
      "\n",
      "show _ t ok ens False None el if == &gt;= else : two tabs :\" \" Three tabs : \" \"\n",
      "\n",
      "12 . 0 * 50 = 600\n",
      "\n",
      "With the GPT-2 tokenizer, we notice the following:\n",
      "\n",
      " · The newline breaks are represented in the tokenizer.\n",
      "\n",
      " · Capitalization is preserved, and the word 'CAPITALIZATION' is represented in four tokens.\n",
      "\n",
      " · The 🎵 鸟 characters are now represented by multiple tokens each. While we see these tokens printed as the /uniFFFD character, they actually stand for different tokens. For example, the 🎵 emoji is broken down into the tokens with token IDs 8582, 236, and 113. The tokenizer is successful in reconstructing the original character from these tokens. We can see that by printing tokenizer.decode([8582, 236, 113]) , which prints out 🎵 .\n",
      "\n",
      " · The two tabs are represented as two tokens (token number 197 in that vocabulary) and the four spaces are represented as three tokens (number 220) with the final space being a part of the token for the closing quote character.\n",
      "\n",
      " · The two tabs are represented as two tokens (token number 197 in that vocabulary) and the four spaces are represented as three tokens (number 220) with the final space being a part of the token for the closing quote character.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "What  is  the  significance  of  whitespace  characters?  These  are important  for  models  to  understand  or  generate  code.  A  model that  uses  a  single  token  to  represent  four  consecutive  whitespace characters is more tuned to a Python code dataset. While a model can live with representing it as four different tokens, it does make the  modeling  more  difficult  as  the  model  needs  to  keep  track  of the indentation level, which often leads to worse performance. This is  an  example  of  where  tokenization  choices  can  help  the  model improve on a certain task.\n",
      "\n",
      "Flan-T5 (2022)\n",
      "\n",
      "Tokenization method: Flan-T5 uses a tokenizer implementation called SentencePiece, introduced in 'SentencePiece: A simple and language independent subword tokenizer and  detokenizer  for  neural  text  processing',  which  supports  BPE  and  the unigram language  model (described  in  'Subword  regularization:  Improving  neural  network translation models with multiple subword candidates').\n",
      "\n",
      "Vocabulary size: 32,100\n",
      "\n",
      "Special tokens:\n",
      "\n",
      " · unk_token &lt;unk&gt;\n",
      "\n",
      " · pad_token &lt;pad&gt;\n",
      "\n",
      "Tokenized text:\n",
      "\n",
      "English and CA PI TAL IZ ATION &lt;unk&gt; &lt;unk&gt; show _ to ken s Fal s e None e l if = = &gt; = else : two tab s : \" \" Three tab s : \" \" 12. 0 * 50 = 600 &lt;/s&gt;\n",
      "\n",
      "The  Flan-T5  family  of  models  use    the  SentencePiece  method.  We  notice  the following:\n",
      "\n",
      " · No newline or whitespace tokens; this would make it challenging for the model to work with code.\n",
      "\n",
      " · The emoji and Chinese characters are both replaced by the &lt;unk&gt; token, making the model completely blind to them.\n",
      "\n",
      "GPT-4 (2023)\n",
      "\n",
      "Tokenization method: BPE\n",
      "\n",
      "Vocabulary size: A little over 100,000\n",
      "\n",
      "Special tokens:\n",
      "\n",
      "|\n",
      "\n",
      " · &lt;|endoftext|&gt;\n",
      "\n",
      " · Fill  in  the  middle  tokens.  These  three  tokens  enable  the  LLM  to  generate  a completion given not only the text before it but also considering the text after it.  This  method  is  explained  in  more  detail  in  the  paper  'Efficient  training  of language models to fill in the middle'; its exact details are beyond the scope of this book. These special tokens are:\n",
      "\n",
      " -&lt;|fim_prefix|&gt;\n",
      "\n",
      " -&lt;|fim_middle|&gt;\n",
      "\n",
      " -&lt;|fim_suffix|&gt;\n",
      "\n",
      "Tokenized text:\n",
      "\n",
      "English and CAPITAL IZATION\n",
      "\n",
      "/uniFFFD /uniFFFD /uniFFFD /uniFFFD /uniFFFD /uniFFFD\n",
      "\n",
      "show _tokens False None elif == &gt;= else : two tabs :\"  \" Three tabs : \" \"\n",
      "\n",
      "12 . 0 * 50 = 600\n",
      "\n",
      "The  GPT-4  tokenizer  behaves  similarly  to  its  ancestor,  the  GPT-2  tokenizer.  Some differences are:\n",
      "\n",
      " · The GPT-4 tokenizer represents the four spaces as a single token. In fact, it has a specific token for every sequence of whitespaces up to a list of 83 whitespaces.\n",
      "\n",
      " · The Python keyword elif has its own token in GPT-4. Both this and the previous point stem from the model's focus on code in addition to natural language.\n",
      "\n",
      " · The GPT-4 tokenizer uses fewer tokens to represent most words. Examples here include 'CAPITALIZATION' (two tokens versus four) and 'tokens' (one token versus three).\n",
      "\n",
      " · Refer  back  to  what  we  said  about  the  GPT-2  tokenizer  with  regards  to  the  Ł tokens.\n",
      "\n",
      "StarCoder2 (2024)\n",
      "\n",
      "StarCoder2 is a 15-billion parameter model focused on generating code described in the paper 'StarCoder 2 and the stack v2: The next generation', which continues the work from the original StarCoder described in 'StarCoder: May the source be with you!'.\n",
      "\n",
      "Tokenization method: Byte pair encoding (BPE)\n",
      "\n",
      "Vocabulary size: 49,152\n",
      "\n",
      "Example special tokens:\n",
      "\n",
      "|\n",
      "\n",
      " · &lt;|endoftext|&gt;\n",
      "\n",
      " · Fill in the middle tokens:\n",
      "\n",
      " -&lt;fim_prefix&gt;\n",
      "\n",
      " -&lt;fim_middle&gt;\n",
      "\n",
      " -&lt;fim_suffix&gt;\n",
      "\n",
      " -&lt;fim_pad&gt;\n",
      "\n",
      " · When  representing  code,  managing  the  context  is  important.  One  file  might make a function call to a function that is defined in a different file. So the model needs some way of being able to identify code that is in different files in the same code  repository,  while  making  a  distinction  between  code  in  different  repos. That's why StarCoder2 uses special tokens for the name of the repository and the filename:\n",
      "\n",
      " -&lt;filename&gt;\n",
      "\n",
      " -&lt;reponame&gt;\n",
      "\n",
      " -&lt;gh_stars&gt;\n",
      "\n",
      "Tokenized text:\n",
      "\n",
      "English and CAPITAL IZATION\n",
      "\n",
      "/uniFFFD /uniFFFD /uniFFFD /uniFFFD /uniFFFD\n",
      "\n",
      "show _ tokens False None elif == &gt;= else : two tabs :\"  \" Three tabs : \" \"\n",
      "\n",
      "1 2 . 0 * 5 0 = 6 0 0\n",
      "\n",
      "This is an encoder that focuses on code generation:\n",
      "\n",
      " · Similar to GPT-4, it encodes the list of whitespaces as a single token.\n",
      "\n",
      " · A  major  difference  here  to  everything  we've  seen  so  far  is  that  each  digit  is assigned its own token (so 600 becomes 6 0 0 ).  The hypothesis here is that this would lead to better representation of numbers and mathematics. In GPT-2, for example, the number 870 is represented as a single token. But 871 is represented as two tokens ( 8 and 71 ). You can intuitively see how that might be confusing to the model and how it represents numbers.\n",
      "\n",
      "Galactica\n",
      "\n",
      "The Galactica model described in 'Galactica: A large language model for science' is focused on scientific knowledge and is trained on many scientific papers, reference materials, and knowledge bases. It pays extra attention to tokenization that makes it more sensitive to the nuances of the dataset it's representing. For example, it includes\n",
      "\n",
      "|\n",
      "\n",
      "special tokens for citations, reasoning, mathematics, amino acid sequences, and DNA sequences.\n",
      "\n",
      "Tokenization method: Byte pair encoding (BPE)\n",
      "\n",
      "Vocabulary size: 50,000\n",
      "\n",
      "Special tokens:\n",
      "\n",
      " · &lt;s&gt;\n",
      "\n",
      " · &lt;pad&gt;\n",
      "\n",
      " · &lt;/s&gt;\n",
      "\n",
      " · &lt;unk&gt;\n",
      "\n",
      " · References: Citations are wrapped within the two special tokens:\n",
      "\n",
      " -[START_REF]\n",
      "\n",
      " -[END_REF]\n",
      "\n",
      " -One  example of usage from the paper is: Recurrent neural net works, long short-term memory [START_REF]Long Short-Term Memory, Hochreiter[END_REF]\n",
      "\n",
      " · Step-by-step reasoning:\n",
      "\n",
      " -&lt;work&gt; is  an  interesting token that the model uses for chain-of-thought reasoning.\n",
      "\n",
      "Tokenized text:\n",
      "\n",
      "English and CAP ITAL IZATION\n",
      "\n",
      "/uniFFFD /uniFFFD /uniFFFD /uniFFFD /uniFFFD /uniFFFD /uniFFFD\n",
      "\n",
      "show _ tokens False None elif == &gt; = else : two t abs : \"  \" Three t abs : \" \"\n",
      "\n",
      "1 2 . 0 * 5 0 = 6 0 0\n",
      "\n",
      "The Galactica tokenizer behaves similar to StarCoder2 in that it has code in mind. It also encodes whitespaces in the same way: assigning a single token to sequences of whitespace of different lengths. It differs in that it also does that for tabs, though. So from all the tokenizers we've seen so far, it's the only one that assigns a single token to the string made up of two tabs ( '\\t\\t' ).\n",
      "\n",
      "Phi-3 (and Llama 2)\n",
      "\n",
      "The Phi-3 model we look at in this book reuses the tokenizer of Llama 2 yet adds a number of special tokens.\n",
      "\n",
      "|\n",
      "\n",
      "Tokenization method: Byte pair encoding (BPE)\n",
      "\n",
      "Vocabulary size: 32,000\n",
      "\n",
      "Special tokens:\n",
      "\n",
      " · &lt;|endoftext|&gt;\n",
      "\n",
      " · Chat tokens: As chat LLMs rose to popularity in 2023, the conversational nature of  LLMs started to be a leading use case. Tokenizers have been adapted to this direction by the addition of tokens that indicate the turns in a conversation and the roles of each speaker. These special tokens include:\n",
      "\n",
      " -&lt;|user|&gt;\n",
      "\n",
      " -&lt;|assistant|&gt;\n",
      "\n",
      " -&lt;|system|&gt;\n",
      "\n",
      "We can now recap our tour by looking at all these examples side by side:\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Galactica\n",
      "\n",
      "Phi-3 and Llama 2\n",
      "\n",
      "English and CAP ITAL IZATION\n",
      "\n",
      "/uniFFFD /uniFFFD /uniFFFD /uniFFFD /uniFFFD /uniFFFD /uniFFFD\n",
      "\n",
      "show _ tokens False None elif == &gt; = else : two t abs : \"  \" Three t abs : \"  \" 1 2 . 0 * 5 0 = 6 0 0\n",
      "\n",
      "&lt;s&gt;\n",
      "\n",
      "English and C AP IT AL IZ ATION\n",
      "\n",
      "/uniFFFD /uniFFFD /uniFFFD /uniFFFD /uniFFFD /uniFFFD /uniFFFD\n",
      "\n",
      "show _ to kens False None elif == &gt;= else : two tabs :\"  \" Three tabs : \"\n",
      "\n",
      "\"\n",
      "\n",
      "1 2 . 0 * 5 0 = 6 0 0\n",
      "\n",
      "Tokenizer Properties\n",
      "\n",
      "The preceding guided tour of trained tokenizers showed a number of ways in which actual  tokenizers  differ  from  each  other.  But  what  determines  their  tokenization behavior?  There  are  three  major  groups  of  design  choices  that  determine  how  the tokenizer will break down text: the tokenization method, the initialization parameters, and the domain of the data the tokenizer targets.\n",
      "\n",
      "Tokenization methods\n",
      "\n",
      "As we've seen, there are a number of tokenization methods with byte pair encoding (BPE) being the more popular one. Each of these methods outlines an algorithm for how to choose an appropriate set of tokens to represent a dataset. You can find a great overview of all these methods on the Hugging Face page that summarizes tokenizers.\n",
      "\n",
      "Tokenizer parameters\n",
      "\n",
      "After choosing a tokenization method, an LLM designer needs to make some decisions about the parameters of the tokenizer. These include:\n",
      "\n",
      "Vocabulary size\n",
      "\n",
      "How many tokens to keep in the tokenizer's vocabulary? (30K and 50K are often used as vocabulary size values, but more and more we're seeing larger sizes like 100K.)\n",
      "\n",
      "Special tokens\n",
      "\n",
      "What special tokens do we want the model to keep track of? We can add as many of these as we want, especially if we want to build an LLM for special use cases. Common choices include:\n",
      "\n",
      " · Beginning of text token (e.g., &lt;s&gt; )\n",
      "\n",
      " · End of text token\n",
      "\n",
      " · Padding token\n",
      "\n",
      "|\n",
      "\n",
      " · Unknown token\n",
      "\n",
      " · CLS token\n",
      "\n",
      " · Masking token\n",
      "\n",
      "Aside from these, the LLM designer can add tokens that help better model the domain of the problem they're trying to focus on, as we've seen with Galactica's &lt;work&gt; and [START_REF] tokens.\n",
      "\n",
      "Capitalization\n",
      "\n",
      "In  languages  such  as  English,  how  do  we  want  to  deal  with  capitalization? Should  we  convert  everything  to  lowercase?  (Name  capitalization  often  carries useful information, but do we want to waste token vocabulary space on all-caps versions of words?)\n",
      "\n",
      "The domain of the data\n",
      "\n",
      "Even if we select the same method and parameters, tokenizer behavior will be different based on the dataset it was trained on (before we even start model training). The tokenization  methods  mentioned  previously  work  by  optimizing  the  vocabulary  to represent a specific dataset. From our guided tour we've seen how that has an impact on datasets like code and multilingual text.\n",
      "\n",
      "For  code,  for  example,  we've  seen  that  a  text-focused  tokenizer  may  tokenize  the indentation spaces like this (we'll highlight some tokens in color):\n",
      "\n",
      "def add_numbers(a, b):\n",
      "\n",
      "```\n",
      "....\"\"\"Add the two numbers 'a' and 'b'.\"\"\" ....return a + b\n",
      "```\n",
      "\n",
      "This may be suboptimal for a code-focused model. Code-focused models are often improved by making different tokenization choices:\n",
      "\n",
      "```\n",
      "def add_numbers(a, b): ....\"\"\"Add the two numbers 'a' and 'b'.\"\"\" ....return a + b\n",
      "```\n",
      "\n",
      "These tokenization choices make the model's job easier and thus its performance has a higher probability of improving.\n",
      "\n",
      "You can find a more detailed tutorial on training tokenizers in the Tokenizers section of  the  Hugging  Face  course  and  in Natural  Language  Processing  with  Transformers, Revised Edition .\n",
      "\n",
      "|\n",
      "\n",
      "Token Embeddings\n",
      "\n",
      "Now that  we  understand  tokenization,  we  have  solved  one  part  of  the  problem  of representing language to a language model. In this sense, language is a sequence of tokens.  And  if  we  train  a  good-enough  model  on  a  large-enough  set  of  tokens,  it starts to capture the complex patterns that appear in its training dataset:\n",
      "\n",
      " · If  the  training data contains a lot of English text, that pattern reveals itself as a model capable of representing and generating the English language.\n",
      "\n",
      " · If  the  training  data  contains  factual  information  (Wikipedia,  for  example),  the model  would  have  the  ability  to  generate  some  factual  information  (see  the following note).\n",
      "\n",
      "The next piece of  the  puzzle  is  finding  the  best  numerical  representation  for  these tokens  that  the  model  can  use  to  calculate  and  properly  model  the  patterns  in  the text.  These  patterns  reveal  themselves  to  us  as  a  model's  coherence  in  a  specific language, or capability to code, or any of the growing list of capabilities we expect from language models.\n",
      "\n",
      "As  we've  seen  in  Chapter  1,  that  is  what  embeddings  are.  They  are  the  numeric representation space utilized to capture the meanings and patterns in language.\n",
      "\n",
      "\n",
      "\n",
      "Oops:  Achieving  a  good  threshold  of  language  coherence  and better-than-average factual generation, however, starts to present a new problem. Some users start to trust the model's fact generation ability (e.g., at the beginning of 2023 some language models were being  dubbed  'Google  killers').  It  didn't  take  long  for  advanced users  to  recognize  that  generation  models  alone  aren't  reliable search engines. This led to the rise of retrieval-augmented generation (RAG), which combines search and LLMs. We cover RAG in more detail in Chapter 8.\n",
      "\n",
      "A Language Model Holds Embeddings for the Vocabulary of Its Tokenizer\n",
      "\n",
      "After a tokenizer is initialized and trained, it is then used in the training process of its associated language model. This is why a pretrained language model is linked with its tokenizer and can't use a different tokenizer without training.\n",
      "\n",
      "The  language  model  holds  an  embedding  vector  for  each  token  in  the  tokenizer's vocabulary, as we can see in Figure 2-7. When we download a pretrained language model, a portion of the model is this embeddings matrix holding all of these vectors.\n",
      "\n",
      "|\n",
      "\n",
      "Before the beginning of the training process, these vectors are randomly initialized like the rest of the model's weights, but the training process assigns them the values that enable the useful behavior they're trained to perform.\n",
      "\n",
      "\n",
      "\n",
      "Creating Contextualized Word Embeddings with Language Models\n",
      "\n",
      "Now that  we've  covered  token  embeddings  as  the  input  to  a  language  model,  let's look  at  how  language  models  can create better  token  embeddings.  This  is  one  of the  primary  ways  to  use  language  models  for  text  representation.  This  empowers applications like named-entity recognition or extractive text summarization (which summarizes  a  long  text  by  highlighting  the  most  important  parts  of  it,  instead  of generating new text as a summary).\n",
      "\n",
      "Instead  of  representing  each  token  or  word  with  a  static  vector,  language  models create contextualized word embeddings (shown in Figure 2-8) that represent a word with a different token based on its context. These vectors can then be used by other systems for a variety of tasks. In addition to the text applications we mentioned in the previous paragraph, these contextualized vectors, for example, are what powers AI  image  generation  systems  like  DALL·E,  Midjourney,  and  Stable  Diffusion,  for example.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Let's look at how we can generate contextualized word embeddings; the majority of this code should be familiar to you by now:\n",
      "\n",
      "\n",
      "\n",
      "The model we're using here is called DeBERTa v3, which at the time of writing is one of the best-performing language models for token embeddings while being small and highly efficient. It is described in the paper 'DeBERTaV3: Improving DeBERTa using ELECTRA-style pre-training gradient-disentangled embedding sharing'.\n",
      "\n",
      "|\n",
      "\n",
      "This code downloads a pretrained tokenizer and model, then uses them to process the string 'Hello world' . The output of the model is then saved in the output variable. Let's  inspect  that  variable  by  first  printing  its  dimensions  (we  expect  it  to  be  a multidimensional array):\n",
      "\n",
      "```\n",
      "output.shape\n",
      "```\n",
      "\n",
      "```\n",
      "This prints out:\n",
      "```\n",
      "\n",
      "```\n",
      "torch.Size([1, 4, 384])\n",
      "```\n",
      "\n",
      "Skipping the first dimension, we can read this as four tokens, each one embedded in a vector of 384 values. The first dimension is the batch dimension used in cases (like training) when we want to send multiple input sentences to the model at the same time (they're processed at the same time, which speeds up the process).\n",
      "\n",
      "But  what  are  these  four  vectors?  Did  the  tokenizer  break  the  two  words  into  four tokens, or is something else happening here? We can use what we've learned about tokenizers to inspect them:\n",
      "\n",
      "```\n",
      "for token in tokens['input_ids']: print(tokenizer.decode(token))\n",
      "```\n",
      "\n",
      "```\n",
      "This prints out:\n",
      "```\n",
      "\n",
      "```\n",
      "[CLS] Hello world [SEP]\n",
      "```\n",
      "\n",
      "This particular tokenizer and model operate by adding the [CLS] and [SEP] tokens to the beginning and end of a string.\n",
      "\n",
      "Our language model has now processed the text input. The result of its output is the following:\n",
      "\n",
      "```\n",
      "tensor([[ [-3.3060, -0.0507, -0.1098, ..., -0.1704, -0.1618, 0.6932], [ 0.8918, 0.0740, -0.1583, ..., 0.1869, 1.4760, 0.0751], [ 0.0871, 0.6364, -0.3050, ..., 0.4729, -0.1829, 1.0157], [-3.1624, -0.1436, -0.0941, ..., -0.0290, -0.1265, 0.7954] ]], grad_fn=<NativeLayerNormBackward0>)\n",
      "```\n",
      "\n",
      "This is the raw output of a language model. The applications of large language models build on top of outputs like this.\n",
      "\n",
      "We recap the input tokenization and resulting outputs of a language model in Figure 2-9. Technically, the switch from token IDs into raw embeddings is the first step that occurs inside a language model.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "A  visual  like  this  is  essential  for  the  next  chapter  when  we  start  to  look  at  how Transformer-based LLMs work.\n",
      "\n",
      "Text Embeddings (for Sentences and Whole Documents)\n",
      "\n",
      "While token embeddings are key to how LLMs operate, a number of LLM applications require operating on entire sentences, paragraphs, or even text documents. This has led to special language models that produce text embeddings-a single vector that represents a piece of text longer than just one token.\n",
      "\n",
      "We  can  think  of  text  embedding  models  as  taking  a  piece  of  text  and  ultimately producing a single vector that represents that text and captures its meaning in some useful form. Figure 2-10 shows that process.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "There  are  multiple  ways  of  producing  a text embedding  vector.  One  of  the  most common ways is to average the values of all the token embeddings produced by the model. Yet high-quality text embedding models tend to be trained specifically for text embedding tasks.\n",
      "\n",
      "We can produce text embeddings with sentence-transformers ,  a  popular  package for leveraging pretrained embedding models.  The package, like 1 transformers in the previous chapter, can be used to load publicly available models. To illustrate creating embeddings,  we  use  the  all-mpnet-base-v2  model.  Note  that  in  Chapter  4,  we  will further explore how you can choose an embedding model for your task.\n",
      "\n",
      "```\n",
      "from sentence_transformers import SentenceTransformer # Load model model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\") # Convert text to text embeddings vector = model.encode(\"Best movie ever!\")\n",
      "```\n",
      "\n",
      "The number of values, or the dimensions, of the embedding vector depends on the underlying embedding model. Let's explore that for our model:\n",
      "\n",
      "vector.shape\n",
      "\n",
      "```\n",
      "(768,)\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "This sentence is now encoded in this one vector with a dimension of 768 numerical values.  In  Part  II  of  this  book,  once  we  start  looking  at  applications,  we'll  start  to see the immense usefulness of these text embeddings vectors in powering everything from categorization to semantic search to RAG.\n",
      "\n",
      "Word Embeddings Beyond LLMs\n",
      "\n",
      "Embeddings are useful even outside of text and language generation. Embeddings, or assigning meaningful vector representations to objects, turns out to be useful in many domains, including recommender engines and robotics. In this section, we'll look at how to use pretrained word2vec embeddings and touch on how the method creates  word  embeddings.  Seeing  how  word2vec  is  trained  will  prime  you  to  learn about contrastive training in Chapter 10. Then in the following section, we'll see how those embeddings can be used for recommendation systems.\n",
      "\n",
      "Using pretrained Word Embeddings\n",
      "\n",
      "Let's  look  at  how we can download pretrained word embeddings (like word2vec or GloVe) using the Gensim library:\n",
      "\n",
      "```\n",
      "import gensim.downloader as api # Download embeddings (66MB, glove, trained on wikipedia, vector size: 50) # Other options include \"word2vec-google-news-300\" # More options at https://github.com/RaRe-Technologies/gensim-data model = api.load(\"glove-wiki-gigaword-50\")\n",
      "```\n",
      "\n",
      "Here,  we've  downloaded  the  embeddings  of  a  large  number  of  words  trained  on Wikipedia. We can then explore the embedding space by seeing the nearest neighbors of a specific word, 'king' for example:\n",
      "\n",
      "```\n",
      "model.most_similar([model['king']], topn=11)\n",
      "```\n",
      "\n",
      "```\n",
      "This outputs:\n",
      "```\n",
      "\n",
      "```\n",
      "[('king', 1.0000001192092896), ('prince', 0.8236179351806641), ('queen', 0.7839043140411377), ('ii', 0.7746230363845825), ('emperor', 0.7736247777938843), ('son', 0.766719400882721), ('uncle', 0.7627150416374207), ('kingdom', 0.7542161345481873), ('throne', 0.7539914846420288), ('brother', 0.7492411136627197), ('ruler', 0.7434253692626953)]\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "The Word2vec Algorithm and Contrastive Training\n",
      "\n",
      "The word2vec algorithm described in the paper 'Efficient estimation of word representations  in  vector  space'  is  described  in  detail  in  The  Illustrated  Word2vec.  The central ideas are condensed here as we build on them when discussing one method for creating embeddings for recommendation engines in the following section.\n",
      "\n",
      "Just  like  LLMs,  word2vec  is  trained  on  examples  generated  from  text.  Let's  say,  for example, we have the text 'Thou shalt not make a machine in the likeness of a human mind' from the Dune novels by Frank Herbert. The algorithm uses a sliding window to generate training examples. We can, for example, have a window size two, meaning that we consider two neighbors on each side of a central word.\n",
      "\n",
      "The embeddings are generated from a classification task. This task is used to train a  neural  network to predict if words commonly appear in the same context or not ( context here  means in many sentences in the training dataset we're modeling). We can think of this as a neural network that takes two words and outputs 1 if they tend to appear in the same context, and 0 if they do not.\n",
      "\n",
      "In the first position for the sliding window, we can generate four training examples, as we can see in Figure 2-11.\n",
      "\n",
      "\n",
      "\n",
      "In  each  of  the  produced  training  examples,  the  word  in  the  center  is  used  as  one input, and each of its neighbors is a distinct second input in each training example. We expect  the  final  trained  model  to  be  able  to  classify  this  neighbor  relationship and output 1 if the two input words it receives are indeed neighbors. These training examples are visualized in Figure 2-12.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "If,  however,  we  have  a  dataset  of  only  a  target  value  of  1,  then  a  model  can  cheat and ace it  by  outputting  1  all  the  time.  To  get  around  this,  we  need  to  enrich  our training dataset with examples of words that are not typically neighbors. These are called negative examples and are shown in Figure 2-13.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "It  turns  out  that  we  don't  have  to  be  too  scientific  in  how  we  choose  the  negative examples.  A  lot  of  useful  models  result  from  the  simple  ability  to  detect  positive examples from randomly generated examples (inspired by an important idea called noise-contrastive  estimation and  described  in  'Noise-contrastive  estimation:  A  new estimation  principle  for  unnormalized  statistical  models').  So  in  this  case,  we  get random words and add them to the dataset and indicate that they are not neighbors (and thus the model should output 0 when it sees them).\n",
      "\n",
      "With this, we've seen two of the main concepts of word2vec (Figure 2-14): skip-gram, the method of selecting neighboring words, and negative sampling, adding negative examples by random sampling from the dataset.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We  can  generate  millions  and  even  billions  of  training  examples  like  this  from running text. Before proceeding to train a neural network on this dataset, we need to  make  a  couple  of  tokenization  decisions,  which,  just  like  we've  seen  with  LLM tokenizers, include how to deal with capitalization and punctuation and how many tokens we want in our vocabulary.\n",
      "\n",
      "We then create an embedding vector for each token, and randomly initialize them, as can be seen in Figure 2-15. In practice, this is a matrix of dimensions vocab_size x embedding_dimensions .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A  model  is  then  trained  on  each  example  to  take  in  two  embedding  vectors  and predict if they're related or not. We can see what this looks like in Figure 2-16.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Based  on  whether  its  prediction  was  correct  or  not,  the  typical  machine  learning training  step  updates  the  embeddings  so  that  the  next  time  the  model  is  presented with  those  two  vectors,  it  has  a  better  chance  of  being  more  correct.  And  by  the end  of  the  training  process,  we  have  better  embeddings  for  all  the  tokens  in  our vocabulary.\n",
      "\n",
      "This idea of a model that takes two vectors and predicts if they have a certain relation is one of the most powerful ideas in machine learning, and time after time has proven to work very well with language models. This is why we're dedicating Chapter 10 to this  concept and how it optimizes language models for specific tasks (like sentence embeddings and retrieval).\n",
      "\n",
      "The same idea is also central to bridging modalities like text and images, which is key to AI image generation models, as we'll see in Chapter 9 on multimodal models. In that  formulation,  a  model  is  presented  with  an  image  and  a  caption,  and  it  should predict whether that caption describes the image or not.\n",
      "\n",
      "Embeddings for Recommendation Systems\n",
      "\n",
      "As we've mentioned, the concept of embeddings is useful in so many other domains. In industry, it's widely used for recommendation systems, for example.\n",
      "\n",
      "Recommending Songs by Embeddings\n",
      "\n",
      "In this section we'll use the word2vec algorithm to embed songs using human-made music playlists. Imagine if we treated each song as we would a word or token, and we  treated  each  playlist  like  a  sentence.  These  embeddings  can  then  be  used  to recommend similar songs that often appear together in playlists.\n",
      "\n",
      "The dataset we'll use was collected by Shuo Chen from Cornell University. It contains playlists  from  hundreds of radio stations around the US. Figure 2-17 demonstrates this dataset.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Let's demonstrate the end product before we look at how it's built. So let's give it a few songs and see what it recommends in response.\n",
      "\n",
      "Let's start by giving it Michael Jackson's 'Billie Jean, ' the song with ID 3822:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "That  looks  reasonable.  Madonna,  Prince,  and  other  Michael  Jackson  songs  are  the nearest neighbors.\n",
      "\n",
      "Let's  step  away  from  pop  and  into  rap,  and  see  the  neighbors  of  2Pac's  'California Love':\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Another quite reasonable list! Now that we know it works, let's see how to build such a system.\n",
      "\n",
      "Training a Song Embedding Model\n",
      "\n",
      "We'll start by loading the dataset containing the song playlists as well as each song's metadata, such as its title and artist:\n",
      "\n",
      "```\n",
      "import pandas as pd from urllib import request # Get the playlist dataset file data = request.urlopen('https://storage.googleapis.com/maps-premium/data set/yes_complete/train.txt') # Parse the playlist dataset file. Skip the first two lines as # they only contain metadata lines = data.read().decode(\"utf-8\").split(' \\n ')[2:] # Remove playlists with only one song playlists = [s.rstrip().split() for s in lines if len(s.split()) > 1] # Load song metadata songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/data set/yes_complete/song_hash.txt') songs_file = songs_file.read().decode(\"utf-8\").split(' \\n ') songs = [s.rstrip().split(' \\t ') for s in songs_file] songs_df = pd.DataFrame(data=songs, columns = ['id', 'title', 'artist']) songs_df = songs_df.set_index('id') Now that we've saved them, let's inspect the playlists list. Each element inside it is a playlist containing a list of song IDs: print( 'Playlist #1: \\n ', playlists, ' \\n ') print( 'Playlist #2: \\n ', playlists) Playlist #1: ['0', '1', '2', '3', '4', '5', ..., '43'] Playlist #2: ['78', '79', '80', '3', '62', ..., '210'] Let's train the model: from gensim.models import Word2Vec playlists, vector_size=32, window=20, negative=50, min_count=1, workers=4\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "# Train our Word2Vec model model = Word2Vec( )\n",
      "```\n",
      "\n",
      "That takes a minute or two to train and results in embeddings being calculated for each  song  that  we  have.  Now  we  can  use  those  embeddings  to  find  similar  songs exactly as we did earlier with words:\n",
      "\n",
      "```\n",
      "song_id = 2172 # Ask the model for songs similar to song #2172 model.wv.most_similar(positive=str(song_id))\n",
      "```\n",
      "\n",
      "This outputs:\n",
      "\n",
      "```\n",
      "[('2976', 0.9977465271949768), ('3167', 0.9977430701255798), ('3094', 0.9975950717926025), ('2640', 0.9966474175453186), ('2849', 0.9963167905807495)]\n",
      "```\n",
      "\n",
      "That is the list of the songs whose embeddings are most similar to song 2172.\n",
      "\n",
      "In this case, the song is:\n",
      "\n",
      "```\n",
      "print(songs_df.iloc)\n",
      "```\n",
      "\n",
      "```\n",
      "title Fade To Black artist Metallica Name: 2172 , dtype: object\n",
      "```\n",
      "\n",
      "This results in recommendations that are all in the same heavy metal and hard rock genre:\n",
      "\n",
      "```\n",
      "import numpy as np def print_recommendations(song_id): similar_songs = np.array( model.wv.most_similar(positive=str(song_id),topn=5) )[:,0] return songs_df.iloc[similar_songs] # Extract recommendations print_recommendations(2172)\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Summary\n",
      "\n",
      "In this  chapter,  we  have  covered  LLM tokens, tokenizers, and useful approaches to using token embeddings. This prepares us to start looking closer at language models in the next chapter, and also opens the door to learn about how embeddings are used beyond language models.\n",
      "\n",
      "We explored how tokenizers are the first step in processing input to an LLM, transforming  raw  textual  input  into  token  IDs.  Common  tokenization  schemes  include breaking text down into words, subword tokens, characters, or bytes, depending on the specific requirements of a given application.\n",
      "\n",
      "A tour of real-world pretrained tokenizers (from BERT to GPT-2, GPT-4, and other models) showed us areas where some tokenizers are better (e.g., preserving information like capitalization, newlines, or tokens in other languages) and other areas where tokenizers  are  just  different  from  each  other  (e.g.,  how  they  break  down  certain words).\n",
      "\n",
      "Three  of  the  major  tokenizer  design  decisions  are  the  tokenizer  algorithm  (e.g., BPE, WordPiece, SentencePiece), tokenization parameters (including vocabulary size, special tokens, capitalization, treatment of capitalization and different languages), and the dataset the tokenizer is trained on.\n",
      "\n",
      "Language models are also creators of high-quality contextualized token embeddings that  improve  on  raw  static  embeddings.  Those  contextualized  token  embeddings are  what's  used  for  tasks  including  named-entity  recognition  (NER), extractive text summarization, and text classification. In addition to producing token embeddings, language  models  can  produce  text  embeddings  that  cover  entire  sentences  or  even documents. This empowers plenty of applications that will be shown in Part II of this book covering language model applications\n",
      "\n",
      "Before  LLMs,  word  embedding  methods  like  word2vec,  GloVe,  and  fastText  were popular.  In  language  processing,  this  has  largely  been  replaced  with  contextualized word embeddings produced by language models. The word2vec algorithm relies on two  main  ideas:  skip-gram  and  negative  sampling.  It  also  uses  contrastive  training similar to the type we'll see in Chapter 10.\n",
      "\n",
      "Embeddings  are  useful  for  creating  and  improving  recommender  systems  as  we discussed in the music recommender we built from curated song playlists.\n",
      "\n",
      "In the next chapter, we will take a deep dive into the process after tokenization: how does  an  LLM  process  these  tokens  and  generate  text?  We  will  look  at  some  of  the main intuitions of how LLMs that use the Transformer architecture work.\n",
      "\n",
      "|\n",
      "\n",
      "Looking Inside Large Language Models\n",
      "\n",
      "Now that we have a sense of tokenization and embeddings, we're ready to dive deeper into the language model and see how it works. In this chapter, we'll look at some of the main intuitions of how Transformer language models work. Our focus will be on text generation models so we get a deeper sense for generative LLMs in particular.\n",
      "\n",
      "We'll  be  looking  at  both  the  concepts  and  some  code  examples  that  demonstrate them.  Let's  start  by  loading  a  language  model  and  getting  it  ready  for  generation by  declaring  a  pipeline.  In  your  first  read,  feel  free  to  skip  the  code  and  focus  on grasping the concepts involved. Then in a second read, the code will get you to start applying these concepts.\n",
      "\n",
      "```\n",
      "import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # Load model and tokenizer tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") model = AutoModelForCausalLM.from_pretrained( \"microsoft/Phi-3-mini-4k-instruct\", device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code= True , ) # Create a pipeline generator = pipeline( \"text-generation\", model=model, tokenizer=tokenizer, return_full_text= False , max_new_tokens=50, do_sample= False , )\n",
      "```\n",
      "\n",
      "An Overview of Transformer Models\n",
      "\n",
      "Let's begin our exploration with a high-level overview of the model, and then we'll see how later work has improved upon the Transformer model since its introduction in 2017.\n",
      "\n",
      "The Inputs and Outputs of a Trained Transformer LLM\n",
      "\n",
      "The most common picture of understanding the behavior of a Transformer LLM is to think of it as a software system that takes in text and generates text in response. Once a large enough text-in-text-out model is trained on a large enough high-quality dataset, it becomes able to generate impressive and useful outputs. Figure 3-1 shows one such model used to author an email.\n",
      "\n",
      "\n",
      "\n",
      "The model does not generate the text all in one operation; it actually generates one token at a time. Figure 3-2 shows four steps of token generation in response to the input  prompt.  Each  token  generation  step  is  one  forward  pass  through  the  model (that's  machine-learning  speak  for  the  inputs  going  into  the  neural  network  and flowing through the computations it needs to produce an output on the other end of the computation graph).\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "After each token generation, we tweak the input prompt for the next generation step by appending the output token to the end of the input prompt. We can see this in Figure 3-3.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "This gives us a more accurate picture of the model as it is simply predicting the next token based on an input prompt. Software around the neural network basically runs it in a loop to sequentially expand the generated text until completion.\n",
      "\n",
      "There's  a  specific  word  used  in  machine  learning  to  describe  models  that  consume their  earlier  predictions  to  make  later  predictions  (e.g.,  the  model's  first  generated token  is  used  to  generate  the  second  token).  They're  called autoregressive models. That is why you'll hear text generation LLMs being called autoregressive models. This is often used to differentiate text generation models from text representation models like BERT, which are not autoregressive.\n",
      "\n",
      "This  autoregressive,  token-by-token  generation  is  what  happens  under  the  hood when we generate text with the LLM like we see here:\n",
      "\n",
      "```\n",
      "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\" output = generator(prompt) print(output['generated_text']) This generates the text:\n",
      "```\n",
      "\n",
      "```\n",
      "Solution 1: Subject: My Sincere Apologies for the Gardening Mishap Dear Sarah, I hope this message finds you well. I am writing to express my deep\n",
      "```\n",
      "\n",
      "We can see the model begin to write the email starting with the subject. It stopped abruptly because it reached the token limit we established by setting max_new_tokens to 50 tokens. If we increase that, it will continue until concluding the email.\n",
      "\n",
      "The Components of the Forward Pass\n",
      "\n",
      "In  addition  to  the  loop,  two  key  internal  components  are  the  tokenizer  and  the language modeling head (LM head). Figure 3-4 shows where these components lie in the system. We saw in the previous chapter how tokenizers break down the text into a sequence of token IDs that then become the input to the model.\n",
      "\n",
      "The tokenizer is followed by the neural network: a stack of Transformer blocks that do all of the processing. That stack is then followed by the LM head, which translates the output of the stack into probability scores for what the most likely next token is.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Recall from Chapter 2 that the tokenizer contains a table of tokens-the tokenizer's vocabulary . The  model  has  a  vector  representation  associated  with  each  of  these tokens in the vocabulary (token embeddings). Figure 3-5 shows both the vocabulary and associated token embeddings for a model with a vocabulary of 50,000 tokens.\n",
      "\n",
      "\n",
      "\n",
      "The flow of the computation follows the direction of the arrow from top to bottom. For each generated token, the process flows once through each of the Transformer blocks in the stack in order, then to the LM head, which finally outputs the probability distribution for the next token, seen in Figure 3-6.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "The LM head is a simple neural network layer itself.  It  is  one  of  multiple  possible 'heads' to attach to a stack of Transformer blocks to build different kinds of systems. Other kinds of  Transformer  heads  include  sequence  classification  heads  and  token classification heads.\n",
      "\n",
      "We can display the order of the layers by simply printing out the model variable. For this model, we have:\n",
      "\n",
      "```\n",
      "Phi3ForCausalLM( (model): Phi3Model( (embed_tokens): Embedding(32064, 3072, padding_idx=32000) (embed_dropout): Dropout(p=0.0, inplace= False ) (layers): ModuleList( (0-31): 32 x Phi3DecoderLayer( (self_attn): Phi3Attention( (o_proj): Linear(in_features=3072, out_features=3072, bias= False ) (qkv_proj): Linear(in_features=3072, out_features=9216, bias= False ) (rotary_emb): Phi3RotaryEmbedding() ) (mlp): Phi3MLP( (gate_up_proj): Linear(in_features=3072, out_features=16384, bias= False ) (down_proj): Linear(in_features=8192, out_features=3072, bias= False ) (activation_fn): SiLU() ) (input_layernorm): Phi3RMSNorm() (resid_attn_dropout): Dropout(p=0.0, inplace= False ) (resid_mlp_dropout): Dropout(p=0.0, inplace= False ) (post_attention_layernorm): Phi3RMSNorm() ) ) (norm): Phi3RMSNorm() ) (lm_head): Linear(in_features=3072, out_features=32064, bias= False ) )\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "Looking at this structure, we can notice the following highlights:\n",
      "\n",
      " · This shows us the various nested layers of the model. The majority of the model is labeled model , followed by lm_head .\n",
      "\n",
      " · Inside the Phi3Model model, we see the embeddings matrix embed_tokens and its dimensions. It has 32,064 tokens each with a vector size of 3,072.\n",
      "\n",
      " · Skipping  the  dropout  layer  for  now,  we  can  see  the  next  major  component  is the stack of Transformer decoder layers. It contains 32 blocks of type Phi3Deco derLayer .\n",
      "\n",
      " · Each of these Transformer blocks includes an attention layer and a feedforward neural  network  (also  known  as  an mlp or  multilevel  perceptron).  We'll  cover these in more detail later in the chapter.\n",
      "\n",
      " · Finally, we see the lm_head taking a vector of size 3,072 and outputting a vector equivalent to the number of tokens the model knows. That output is the probability score for each token that helps us select the output token.\n",
      "\n",
      "Choosing a Single Token from the Probability Distribution (Sampling/ Decoding)\n",
      "\n",
      "At the end of processing, the output of the model is a probability score for each token in the vocabulary, as we saw previously in Figure 3-6. The method of choosing a single token from the probability distribution is called the decoding strategy . Figure 3-7 shows how this leads to picking the token 'Dear' in one example.\n",
      "\n",
      "The  easiest  decoding  strategy  would  be  to  always  pick  the  token  with  the  highest probability score. In practice, this doesn't tend to lead to the best outputs for most use cases. A better approach is to add some randomness and sometimes choose the second or third highest probability token. The idea here is to basically sample from the probability distribution based on the probability score, as the statisticians would say.\n",
      "\n",
      "What this means for the example in Figure 3-7 is that if the token 'Dear' has a 40% probability of being the next token, then it has a 40% chance of being picked (instead of greedy search, which would pick it directly for having the highest score). So with this  method,  all  the  other  tokens  have  a  chance  of  being  picked  according  to  their score.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Choosing  the  highest  scoring  token  every  time  is  called greedy  decoding .  It' s  what happens  if  you  set  the  temperature  parameter  to  zero  in  an  LLM.  We  cover  the concept of temperature in Chapter 6.\n",
      "\n",
      "Let's look more closely at the code that demonstrates this process. In this code block, we pass the input tokens through the model, and then lm_head :\n",
      "\n",
      "```\n",
      "prompt = \"The capital of France is\" # Tokenize the input prompt input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids # Tokenize the input prompt input_ids = input_ids.to(\"cuda\") # Get the output of the model before the lm_head model_output = model.model(input_ids) # Get the output of the lm_head lm_head_output = model.lm_head(model_output)\n",
      "```\n",
      "\n",
      "Now, lm_head_output is  of  the shape [1, 6, 32064]. We can access the token probability  scores  for  the  last  generated  token  using lm_head_output[0,-1] ,  which  uses the  index  0  across  the  batch  dimension;  the  index  -1  gets  us  the  last  token  in  the sequence. This is now a list of probability scores for all 32,064 tokens. We can get the top scoring token ID, and then decode it to arrive at the text of the generated output token:\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "In this case this turns out to be:\n",
      "\n",
      "Paris\n",
      "\n",
      "Parallel Token Processing and Context Size\n",
      "\n",
      "One of  the  most  compelling  features  of  Transformers  is  that  they  lend  themselves better to parallel computing than previous neural network architectures in language processing. In text generation, we get a first glance at this when looking at how each token is processed. We know from the previous chapter that the tokenizer will break down the text  into  tokens.  Each  of  these  input  tokens  then  flows  through  its  own computation path (that's a good first intuition, at least). We can see these individual processing tracks or streams in Figure 3-8.\n",
      "\n",
      "\n",
      "\n",
      "Current Transformer models have a limit for how many tokens they can process at once. That limit is called the model's context length. A model with 4K context length can only process 4K tokens and would only have 4K of these streams.\n",
      "\n",
      "|\n",
      "\n",
      "Each  of  the  token  streams  starts  with  an  input  vector  (the  embedding  vector  and some positional information; we'll discuss positional embeddings later in the chapter).  At  the  end  of  the  stream,  another  vector  emerges  as  the  result  of  the  model's processing, as shown in Figure 3-9.\n",
      "\n",
      "\n",
      "\n",
      "For text generation, only the output result of the last stream is used to predict the next token. That output vector is the only input into the LM head as it calculates the probabilities of the next token.\n",
      "\n",
      "You may wonder why we go through the trouble of calculating all the token streams if  we're  discarding  the  outputs  of  all  but  the  last  token.  The  answer  is  that  the calculations  of  the  previous  streams  are  required  and  used  in  calculating  the  final stream. Yes, we're not using their final output vector, but we use earlier outputs (in each Transformer block) in the Transformer block's attention mechanism.\n",
      "\n",
      "|\n",
      "\n",
      "If  you're  following  along  with  the  code  examples, recall that the output of lm_head was of the shape [1, 6, 32064]. That was because the input to it was of the shape [1, 6,  3072],  which  is  a  batch  of  one  input  string,  containing  six  tokens,  each  of  them represented by a vector of size 3,072 corresponding to the output vectors after the stack of Transformer blocks.\n",
      "\n",
      "We can access these matrices and view their dimensions by printing:\n",
      "\n",
      "```\n",
      "model_output.shape This outputs: torch.Size([1, 6, 3072]) Similarly, we can print the output of the LM head: lm_head_output.shape This outputs: torch.Size([1, 6, 32064])\n",
      "```\n",
      "\n",
      "Speeding Up Generation by Caching Keys and Values\n",
      "\n",
      "Recall that when generating the second token, we simply append the output token to the input and do another forward pass through the model. If we give the model the ability to cache the results of the previous calculation (especially some of the specific vectors in the attention mechanism), we no longer need to repeat the calculations of the previous streams. This time the only needed calculation is for the last stream. This is an optimization technique called the keys and values (kv) cache and it provides a significant speedup of the generation process. Keys and values are some of the central components of the attention mechanism, as we'll see later in this chapter.\n",
      "\n",
      "Figure  3-10  shows  how  when  generating  the  second  token,  only  one  processing stream is active as we cache the results of the previous streams.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "In  Hugging  Face  Transformers,  cache  is  enabled  by  default.  We  can  disable  it  by setting use_cache to False .  We can see the difference in speed by asking for a long generation, and timing the generation with and without caching:\n",
      "\n",
      "```\n",
      "prompt = \"Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\" # Tokenize the input prompt input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids input_ids = input_ids.to(\"cuda\")\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "Then we time how long it takes to generate 100 tokens with caching. We can use the %%timeit magic command in Jupyter or Colab to time how long the execution takes (it runs the command several times and gets the average):\n",
      "\n",
      "```\n",
      "%%timeit -n 1 # Generate the text generation_output = model.generate( input_ids=input_ids, max_new_tokens=100, use_cache= True )\n",
      "```\n",
      "\n",
      "On a Colab with a T4 GPU, this comes to 4.5 seconds. How long would that take if we disable the cache, however?\n",
      "\n",
      "```\n",
      "%%timeit -n 1 generation_output = model.generate(\n",
      "```\n",
      "\n",
      "```\n",
      "# Generate the text input_ids=input_ids, max_new_tokens=100, use_cache= False )\n",
      "```\n",
      "\n",
      "This comes out to 21.8 seconds. A dramatic difference. In fact, from a user experience standpoint, even the four-second generation time tends to be a long time to wait for a user that's staring at a screen and waiting for an output from the model. This is one reason why LLM APIs stream the output tokens as the model generates them instead of waiting for the entire generation to be completed.\n",
      "\n",
      "Inside the Transformer Block\n",
      "\n",
      "We can now talk about where the vast majority  of  processing  happens:  the  Transformer blocks.  As  Figure  3-11  shows,  Transformer  LLMs  are  composed  of  a  series Transformer blocks (often in the range of six in the original Transformer paper, to over a hundred in many large LLMs). Each block processes its inputs, then passes the results of its processing to the next block.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "A Transformer block (Figure 3-12) is made up of two successive components:\n",
      "\n",
      " 1. The attention layer is mainly concerned with incorporating relevant information from other input tokens and positions\n",
      "\n",
      " 2. The feedforward layer houses the majority of the model's processing capacity\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "The feedforward neural network at a glance\n",
      "\n",
      "A simple example giving the intuition of the feedforward neural network would be if we pass the simple input 'The Shawshank' to a language model, with the expectation that it will generate 'Redemption' as the most probable next word (in reference to the film from 1994).\n",
      "\n",
      "The feedforward neural network (collectively in all  the  model  layers)  is  the  source of this information, as Figure 3-13 shows. When the model was successfully trained to model a massive text archive (which included many mentions of 'The Shawshank Redemption'),  it  learned  and  stored  the  information  (and  behaviors)  that  make  it succeed at this task.\n",
      "\n",
      "\n",
      "\n",
      "For  an  LLM  to  be  successfully  trained,  it  needs  to  memorize  a  lot  of  information. But  it  is  not  simply  a  large  database.  Memorization  is  only  one  ingredient  in  the recipe of impressive text generation. The model is able to use this same machinery to interpolate between data points and more complex patterns to be able to generalizewhich  means  doing  well  on  inputs  it  hadn't  seen  in  the  past  and  were  not  in  its training dataset.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "When  you  use  a  modern  commercial  LLM,  the  outputs  you  get are not the ones mentioned earlier in the strict meaning of a 'language model.' Passing 'The Shawshank' to a chat LLM like GPT-4 produces an output:\n",
      "\n",
      "\"The Shawshank Redemption\" is a 1994 film directed by Frank Darabont and is based on the novella \"Rita Hayworth and Shawshank Redemption\" written by Stephen King. ...etc.\n",
      "\n",
      "This  is  because  raw  language  models  (like  GPT-3)  are  difficult for  people  to  properly  utilize.  This  is  why  the  language  model is  then  trained  on  instruction-tuning  and  human  preference  and feedback  fine-tuning  to  match  people's  expectations  of  what  the model should output.\n",
      "\n",
      "The attention layer at a glance\n",
      "\n",
      "Context  is  vital  in  order  to  properly  model  language.  Simple  memorization  and interpolation  based  on  the  previous  token  can  only  take  us  so  far.  We  know  that because  this  was  one  of  the  leading  approaches  to  build  language  models  before neural networks (see Chapter 3, 'N-gram Language Models' of Speech and Language Processing by Daniel Jurafsky and James H. Martin).\n",
      "\n",
      "Attention is a mechanism that helps the model incorporate context as it's processing a specific token. Think of the following prompt:\n",
      "\n",
      "'The dog chased the squirrel because it '\n",
      "\n",
      "For the model to predict what comes after 'it, ' it needs to know what 'it' refers to. Does it refer to the dog or the squirrel?\n",
      "\n",
      "In a trained Transformer LLM, the attention mechanism makes that determination. Attention adds information from the context into the representation of the 'it' token. We can see a simple version of that in Figure 3-14.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "The model does that based on the patterns seen and learned from the training dataset. Perhaps previous sentences also give more clues, like, for example, referring to the dog as 'she' thus making it clear that 'it' refers to the squirrel.\n",
      "\n",
      "Attention is all you need\n",
      "\n",
      "It  is  worth  diving  deeper  into  the  attention  mechanism.  The  most  stripped-down version of the mechanism is shown in Figure 3-15. It shows multiple token positions going into the attention layer; the final one is the one being currently processed (the pink arrow). The attention mechanism operates on the input vector at that position. It  incorporates relevant information from the context into the vector it produces as the output for that position.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Two main steps are involved in the attention mechanism:\n",
      "\n",
      " 1. A way to score how relevant each of the previous input tokens are to the current token being processed (in the pink arrow).\n",
      "\n",
      " 2. Using those scores, we combine the information from the various positions into a single output vector.\n",
      "\n",
      "Figure 3-16 shows these two steps.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "To  give  the  Transformer  more  extensive  attention  capability,  the  attention  mechanism  is  duplicated  and  executed  multiple  times  in  parallel.  Each  of  these  parallel applications  of  attention  is  conducted  into  an attention  head . This  increases  the model's capacity to model complex patterns in the input sequence that require paying attention to different patterns at once.\n",
      "\n",
      "Figure 3-17 shows the intuition of how attention heads run in parallel with a preceding step of splitting information and a later step of combining the results of all the heads.\n",
      "\n",
      "\n",
      "\n",
      "How attention is calculated\n",
      "\n",
      "Let's look at how attention is calculated inside a single attention head. Before we start the calculation, let's observe the following as the starting position:\n",
      "\n",
      " · The  attention  layer  (of  a  generative  LLM)  is  processing  attention  for  a  single position.\n",
      "\n",
      " · The inputs to the layer are:\n",
      "\n",
      " -The vector representation of the current position or token\n",
      "\n",
      " -The vector representations of the previous tokens\n",
      "\n",
      " · The goal is to produce a new representation of the current position that incorporates relevant information from the previous tokens:\n",
      "\n",
      " -For example, if we're processing the last position in the sentence 'Sarah fed the cat because it, ' we want 'it' to represent the cat-so attention bakes in 'cat information' from the cat token.\n",
      "\n",
      "|\n",
      "\n",
      " · The training process produces three projection matrices that produce the components that interact in this calculation:\n",
      "\n",
      " -A query projection matrix\n",
      "\n",
      " -A key projection matrix\n",
      "\n",
      " -A value projection matrix\n",
      "\n",
      "Figure 3-18 shows the starting position for all of these components before the attention calculations start. For simplicity, let's look at only one attention head because the other heads have identical calculations but with their individual projection matrices.\n",
      "\n",
      "\n",
      "\n",
      "Attention starts by multiplying the inputs by the projection matrices to create three new matrices. These are called the queries, keys, and values matrices. These matrices contain the information of the input tokens projected to three different spaces that help carry out the two steps of attention:\n",
      "\n",
      " 1. Relevance scoring\n",
      "\n",
      " 2. Combining information\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Self-attention: Relevance scoring\n",
      "\n",
      "In a generative Transformer, we're generating one token at a time. This means we're processing one position at a time. So the attention mechanism here is only concerned with this one position, and how information from other positions can be pulled in to inform this position.\n",
      "\n",
      "|\n",
      "\n",
      "The relevance scoring step of attention is conducted by multiplying the query vector of  the  current  position  with  the  keys  matrix.  This  produces  a  score  stating  how relevant each previous token is. Passing that by a softmax operation normalizes these scores so they sum up to 1. Figure 3-20 shows the relevance score resulting from this calculation.\n",
      "\n",
      "\n",
      "\n",
      "Self-attention: Combining information\n",
      "\n",
      "Now that we have the relevance scores, we multiply the value vector associated with each token by that token's score. Summing up those resulting vectors produces the output of this attention step, as we see in Figure 3-21.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Recent Improvements to the Transformer Architecture\n",
      "\n",
      "Since  the  release  of  the  Transformer  architecture,  much  work  has  been  done  to improve it and create better models. This spans training on larger datasets and optimizations for the training process and learning rates to use, but it also extends to the architecture itself. At the time of writing, a lot of the ideas of the original Transformer stand unchanged. There are a few architectural ideas that have proved to be valuable. They contribute to the performance of more recent Transformer models like Llama 2. In this final section of the chapter, we go over a number of the important recent developments of the Transformer architecture.\n",
      "\n",
      "|\n",
      "\n",
      "More Efficient Attention\n",
      "\n",
      "The area that gets the most focus from the research community is the attention layer of the Transformer. This is because the attention calculation is the most computationally expensive part of the process.\n",
      "\n",
      "Local/sparse attention\n",
      "\n",
      "As Transformers started getting larger, ideas like sparse attention ('Generating long sequences  with  sparse  transformers')  and  sliding  window  attention  ('Longformer: The long-document transformer') provided improvements for the efficiency of the attention calculation. Sparse attention limits the context of previous tokens that the model can attend to, as we can see in Figure 3-22.\n",
      "\n",
      "\n",
      "\n",
      "One model that incorporates such a mechanism is GPT-3. But it does not use that for  all  the  Transformer  blocks-the  quality  of  the  generation  would  vastly  degrade if the model could only see a small number of previous tokens. The GPT-3 architecture  interweaved  full-attention  and  efficient-attention  Transformer  blocks.  So  the Transformer blocks alternate between full attention (e.g., blocks 1 and 3) and sparse attention (e.g., blocks 2 and 4).\n",
      "\n",
      "To  demonstrate  different  kinds  of  attention,  review  Figure  3-23,  which  shows  how different attention mechanisms work. Each figure shows which previous tokens (light blue) can be attended to when processing the current token (in dark blue).\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Each row corresponds to a token being processed. The color coding indicates which tokens the model is able to pay attention to while it's processing the token in the dark blue cell. Figure 3-24 describes this with more clarity.\n",
      "\n",
      "\n",
      "\n",
      "This  figure  also  shows  the  autoregressive  nature  of  decoder  Transformer  blocks (which make up most text generation models); they can only pay attention to previous tokens. Contrast this to BERT, which can pay attention to both sides (hence the B in BERT stands for bidirectional).\n",
      "\n",
      "|\n",
      "\n",
      "Multi-query and grouped-query attention\n",
      "\n",
      "A more recent efficient attention tweak to the Transformer is grouped-query attention ('GQA: Training generalized multi-query transformer models from multi-head checkpoints'), which is used by models like Llama 2 and 3. Figure 3-25 shows these different types of attention, and the next section continues to explain them.\n",
      "\n",
      "\n",
      "\n",
      "Grouped-query attention builds on multi-query attention ('Fast transformer decoding: One write-head is all you need'). These methods improve inference scalability of larger models by reducing the size of the matrices involved.\n",
      "\n",
      "Optimizing attention: From multi-head to multi-query to grouped query\n",
      "\n",
      "Earlier in the chapter we showed how the Transformer paper described multi-headed attention. The Illustrated Transformer discusses in detail how the queries, keys, and values matrices are used to conduct the attention operation. Figure 3-26 shows how each 'attention head' has its own distinct query, key, and value matrices calculated for a given input.\n",
      "\n",
      "The  way  that  multi-query  attention  optimizes  this  is  to  share  the  keys  and  values matrices between all the heads. So the only unique matrices for each head would be the queries matrices, as we can see in Figure 3-27.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "As model sizes grow, however, this optimization can be too punishing and we can afford to use a little more memory to improve the quality of the models. This is where grouped-query attention comes in. Instead of cutting the number of keys and values matrices to one of each, it allows us to use more (but less than the number of heads). Figure 3-28 shows these groups and how each group of attention heads shares keys and values matrices.\n",
      "\n",
      "\n",
      "\n",
      "Flash Attention\n",
      "\n",
      "Flash  Attention  is  a  popular  method  and  implementation  that  provides  significant speedups for both training and inference of Transformer LLMs on GPUs. It speeds up the attention calculation by optimizing what values are loaded and moved between a GPU's shared memory (SRAM) and high bandwidth memory (HBM). It is described in  detail  in  the  papers  'FlashAttention:  Fast  and  memory-efficient  exact  attention with IO-awareness' and the subsequent 'FlashAttention-2: Faster attention with better parallelism and work partitioning'.\n",
      "\n",
      "|\n",
      "\n",
      "The Transformer Block\n",
      "\n",
      "Recall that the two major components of a Transformer block are an attention layer and  a  feedforward  neural  network.  A  more  detailed  view  of  the  block  would  also reveal the residual connections and layer-normalization operations that we can see in Figure 3-29.\n",
      "\n",
      "\n",
      "\n",
      "The  latest  Transformer  models  at  the  time  of  this  writing  still  retain  the  major components, yet make a number of tweaks as we can see in Figure 3-30.\n",
      "\n",
      "One of the differences we see in this version of the Transformer block is that normalization  happens  prior  to  attention  and  the  feedforward  layers.  This  has  been reported to reduce the required training time (read: 'On layer normalization in the Transformer  architecture').  Another  improvement  in  normalization  here  is  using RMSNorm,  which  is  simpler  and  more  efficient  than  the  LayerNorm  used  in  the original Transformer (read: 'Root mean square layer normalization'). Lastly, instead of the original Transformer's ReLU activation function, newer variants like SwiGLU (described in 'GLU Variants Improve Transformer') are now more common.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Positional Embeddings (RoPE)\n",
      "\n",
      "Positional  embeddings  have  been  a  key  component  since  the  original  Transformer. They  enable  the  model  to  keep  track  of  the  order  of  tokens/words  in  a  sequence/ sentence,  which  is  an  indispensable  source  of  information  in  language.  From  the many  positional  encoding  schemes  proposed  in  the  past  years,  rotary  positional embeddings  (or  'RoPE,'  introduced  in  'RoFormer:  Enhanced  Transformer  with rotary position embedding') is especially important to point out.\n",
      "\n",
      "The  original  Transformer  paper  and  some  of  the  early  variants  had  absolute  positional embeddings that, in essence, marked the first token as position 1, the second as position 2...etc. These could either be static methods (where the positional vectors are generated using geometric functions) or learned (where the model training assigns them  their  values  during  the  learning  process).  Some  challenges  arise  from  such methods when we scale up models, which requires us to find ways to improve their efficiency.\n",
      "\n",
      "For example, one challenge in efficiently training models with large context is that a lot of documents in the training set are much shorter than that context. It would\n",
      "\n",
      "|\n",
      "\n",
      "be  inefficient  to  allocate  the  entire,  say,  4K  context  to  a  short  10-word  sentence. So during model training, documents are packed together into each context in the training batch, as Figure 3-31 shows.\n",
      "\n",
      "\n",
      "\n",
      "Learn  more  about  packing  by  reading  'Efficient  sequence  packing  without  crosscontamination: Accelerating large language models without impacting performance' and watching the great visuals in 'Introducing packed BERT for 2X training speed-up in natural language processing'.\n",
      "\n",
      "Positional embedding methods have to adapt to this and other practical considerations. If Document 50, for example, starts at position 50, then we' d be misinforming the  model  if  we  tell  it  that  that  first  token  is  number  50  and  that  would  affect its  performance  (because  it  would  assume  there's  previous  context  while  in  reality the  earlier  tokens  belong  to  a  different  and  unrelated  document  the  model  should ignore).\n",
      "\n",
      "Instead  of  the  static,  absolute  embeddings  that  are  added  in  the  beginning  of  the forward pass, rotary embeddings are a method to encode positional information in a way that captures absolute and relative token position information. It is based on the idea of rotating vectors in their embeddings space. In the forward pass, they are added in the attention step, as Figure 3-32 shows.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "During the attention process, the positional information is mixed in specifically to the queries and keys matrices just before we multiply them for relevance scoring, as we can see in Figure 3-33.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Other Architectural Experiments and Improvements\n",
      "\n",
      "Many tweaks of the Transformer are proposed and researched on a continuous basis. ' A  Survey  of  Transformers'  highlights  a  few  of  the  main  directions.  Transformer architectures are also constantly adapted to domains beyond LLMs. Computer vision is an area where a lot of Transformer architecture research is happening (see: 'Transformers in vision: A survey' and ' A survey on vision transformer'). Other domains include  robotics  (see  'Open  X-Embodiment:  Robotic  learning  datasets  and  RT-X models') and time series (see 'Transformers in time series: A survey').\n",
      "\n",
      "|\n",
      "\n",
      "Summary\n",
      "\n",
      "In this chapter we discussed the main intuitions of Transformers and recent developments that enable the latest Transformer LLMs. We went over many new concepts, so let's break down the key concepts that we discussed in this chapter:\n",
      "\n",
      " · A Transformer LLM generates one token at a time.\n",
      "\n",
      " · That output token is appended to the prompt , then this updated prompt is presented to the model again for another forward pass to generate the next token.\n",
      "\n",
      " · The three major components of the Transformer LLM are the tokenizer, a stack of Transformer blocks, and a language modeling head.\n",
      "\n",
      " · The tokenizer contains the token vocabulary for the model. The model has token embeddings associated with those tokens. Breaking the text into tokens and then using  the  embeddings  of  these  tokens  is  the  first  step  in  the  token  generation process.\n",
      "\n",
      " · The forward pass flows through all the stages once, one by one .\n",
      "\n",
      " · Near  the  end  of  the  process,  the  LM  head  scores  the probabilities  of  the  next possible  token . Decoding  strategies  inform  which  actual  token  to  pick  as  the output for this generation step (sometimes it's the most probable next token, but not always).\n",
      "\n",
      " · One  reason  the  Transformer  excels  is  its  ability  to  process  tokens  in  parallel. Each of the input tokens flow into their individual tracks or streams of processing . The number of streams is the model's 'context size' and this represents the max number of tokens the model can operate on.\n",
      "\n",
      " · Because Transformer LLMs loop to generate the text one token at a time, it's a good idea to cache the  processing results of each step so we don't duplicate the processing effort (these results are stored as various matrices within the layers).\n",
      "\n",
      " · The majority of processing happens within Transformer blocks .  These are made up of two components. One of them is the feedforward neural network, which is able to store information and make predictions and interpolations from data it was trained on.\n",
      "\n",
      " · The  second  major  component  of  a  Transformer  block  is  the attention layer. Attention incorporates contextual information to allow the model to better capture the nuance of language.\n",
      "\n",
      " · Attention happens in two major steps: (1) scoring relevance and (2) combining information.\n",
      "\n",
      "|\n",
      "\n",
      " · A Transformer attention layer conducts several attention operations in parallel, each occurring inside an attention head , and their outputs are aggregated to make up the output of the attention layer.\n",
      "\n",
      " · Attention can be accelerated via sharing the keys and values matrices between all heads, or groups of heads ( grouped-query attention ).\n",
      "\n",
      " · Methods  like Flash  Attention speed  up  the  attention  calculation  by  optimizing how the operation is done on the different memory systems of a GPU.\n",
      "\n",
      "Transformers  continue  to  see  new  developments  and  proposed  tweaks  to  improve them  in  different  scenarios,  including  language  models  and  other  domains  and applications.\n",
      "\n",
      "In  Part  II  of  the  book,  we  will  cover  some  of  these  practical  applications  of  LLMs. In Chapter 4, we start with text classification, a common task in Language AI. This next chapter serves as an introduction to applying both generative and representation models.\n",
      "\n",
      "|\n",
      "\n",
      "Using Pretrained Language Models\n",
      "\n",
      "Text Classification\n",
      "\n",
      "A common task in natural language processing is classification. The goal of the task is  to  train  a  model  to  assign  a  label  or  class  to  some  input  text  (see  Figure  4-1). Classifying  text  is  used  across  the  world  for  a  wide  range  of  applications,  from sentiment analysis and intent detection to extracting entities and detecting language. The impact of language models, both representative and generative, on classification cannot be understated.\n",
      "\n",
      "\n",
      "\n",
      "In  this  chapter,  we  will  discuss  several  ways  to  use  language  models  for  classifying text. It will serve as an accessible introduction to using language models that already have been trained. Due to the broad field of text classification, we will discuss several techniques and use them to explore the field of language models:\n",
      "\n",
      " · 'Text Classification with Representation Models' on page 113 demonstrates the flexibility  of  nongenerative  models  for  classification.  We  will  cover  both  taskspecific models and embedding models.\n",
      "\n",
      " · 'Text Classification with Generative Models' on page 127 is an introduction to generative language models as most of them can be used for classification. We will cover both an open source as well as a closed source language model.\n",
      "\n",
      "In this chapter, we will focus on leveraging pretrained language models, models that already have been trained on large amounts of data that can be used for classifying text. As illustrated in Figure 4-2, we will examine both representation and language models and explore their differences.\n",
      "\n",
      "\n",
      "\n",
      "This chapter serves as an introduction to a variety of language models, both generative and nongenerative. We will encounter common packages for loading and using these models.\n",
      "\n",
      "\n",
      "\n",
      "Although this book focuses on LLMs, it is highly advised to compare  these  examples  against  classic,  but  strong  baselines  such  as representing  text  with  TF-IDF  and  training  a  logistic  regression classifier on top of that.\n",
      "\n",
      "The Sentiment of Movie Reviews\n",
      "\n",
      "You can find the data we use to explore techniques for classifying text on the Hugging Face Hub, a platform for hosting models but also data. We will use the well-known 'rotten_tomatoes' dataset to train and evaluate our models.  It contains 5,331 positive 1 and 5,331 negative movie reviews from Rotten Tomatoes.\n",
      "\n",
      "To load this data, we make use of the datasets package, which will be used throughout the book:\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "from datasets import load_dataset # Load our data\n",
      "```\n",
      "\n",
      "```\n",
      "data = load_dataset(\"rotten_tomatoes\") data\n",
      "```\n",
      "\n",
      "```\n",
      "DatasetDict({ train: Dataset({ features: ['text', 'label'], num_rows: 8530 }) validation: Dataset({ features: ['text', 'label'], num_rows: 1066 }) test: Dataset({ features: ['text', 'label'], num_rows: 1066 }) })\n",
      "```\n",
      "\n",
      "The  data  is  split  up  into train , test ,  and validation splits.  Throughout  this  chapter, we  will  use  the  train  split  when  we  train  a  model  and  the  test  split  for  validating the  results.  Note  that  the  additional  validation  split  can  be  used  to  further  validate generalization if you used the train and test splits to perform hyperparameter tuning.\n",
      "\n",
      "Let's take a look at some examples in our train split:\n",
      "\n",
      "```\n",
      "data[\"train\"][0, -1] {'text': ['the rock is destined to be the 21st centurys new \" conan \" and that hes going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .', 'things really get weird , though not particularly scary : the movie is all portent and no content .'], 'label': [1, 0]}\n",
      "```\n",
      "\n",
      "These short reviews are either labeled as positive (1) or negative (0). This means that we will focus on binary sentiment classification.\n",
      "\n",
      "Text Classification with Representation Models\n",
      "\n",
      "Classification with pretrained representation models generally comes in two flavors, either  using  a  task-specific  model  or  an  embedding  model.  As  we  explored  in  the previous chapter, these models are created by fine-tuning a foundation model, like BERT, on a specific downstream task as illustrated in Figure 4-3.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "A task-specific model is a representation model, such as BERT, trained for a specific task,  like  sentiment  analysis.  As  we  explored  in  Chapter  1,  an  embedding  model generates  general-purpose  embeddings  that  can  be  used  for  a  variety  of  tasks  not limited to classification, like semantic search (see Chapter 8).\n",
      "\n",
      "The process of fine-tuning a BERT model for classification is covered in Chapter 11 while creating an embedding model is covered in Chapter 10. In this chapter, we keep both models frozen (nontrainable) and only use their output as shown in Figure 4-4.\n",
      "\n",
      "\n",
      "\n",
      "We will  leverage  pretrained  models  that  others  have  already  fine-tuned  for  us  and explore how they can be used to classify our selected movie reviews.\n",
      "\n",
      "|\n",
      "\n",
      "Model Selection\n",
      "\n",
      "Choosing the  right  models  is  not  as  straightforward  as  you  might  think  with  over 60,000 models on the Hugging Face Hub for text classification and more than 8,000 models  that  generate  embeddings  at  the  moment  of  writing.  Moreover,  it's  crucial to select a model that fits your use case and consider its language compatibility, the underlying architecture, size, and performance.\n",
      "\n",
      "Let's  start  with  the  underlying  architecture.  As  we  explored  in  Chapter  1,  BERT,  a well-known encoder-only architecture, is a popular choice for creating task-specific and embedding models. While generative models, like the GPT family, are incredible models, encoder-only models similarly excel in task-specific use cases and tend to be significantly smaller in size.\n",
      "\n",
      "Over the years, many variations of BERT have been developed, including RoBERTa, 2 DistilBERT,  ALBERT,  and DeBERTa,  each trained in various contexts. You can find 3 4 5 an overview of some well-known BERT-like models in Figure 4-5.\n",
      "\n",
      "\n",
      "\n",
      "Selecting the right model for the job can be a form of art in itself. Trying thousands of pretrained models that can be found on Hugging Face's Hub is not feasible so we need to be efficient with the models that we choose. Having said that, several models\n",
      "\n",
      "|\n",
      "\n",
      "are great starting points and give you an idea of the base performance of these kinds of models. Consider them solid baselines:\n",
      "\n",
      " · BERT base model (uncased)\n",
      "\n",
      " · RoBERTa base model\n",
      "\n",
      " · DistilBERT base model (uncased)\n",
      "\n",
      " · DeBERTa base model\n",
      "\n",
      " · bert-tiny\n",
      "\n",
      " · ALBERT base v2\n",
      "\n",
      "For  the  task-specific  model,  we  are  choosing  the  Twitter-RoBERTa-base  for  Sentiment Analysis model. This is a RoBERTa model fine-tuned on tweets for sentiment analysis. Although this was not trained specifically for movie reviews, it is interesting to explore how this model generalizes.\n",
      "\n",
      "When selecting  models  to  generate  embeddings  from,  the  MTEB  leaderboard  is  a great place to start. It contains open and closed source models benchmarked across several tasks. Make sure to not only take performance into account. The importance of  inference speed should not be underestimated in real-life solutions. As such, we will use sentence-transformers/all-mpnet-base-v2 as the embedding throughout this section. It is a small but performant model.\n",
      "\n",
      "Using a Task-Specific Model\n",
      "\n",
      "Now that we have selected our task-specific representation model, let's start by loading our model:\n",
      "\n",
      "```\n",
      "from transformers import pipeline # Path to our HF model model_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\" # Load model into pipeline pipe = pipeline( model=model_path, tokenizer=model_path, return_all_scores= True , device=\"cuda:0\" )\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "As we load our model, we also load the tokenizer, which is responsible for converting input text into individual tokens, as illustrated in Figure 4-6. Although that parameter is not needed as it is loaded automatically, it illustrates what is happening under the hood.\n",
      "\n",
      "\n",
      "\n",
      "These  tokens  are  at  the  core  of  most  language  models,  as  explored  in  depth  in Chapter 2. A major benefit of these tokens is that they can be combined to generate representations even if they were not in the training data, as shown in Figure 4-7.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "After loading all the necessary components, we can go ahead and use our model on the test split of our data:\n",
      "\n",
      "```\n",
      "import numpy as np from tqdm import tqdm from transformers.pipelines.pt_utils import KeyDataset # Run inference y_pred = [] for output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")), total=len(data[\"test\"])): negative_score = output[\"score\"] positive_score = output[\"score\"] assignment = np.argmax([negative_score, positive_score]) y_pred.append(assignment)\n",
      "```\n",
      "\n",
      "Now that we have generated our predictions, all that is left is evaluation. We create a small function that we can easily use throughout this chapter:\n",
      "\n",
      "```\n",
      "from sklearn.metrics import classification_report def evaluate_performance(y_true, y_pred): \"\"\"Create and print the classification report\"\"\" performance = classification_report( y_true, y_pred, target_names=[\"Negative Review\", \"Positive Review\"]\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "print(performance)\n",
      "\n",
      "Next, let's create our classification report:\n",
      "\n",
      "evaluate_performance(data[\"test\"][\"label\"], y_pred)\n",
      "\n",
      "\n",
      "\n",
      "To  read  the  resulting  classification  report,  let's  first  start  by  exploring  how  we  can identify  correct  and  incorrect  predictions.  There  are  four  combinations  depending on  whether  we  predict  something  correctly  (True)  versus  incorrectly  (False)  and whether we predict the correct class (Positive) versus incorrect class (Negative). We can illustrate  these  combinations  as  a  matrix,  commonly  referred  to  as  a confusion matrix , in Figure 4-8.\n",
      "\n",
      "\n",
      "\n",
      "Using the confusion matrix, we can derive several formulas to describe the quality of the model. In the previously generated classification report we can see four such methods, namely precision , recall , accuracy , and the F1 score:\n",
      "\n",
      " · Precision measures how many of the items found are relevant, which indicates the accuracy of the relevant results.\n",
      "\n",
      " · Recall refers to how many relevant classes were found, which indicates its ability to find all relevant results.\n",
      "\n",
      "|\n",
      "\n",
      " · Accuracy refers  to  how  many  correct  predictions  the  model  makes  out  of  all predictions, which indicates the overall correctness of the model.\n",
      "\n",
      " · The F1  score balances  both  precision  and  recall  to  create  a  model's  overall performance.\n",
      "\n",
      "These  four  metrics  are  illustrated  in  Figure  4-9,  which  describes  them  using  the aforementioned classification report.\n",
      "\n",
      "\n",
      "\n",
      "We will consider the weighted average of the F1 score throughout the examples in this book to make sure each class is treated equally. Our pretrained BERT model gives us an F1 score of 0.80 (we are reading this from the weighted avg row and the f1-score column), which is great for a model not trained specifically on our domain data!\n",
      "\n",
      "To  improve  the  performance  of  our  selected  model,  we  could  do  a  few  different things including selecting a model trained on our domain data, movie reviews in this case, like DistilBERT base uncased finetuned SST-2. We could also shift our focus to another flavor of representation models, namely embedding models.\n",
      "\n",
      "Classification Tasks That Leverage Embeddings\n",
      "\n",
      "In  the  previous  example,  we  used  a  pretrained  task-specific  model  for  sentiment analysis. However, what if we cannot find a model that was pretrained for this specific task? Do we need to fine-tune a representation model ourselves? The answer is no!\n",
      "\n",
      "|\n",
      "\n",
      "There  might  be  times  when  you  want  to  fine-tune  the  model  yourself  if  you  have sufficient computing available (see Chapter 11). However, not everyone has access to extensive computing. This is where general-purpose embedding models come in.\n",
      "\n",
      "Supervised Classification\n",
      "\n",
      "Unlike the previous example, we can perform part of the training process ourselves by approaching it from a more classical perspective. Instead of directly using the representation model for classification, we will use an embedding model for generating features. Those features can then be fed into a classifier, thereby creating a two-step approach as shown in Figure 4-10.\n",
      "\n",
      "\n",
      "\n",
      "A major benefit of this separation is that we do not need to fine-tune our embedding model,  which  can  be  costly.  In  contrast,  we  can  train  a  classifier,  like  a  logistic regression, on the CPU instead.\n",
      "\n",
      "In  the  first  step,  we  convert  our  textual  input  to  embeddings  using  the  embedding model as shown in Figure 4-11. Note that this model is similarly kept frozen and is not updated during the training process.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "We can perform this step with sentence-transformer ,  a  popular package for leveraging pretrained embedding models.  Creating the embeddings is straightforward: 6\n",
      "\n",
      "from sentence_transformers import SentenceTransformer\n",
      "\n",
      "```\n",
      "# Load model model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\") # Convert text to embeddings train_embeddings = model.encode(data[\"train\"][\"text\"], show_progress_bar= True ) test_embeddings = model.encode(data[\"test\"][\"text\"], show_progress_bar= True )\n",
      "```\n",
      "\n",
      "As we covered in Chapter 1, these embeddings are numerical representations of the input text. The number of values, or dimension, of the embedding depends on the underlying embedding model. Let's explore that for our model:\n",
      "\n",
      "train_embeddings.shape\n",
      "\n",
      "```\n",
      "(8530, 768)\n",
      "```\n",
      "\n",
      "This shows that each of our 8,530 input documents has an embedding dimension of 768 and therefore each embedding contains 768 numerical values.\n",
      "\n",
      "In the second step, these embeddings serve as the input features to the classifier illustrated in Figure 4-12. The classifier is trainable and not limited to logistic regression and can take on any form as long as it performs classification.\n",
      "\n",
      "\n",
      "\n",
      "We will keep this step straightforward and use a logistic regression as the classifier. To train it, we only need to use the generated embeddings together with our labels:\n",
      "\n",
      "|\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "# Train a logistic regression on our train embeddings clf = LogisticRegression(random_state=42) clf.fit(train_embeddings, data[\"train\"][\"label\"])\n",
      "\n",
      "Next, let's evaluate our model:\n",
      "\n",
      "# Predict previously unseen instances\n",
      "\n",
      "y_pred = clf.predict(test_embeddings)\n",
      "\n",
      "evaluate_performance(data[\"test\"][\"label\"], y_pred)\n",
      "\n",
      "\n",
      "\n",
      "By training  a  classifier  on  top  of  our  embeddings,  we  managed  to  get  an  F1  score of  0.85!  This  demonstrates the possibilities of training a lightweight classifier while keeping the underlying embedding model frozen.\n",
      "\n",
      "\n",
      "\n",
      "In  this  example,  we  used sentence-transformers to  extract  our embeddings,  which  benefits  from  a  GPU  to  speed  up  inference. However, we can remove this GPU dependency by using an external API to create the embeddings. Popular choices for generating embeddings are Cohere's and OpenAI's offerings. As a result, this would allow the pipeline to run entirely on the CPU.\n",
      "\n",
      "What If We Do Not Have Labeled Data?\n",
      "\n",
      "In our previous example, we had labeled data that we could leverage, but this might not always be the case in practice. Getting labeled data is a resource-intensive task that can require significant human labor. Moreover, is it actually worthwhile to collect these labels?\n",
      "\n",
      "To test this, we can perform zero-shot classification, where we have no labeled data to  explore  whether  the  task  seems  feasible.  Although  we  know  the  definition  of the  labels  (their  names),  we  do  not  have  labeled  data  to  support  them.  Zero-shot classification  attempts  to  predict  the  labels  of  input  text  even  though  it  was  not trained on them, as shown in Figure 4-13.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "To perform zero-shot classification with embeddings, there is a neat trick that we can use. We can describe our labels based on what they should represent. For example, a negative label for movie reviews can be described as 'This is a negative movie review. ' By describing and embedding the labels and documents, we have data that we can work with. This process, as illustrated in Figure 4-14, allows us to generate our own target labels without the need to actually have any labeled data.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "We can create these label embeddings using the .encode function as we did earlier:\n",
      "\n",
      "\n",
      "\n",
      "To assign labels to documents, we can apply cosine similarity to the document label pairs.  This  is  the  cosine  of  the  angle  between  vectors,  which  is  calculated  through the  dot  product  of  the  embeddings  and  divided  by  the  product  of  their  lengths,  as illustrated in Figure 4-15.\n",
      "\n",
      "\n",
      "\n",
      "We can use cosine similarity to check how similar a given document is to the description of the candidate labels. The label with the highest similarity to the document is chosen as illustrated in Figure 4-16.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "To  perform  cosine  similarity  on  the  embeddings,  we  only  need  to  compare  the document embeddings with the label embeddings and get the best matching pairs:\n",
      "\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "\n",
      "# Find the best matching label for each document sim_matrix = cosine_similarity(test_embeddings, label_embeddings) y_pred = np.argmax(sim_matrix, axis=1)\n",
      "\n",
      "And that is it! We only needed to come up with names for our labels to perform our classification tasks. Let's see how well this method works:\n",
      "\n",
      "evaluate_performance(data[\"test\"][\"label\"], y_pred)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If you are familiar with zero-shot classification with Transformerbased models, you might wonder why we choose to illustrate this with  embeddings  instead.  Although  natural  language  inference models are amazing for zero-shot classification, the example here demonstrates  the  flexibility  of  embeddings  for  a  variety  of  tasks. As you will see throughout the book, embeddings can be found in most Language AI use cases and are often an underestimated but incredibly vital component.\n",
      "\n",
      "An F1 score of 0.78 is quite impressive considering we did not use any labeled data at all! This just shows how versatile and useful embeddings are, especially if you are a bit creative with how they are used.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Let's  put  that  creativity  to  the  test.  We  decided  upon  ' A  negative/positive  review'  as  the  name  of  our  labels  but  that  can  be improved.  Instead,  we  can  make  them  a  bit  more  concrete  and specific toward our data by using ' A very negative/positive movie review' instead. This way, the embedding will capture that it is a movie review and will focus a bit more on the extremes of the two labels. Try it out and explore how it affects the results.\n",
      "\n",
      "Text Classification with Generative Models\n",
      "\n",
      "Classification with generative language models, such as OpenAI's GPT models, works a bit differently from what we have done thus far. These models take as input some text and generative text and are thereby aptly named sequence-to-sequence models. This is in stark contrast to our task-specific model, which outputs a class instead, as illustrated in Figure 4-17.\n",
      "\n",
      "\n",
      "\n",
      "These generative models are generally trained on a wide variety of tasks and usually do not perform your use case out of the box. For instance, if we give a generative model a movie review without any context, it has no idea what to do with it.\n",
      "\n",
      "Instead, we need to help it understand the context and guide it toward the answers that we are looking for. As demonstrated in Figure 4-18, this guiding process is done mainly  through  the  instruction,  or prompt ,  that  you  give  such  a  model.  Iteratively improving your prompt to get your preferred output is called prompt engineering .\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "In this section, we will demonstrate how we can leverage different types of generative models to perform classification without our Rotten Tomatoes dataset.\n",
      "\n",
      "Using the Text-to-Text Transfer Transformer\n",
      "\n",
      "Throughout this  book,  we  will  explore  mostly  encoder-only  (representation)  models  like  BERT  and  decoder-only  (generative)  models  like  ChatGPT.  However,  as discussed in Chapter 1, the original Transformer architecture actually consists of an encoder-decoder architecture. Like the decoder-only models, these encoder-decoder models are sequence-to-sequence models and generally fall in the category of generative models.\n",
      "\n",
      "An  interesting  family  of  models  that  leverage  this  architecture  is  the  Text-to-Text Transfer  Transformer  or  T5  model.  Illustrated  in  Figure  4-19,  its  architecture  is similar to the original Transformer where 12 decoders and 12 encoders are stacked together. 7\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "With  this  architecture,  these  models  were  first  pretrained  using  masked  language modeling. In the first step of training, illustrated in Figure 4-20, instead of masking individual tokens, sets of tokens (or token spans ) were masked during pretraining.\n",
      "\n",
      "\n",
      "\n",
      "The  second  step  of  training,  namely  fine-tuning  the  base  model,  is  where  the  real magic happens. Instead of fine-tuning the model for one specific task, each task is converted to a sequence-to-sequence task and trained simultaneously. As illustrated in Figure 4-21, this allows the model to be trained on a wide variety of tasks.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "This method of fine-tuning was extended in the paper 'Scaling instruction-finetuned language models', which introduced more than a thousand tasks during fine-tuning that  more  closely  follow  instructions  as  we  know  them  from  GPT  models.   This 8 resulted in the Flan-T5 family of models that benefit from this large variety of tasks.\n",
      "\n",
      "To  use  this  pretrained  Flan-T5  model  for  classification,  we  will  start  by  loading  it through  the \"text2text-generation\" task,  which  is  generally  reserved  for  these encoder-decoder models:\n",
      "\n",
      "```\n",
      "# Load our model pipe = pipeline( \"text2text-generation\", model=\"google/flan-t5-small\", device=\"cuda:0\" )\n",
      "```\n",
      "\n",
      "The Flan-T5 model comes in various sizes  (flan-t5-small/base/large/xl/xxl)  and  we will use the smallest to speed things up a bit. However, feel free to play around with larger models to see if you can improve the results.\n",
      "\n",
      "Compared to our task-specific model, we cannot just give the model some text and hope it will output the sentiment. Instead, we will have to instruct the model to do so.\n",
      "\n",
      "|\n",
      "\n",
      "Thus, we prefix each document with the prompt 'Is the following sentence positive or negative?':\n",
      "\n",
      "```\n",
      "# Prepare our data prompt = \"Is the following sentence positive or negative? \" data = data.map( lambda example: {\"t5\": prompt + example['text']}) data\n",
      "```\n",
      "\n",
      "```\n",
      "DatasetDict({ train: Dataset({ features: ['text', 'label', 't5'], num_rows: 8530 }) validation: Dataset({ features: ['text', 'label', 't5'], num_rows: 1066 }) test: Dataset({ features: ['text', 'label', 't5'], num_rows: 1066 }) })\n",
      "```\n",
      "\n",
      "After creating our updated data, we can run the pipeline similar to the task-specific example:\n",
      "\n",
      "```\n",
      "# Run inference tqdm(pipe(KeyDataset(data[\"test\"], \"t5\")),\n",
      "```\n",
      "\n",
      "```\n",
      "y_pred = [] for output in total=len(data[\"test\"])): text = output[\"generated_text\"] y_pred.append(0 if text == \"negative\" else 1)\n",
      "```\n",
      "\n",
      "Since this model generates text, we did need to convert the textual output to numerical  values.  The  output  word  'negative'  was  mapped  to  0  whereas  'positive'  was mapped to 1.\n",
      "\n",
      "These numerical values now allow us to test the quality of the model in the same way we have done before:\n",
      "\n",
      "```\n",
      "evaluate_performance(data[\"test\"][\"label\"], y_pred)\n",
      "```\n",
      "\n",
      "```\n",
      "precision    recall  f1-score   support Negative Review       0.83      0.85      0.84       533 Positive Review       0.85      0.83      0.84       533 accuracy                           0.84      1066 macro avg       0.84      0.84      0.84      1066 weighted avg       0.84      0.84      0.84      1066\n",
      "```\n",
      "\n",
      "With an F1 score of 0.84, it is clear this Flan-T5 model is an amazing first look into the capabilities of generative models.\n",
      "\n",
      "|\n",
      "\n",
      "ChatGPT for Classification\n",
      "\n",
      "Although we focus throughout the book on open source models, another major component of the Language AI field is closed sourced models; in particular, ChatGPT.\n",
      "\n",
      "Although  the  underlying  architecture  of  the  original  ChatGPT  model  (GPT-3.5) is  not  shared,  we  can  assume  from  its  name  that  it  is  based  on  the  decoder-only architecture that we have seen in the GPT models thus far.\n",
      "\n",
      "Fortunately,  OpenAI  shared  an  overview  of  the  training  procedure  that  involved an  important  component,  namely  preference  tuning.  As  illustrated  in  Figure  4-22, OpenAI first  manually  created  the  desired  output  to  an  input  prompt  (instruction data) and used that data to create a first variant of its model.\n",
      "\n",
      "\n",
      "\n",
      "OpenAI used the resulting model to generate multiple outputs that were manually ranked  from  best  to  worst.  As  shown  in  Figure  4-23,  this  ranking  demonstrates a  preference  for  certain  outputs  (preference  data)  and  was  used  to  create  its  final model, ChatGPT.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "A  major  benefit  of  using  preference  data  over  instruction  data  is  the  nuance  it represents.  By  demonstrating  the  difference  between  a  good  and  better  output  the generative model learns to generate text that resembles human preference. In Chapter  12,  we  will  explore  how these fine-tuning and preference-tuning methodologies work and how you can perform them yourself.\n",
      "\n",
      "The process of using a closed sourced model is quite different from the open sourced examples  we  have  seen  thus  far.  Instead  of  loading  the  model,  we  can  access  the model through OpenAI's API.\n",
      "\n",
      "Before  we  go  into  the  classification  example,  you  will  first  need  to  create  a  free account  on https://oreil.ly/AEXvA and  create  an  API  key  here: https://oreil.ly/lrTXl . After doing so, you can use your API to communicate with OpenAI's servers.\n",
      "\n",
      "We can use this key to create a client:\n",
      "\n",
      "```\n",
      "import openai # Create client client = openai.OpenAI(api_key=\"YOUR_KEY_HERE\")\n",
      "```\n",
      "\n",
      "Using  this  client,  we  create  the chatgpt_generation function,  which  allows  us  to generate  some  text  based  on  a  specific  prompt,  input  document,  and  the  selected model:\n",
      "\n",
      "```\n",
      "def chatgpt_generation(prompt, document, model=\"gpt-3.5-turbo-0125\"): \"\"\"Generate an output based on a prompt and an input document.\"\"\" messages=[ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\":   prompt.replace(\"[DOCUMENT]\", document) } ] chat_completion = client.chat.completions.create( messages=messages, model=model, temperature=0 ) return chat_completion.choices.message.content\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "Next, we will need to create a template to ask the model to perform the classification:\n",
      "\n",
      "```\n",
      "# Define a prompt template as a base prompt = \"\"\"Predict whether the following document is a positive or negative movie review: [DOCUMENT] If it is positive return 1 and if it is negative return 0. Do not give any other answers. \"\"\" # Predict the target using GPT document = \"unpretentious , charming , quirky , original\" chatgpt_generation(prompt, document)\n",
      "```\n",
      "\n",
      "This template is merely an example and can be changed however you want. For now, we kept it as simple as possible to illustrate how to use such a template.\n",
      "\n",
      "Before  you  use  this  over  a  potentially  large  dataset,  it  is  important  to  always  keep track  of  your  usage.  External  APIs  such  as  OpenAI's  offering  can  quickly  become costly if you perform many requests. At the time of writing, running our test dataset using  the  'gpt-3.5-turbo-0125'  model  costs  3  cents,  which  is  covered  by  the  free account, but this might change in the future.\n",
      "\n",
      "\n",
      "\n",
      "When dealing  with  external  APIs,  you  might  run  into  rate  limit errors. These appear when you call the API too often as some APIs might limit the rate with which you can use it per minute or hour.\n",
      "\n",
      "To  prevent  these  errors,  we  can  implement  several  methods  for retrying the request, including something referred to as exponential backoff .  It  performs  a  short  sleep  each  time  we  hit  a  rate  limit error  and  then  retries  the  unsuccessful  request.  Whenever  it  is unsuccessful again, the sleep length is increased until the request is successful or we hit a maximum number of retries.\n",
      "\n",
      "To use it with OpenAI, there is a great guide that can help you get started.\n",
      "\n",
      "Next, we can run this for all reviews in the test dataset to get its predictions. You can skip this if you want to save your (free) credits for other tasks.\n",
      "\n",
      "```\n",
      "# You can skip this if you want to save your (free) credits predictions = [ chatgpt_generation(prompt, doc) for doc in tqdm(data[\"test\"][\"text\"]) ]\n",
      "```\n",
      "\n",
      "Like the previous example, we need to convert the output from strings to integers to evaluate its performance:\n",
      "\n",
      "|\n",
      "\n",
      "# Extract predictions\n",
      "\n",
      "y_pred = [int(pred) for pred in predictions]\n",
      "\n",
      "# Evaluate performance\n",
      "\n",
      "evaluate_performance(data[\"test\"][\"label\"], y_pred)\n",
      "\n",
      "\n",
      "\n",
      "The F1 score of 0.91 already gives a glimpse into the performance of the model that brought generative AI to the masses. However, since we do not know what data the model was trained on, we cannot easily use these kinds of metrics for evaluating the model. For all we know, it might have actually been trained on our dataset!\n",
      "\n",
      "In  Chapter  12,  we  will  explore  how  we  can  evaluate  both  open  source  and  closed source models on more generalized tasks.\n",
      "\n",
      "Summary\n",
      "\n",
      "In this chapter, we discussed many different techniques for performing a wide variety of classification tasks, from fine-tuning your entire model to no tuning at all! Classifying textual data is not as straightforward as it may seem on the surface and there is an incredible amount of creative techniques for doing so.\n",
      "\n",
      "In this chapter, we explored text classification using both generative and representation  language  models.  Our  goal  was  to  assign  a  label  or  class  to  input  text  for  the classification of a review's sentiment.\n",
      "\n",
      "We  explored  two  types  of  representation  models,  a  task-specific  model  and  an embedding model. The task-specific model was pretrained on a large dataset specifically  for  sentiment  analysis  and  showed  us  that  pretrained  models  are  a  great technique  for  classifying  documents.  The  embedding  model  was  used  to  generate multipurpose embeddings that we used as the input to train a classifier.\n",
      "\n",
      "Similarly,  we  explored  two  types  of  generative  models,  an  open  source  encoderdecoder  model  (Flan-T5)  and  a  closed  source  decoder-only  model  (GPT-3.5).  We used these generative models in text classification without requiring specific (additional) training on domain data or labeled datasets.\n",
      "\n",
      "In the next chapter, we will continue with classification but focus instead on unsupervised classification. What can we do if we have textual data without any labels? What information can we extract? We will focus on clustering our data as well as naming the clusters with topic modeling techniques.\n",
      "\n",
      "|\n",
      "\n",
      "Text Clustering and Topic Modeling\n",
      "\n",
      "Although  supervised  techniques,  such  as  classification,  have  reigned  supreme  over the  last  few  years  in  the  industry,  the  potential  of  unsupervised  techniques  such  as text clustering cannot be understated.\n",
      "\n",
      "Text clustering aims to group similar texts based on their semantic content, meaning, and relationships. As illustrated in Figure 5-1, the resulting clusters of semantically similar  documents  not  only  facilitate  efficient  categorization  of  large  volumes  of unstructured text but also allow for quick exploratory data analysis.\n",
      "\n",
      "\n",
      "\n",
      "The  recent  evolution  of  language  models,  which  enable  contextual  and  semantic representations  of  text,  has  enhanced  the  effectiveness  of  text  clustering.  Language is  more  than  a  bag  of  words,  and  recent  language  models  have  proved  to  be  quite\n",
      "\n",
      "capable of capturing that notion. Text clustering, unbound by supervision, allows for creative solutions and diverse applications, such as finding outliers, speedup labeling, and finding incorrectly labeled data.\n",
      "\n",
      "Text clustering has also found itself in the realm of topic modeling, where we want to discover (abstract) topics that appear in large collections of textual data. As shown in Figure 5-2, we generally describe a topic using keywords or keyphrases and, ideally, have a single overarching label.\n",
      "\n",
      "\n",
      "\n",
      "In  this  chapter,  we  will  first  explore  how  to  perform  clustering  with  embedding models and then transition to a text-clustering-inspired method of topic modeling, namely BERTopic.\n",
      "\n",
      "Text  clustering  and  topic  modeling  have  an  important  role  in  this  book  as  they explore  creative  ways  to  combine  a  variety  of  different  language  models.  We  will explore how combining encoder-only (embeddings), decoder-only (generative), and even  classical  methods  (bag-of-words)  can  result  in  amazing  new  techniques  and pipelines.\n",
      "\n",
      "ArXiv's Articles: Computation and Language\n",
      "\n",
      "Throughout  this  chapter,  we  will  be  running  clustering  and  topic  modeling  algorithms  on  ArXiv  articles.  ArXiv  is  an  open-access  platform  for  scholarly  articles, mostly in the fields of computer science, mathematics, and physics. We will explore articles  in  the  field  of  Computation  and  Language  to  keep  with  the  theme  of  this book.  The  dataset  contains  44,949  abstracts  between  1991  and  2024  from  ArXiv's cs.CL (Computation and Language) section.\n",
      "\n",
      "We load the data and create separate variables for the abstracts, titles, and years of each article:\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "# Load data from Hugging Face from datasets import load_dataset dataset = load_dataset(\"maartengr/arxiv_nlp\")[\"train\"] # Extract metadata abstracts = dataset[\"Abstracts\"] titles = dataset[\"Titles\"]\n",
      "```\n",
      "\n",
      "A Common Pipeline for Text Clustering\n",
      "\n",
      "Text clustering allows for discovering patterns in data that you may or may not be familiar with. It allows for getting an intuitive understanding of the task, for example, a classification task, but also of its complexity. As a result, text clustering can become more than just a quick method for exploratory data analysis.\n",
      "\n",
      "Although there are many methods for text clustering, from graph-based neural networks to centroid-based clustering  techniques,  a  common  pipeline  that  has  gained popularity involves three steps and algorithms:\n",
      "\n",
      " 1. Convert the input documents to embeddings with an embedding model .\n",
      "\n",
      " 2. Reduce the dimensionality of embeddings with a dimensionality reduction model .\n",
      "\n",
      " 3. Find groups of semantically similar documents with a cluster model .\n",
      "\n",
      "Embedding Documents\n",
      "\n",
      "The first step is to convert our textual data to embeddings, as illustrated in Figure 5-3. Recall from previous chapters that embeddings are numerical representations of text that attempt to capture its meaning.\n",
      "\n",
      "\n",
      "\n",
      "Choosing  embedding  models  optimized  for  semantic  similarity  tasks  is  especially important  for  clustering  as  we  attempt  to  find  groups  of  semantically  similar documents.  Fortunately,  most  embedding  models  at  the  time  of  writing  focus  on just that, semantic similarity.\n",
      "\n",
      "|\n",
      "\n",
      "As  we  did  in  the  previous  chapter,  we  will  use  the  MTEB  leaderboard  to  select an  embedding  model.  We  will  need  an  embedding  model  that  has  a  decent  score on  clustering  tasks  but  also  is  small  enough  to  run  quickly.  Instead  of  using  the 'sentence-transformers/all-mpnet-base-v2'  model  we  used  in  the  previous  chapter, we use the 'thenlper/gte-small' model instead. It is a more recent model that outperforms the previous model on clustering tasks and due to its small size is even faster for  inference.  However,  feel  free  to  play  around  with  newer  models  that  have  been released since!\n",
      "\n",
      "```\n",
      "from sentence_transformers import SentenceTransformer # Create an embedding for each abstract embedding_model = SentenceTransformer(\"thenlper/gte-small\") embeddings = embedding_model.encode(abstracts, show_progress_bar= True ) Let's check how many values each document embedding contains: # Check the dimensions of the resulting embeddings\n",
      "```\n",
      "\n",
      "```\n",
      "embeddings.shape (44949, 384)\n",
      "```\n",
      "\n",
      "Each embedding has 384 values that together represent the semantic representation of  the  document.  You  can  view  these  embeddings  as  the  features  that  we  want  to cluster.\n",
      "\n",
      "Reducing the Dimensionality of Embeddings\n",
      "\n",
      "Before we cluster the embeddings, we will first need to take their high dimensionality into account. As the number of dimensions increases, there is an exponential growth in the number of possible values within each dimension. Finding all subspaces within each dimension becomes increasingly complex.\n",
      "\n",
      "As  a  result,  high-dimensional  data  can  be  troublesome  for  many  clustering  techniques as it gets more difficult to identify meaningful clusters. Instead, we can make use  of  dimensionality  reduction.  As  illustrated  in  Figure  5-4,  this  technique  allows us  to  reduce  the  size  of  the  dimensional  space  and  represent  the  same  data  with fewer  dimensions.  Dimensionality  reduction  techniques  aim  to  preserve  the  global structure of high-dimensional data by finding low-dimensional representations.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Note that this is a compression technique and that the underlying algorithm is not arbitrarily removing dimensions. To help the cluster model create meaningful clusters, the second step in our clustering pipeline is therefore dimensionality reduction, as shown in Figure 5-5.\n",
      "\n",
      "\n",
      "\n",
      "Well-known methods for dimensionality reduction are Principal Component Analysis (PCA)  and Uniform Manifold Approximation and Projection (UMAP).  For this 1 2\n",
      "\n",
      "|\n",
      "\n",
      "pipeline, we are going with UMAP as it tends to handle nonlinear relationships and structures a bit better than PCA.\n",
      "\n",
      "\n",
      "\n",
      "Dimensionality  reduction  techniques,  however,  are  not  flawless. They  do  not  perfectly  capture  high-dimensional  data  in  a  lowerdimensional  representation.  Information  will  always  be  lost  with this procedure. There is a balance between reducing dimensionality and keeping as much information as possible.\n",
      "\n",
      "To  perform  dimensionality  reduction,  we  need  to  instantiate  our  UMAP  class  and pass the generated embeddings to it:\n",
      "\n",
      "```\n",
      "from umap import UMAP # We reduce the input embeddings from 384 dimensions to 5 dimensions umap_model = UMAP( n_components=5, min_dist=0.0, metric='cosine', random_state=42 ) reduced_embeddings = umap_model.fit_transform(embeddings)\n",
      "```\n",
      "\n",
      "We  can  use  the n_components parameter  to  decide  the shape  of the lowerdimensional space, namely 5 dimensions. Generally, values between 5 and 10 work well to capture high-dimensional global structures.\n",
      "\n",
      "The min_dist parameter is the minimum distance between embedded points. We are setting this to 0 as that generally results in tighter clusters. We set metric to 'cosine' as Euclidean-based methods have issues dealing with high-dimensional data.\n",
      "\n",
      "Note that setting a random_state in UMAP will make the results reproducible across sessions but will disable parallelism and therefore slow down training.\n",
      "\n",
      "Cluster the Reduced Embeddings\n",
      "\n",
      "The third step is to cluster the reduced embeddings, as illustrated in Figure 5-6.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Although  a  common  choice  is  a  centroid-based  algorithm  like  k-means,  which requires  a  set  of  clusters  to  be  generated,  we  do  not  know  the  number  of  clusters beforehand. Instead, a density-based algorithm freely calculates the number of clusters  and  does  not  force  all  data  points  to  be  part  of  a  cluster,  as  illustrated  in Figure 5-7.\n",
      "\n",
      "\n",
      "\n",
      "A common density-based model is Hierarchical Density-Based Spatial Clustering of Applications  with  Noise  (HDBSCAN).   HDBSCAN  is  a  hierarchical  variation  of  a 3 clustering  algorithm  called  DBSCAN  that  allows  for  dense  (micro)-clusters  to  be found without having to explicitly specify the number of clusters.  As a density-based 4 method, HDBSCAN can also detect outliers in the data, which are data points that do not belong to any cluster. These outliers will not be assigned or forced to belong to any cluster. In other words, they are ignored. Since ArXiv articles might contain some niche papers, using a model that detects outliers could be helpful.\n",
      "\n",
      "As with the previous packages, using HDBSCAN is straightforward. We only need to instantiate the model and pass our reduced embeddings to it:\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "from hdbscan import HDBSCAN # We fit the model and extract the clusters hdbscan_model = HDBSCAN( min_cluster_size=50, metric=\"euclidean\", cluster_selection_method=\"eom\" ).fit(reduced_embeddings) clusters = hdbscan_model.labels_ # How many clusters did we generate? len(set(clusters)) 156\n",
      "```\n",
      "\n",
      "With HDBSCAN, we generated 156 clusters in our dataset. To create more clusters, we will need to reduce the value of min_cluster_size as it represents the minimum size that a cluster can take.\n",
      "\n",
      "Inspecting the Clusters\n",
      "\n",
      "Now that we have generated our clusters, we can inspect each cluster manually and explore the assigned documents to get an understanding of its content. For example, let us take a few random documents from cluster 0:\n",
      "\n",
      "```\n",
      "import numpy as np # Print first three documents in cluster 0 cluster = 0 for index in np.where(clusters==cluster)[:3]: print(abstracts[index][:300] + \"... \\n \") This works aims to design a statistical machine translation from English text to American Sign Language (ASL). The system is based on Moses tool with some modifications and the results are synthesized through a 3D avatar for interpretation. First, we translate the input text to gloss, a written fo... Researches on signed languages still strongly dissociate lin- guistic issues related on phonological and phonetic aspects, and gesture studies for recognition and synthesis purposes. This paper focuses on the imbrication of motion and meaning for the analysis, synthesis and evaluation of sign lang... Modern computational linguistic software cannot produce important aspects of sign language translation. Using some researches we deduce that the majority of automatic sign language translation systems ignore many aspects when they generate animation; therefore the interpretation lost the truth inf...\n",
      "```\n",
      "\n",
      "From these documents, it seems that this cluster contains documents mostly about translation from and to sign language, interesting!\n",
      "\n",
      "We can take this one step further and attempt to visualize our results instead of going through  all  documents  manually.  To  do  so,  we  will  need  to  reduce  our  document embeddings to two dimensions, as that allows us to plot the documents on an x/y plane:\n",
      "\n",
      "|\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "```\n",
      "# Reduce 384-dimensional embeddings to two dimensions for easier visualization\n",
      "```\n",
      "\n",
      "```\n",
      "reduced_embeddings = UMAP( n_components=2, min_dist=0.0, metric=\"cosine\", random_state=42 ).fit_transform(embeddings) # Create dataframe df = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"]) df[\"title\"] = titles df[\"cluster\"] = [str(c) for c in clusters] # Select outliers and non-outliers (clusters) to_plot = df.loc[df.cluster != \"-1\", :] outliers = df.loc[df.cluster == \"-1\", :]\n",
      "```\n",
      "\n",
      "We also created a dataframe for our clusters ( clusters_df ) and for the outliers ( out liers_df )  separately since we generally want to focus on the clusters and highlight those.\n",
      "\n",
      "\n",
      "\n",
      "Using  any  dimensionality  reduction  technique  for  visualization purposes  creates  information  loss.  It  is  merely  an  approximation of what our original embeddings look like. Although it is informative,  it  might  push  clusters  together  and  drive  them  further  apart than  they  actually  are.  Human  evaluation,  inspecting  the  clusters ourselves, is therefore a key component of cluster analysis!\n",
      "\n",
      "To generate a static plot, we will use the well-known plotting library, matplotlib :\n",
      "\n",
      "```\n",
      "import matplotlib.pyplot as plt # Plot outliers and non-outliers separately plt.scatter(outliers_df.x, outliers_df.y, alpha=0.05, s=2, c=\"grey\") plt.scatter( clusters_df.x, clusters_df.y, c=clusters_df.cluster.astype(int), alpha=0.6, s=2, cmap=\"tab20b\" ) plt.axis(\"off\")\n",
      "```\n",
      "\n",
      "As we can see in Figure 5-8, it tends to capture major clusters quite well. Note how clusters of points are colored in the same color, indicating that HDBSCAN put them in  a  group  together.  Since  we  have  a  large  number  of  clusters,  the  plotting  library cycles the colors between clusters, so don't think that all green points are one cluster, for example.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "This is visually appealing but does not yet allow us to see what is happening inside the clusters. Instead, we can extend this visualization by going from text clustering to topic modeling.\n",
      "\n",
      "From Text Clustering to Topic Modeling\n",
      "\n",
      "Text  clustering  is  a  powerful  tool  for  finding  structure  among  large  collections  of documents.  In  our  previous  example,  we  could  manually  inspect  each  cluster  and identify  them  based  on  their  collection  of  documents.  For  instance,  we  explored  a cluster that contained documents about sign language. We could say that the topic of that cluster is 'sign language. '\n",
      "\n",
      "This  idea  of  finding  themes  or  latent  topics  in  a  collection  of  textual  data  is  often referred  to  as topic  modeling .  Traditionally,  it  involves  finding  a  set  of  keywords  or phrases that best represent and capture the meaning of the topic, as we illustrate in Figure 5-9.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Instead of labeling a topic as 'sign language, ' these techniques use keywords such as 'sign, ' 'language, ' and 'translation' to describe the topic. As such, this does not give a single label to a topic and instead requires the user to understand the meaning of the topic through those keywords.\n",
      "\n",
      "Classic approaches, like latent Dirichlet allocation, assume that each topic is characterized by a probability distribution of words in a corpus's vocabulary.  Figure 5-10 5 demonstrates how each word in a vocabulary is scored against its relevance to each topic.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "These approaches generally use a bag-of-words technique for the main features of the textual data, which does not take the context nor the meaning of words and phrases into account. In contrast, our text clustering example does take both into account as it relies on Transformer-based embeddings that are optimized for semantic similarity and contextual meaning through attention.\n",
      "\n",
      "In  this  section,  we  will  extend  text  clustering  into  the  realm  of  topic  modeling through  a  highly  modular  text  clustering  and  topic  modeling  framework,  namely BERTopic.\n",
      "\n",
      "BERTopic: A Modular Topic Modeling Framework\n",
      "\n",
      "BERTopic is a topic modeling technique that leverages clusters of semantically similar texts to extract various types of topic representations.  The underlying algorithm can 6 be thought of in two steps.\n",
      "\n",
      "First,  as  illustrated  in  Figure  5-11,  we  follow  the  same  procedure  as  we  did  before in  our  text  clustering  example.  We  embed  documents,  reduce  their  dimensionality, and finally  cluster  the  reduced  embedding  to  create  groups  of  semantically  similar documents.\n",
      "\n",
      "\n",
      "\n",
      "Second, it models a distribution over words in the corpus's vocabulary by leveraging a classic method, namely bag-of-words. The bag-of-words, as we discussed briefly in Chapter 1 and illustrate in Figure 5-12, does exactly what its name implies, counting\n",
      "\n",
      "|\n",
      "\n",
      "the number of times each word appears in a document. The resulting representation could be used to extract the most frequent words inside a document.\n",
      "\n",
      "\n",
      "\n",
      "There are  two  caveats,  however.  First,  this  is  a  representation  on  a  document  level and  we  are  interested  in  a  cluster-level  perspective.  To  address  this,  the  frequency of  words  is  calculated  within  the  entire  cluster  instead  of  only  the  document,  as illustrated in Figure 5-13.\n",
      "\n",
      "\n",
      "\n",
      "Second, stop words like 'the' and 'I' tend to appear often in documents and provide little meaning to the actual documents. BERTopic uses a class-based variant of term frequency-inverse document frequency (c-TF-IDF) to put more weight on words that are more meaningful to a cluster and put less weight on words that are used across all clusters.\n",
      "\n",
      "Each word in the bag-of-words, the c-TF in c-TF-IDF, is multiplied by the IDF value of  each  word.  As  shown  in  Figure  5-14,  the  IDF  value  is  calculated  by  taking  the\n",
      "\n",
      "|\n",
      "\n",
      "logarithm of the average frequency of all words across all clusters divided by the total frequency of each word.\n",
      "\n",
      "\n",
      "\n",
      "The  result  is  a  weight  ('IDF')  for  each  word  that  we  can  multiply  with  their  frequency ('c-TF') to get the weighted values ('c-TF-IDF').\n",
      "\n",
      "This  second  part  of  the  procedure,  as  shown  in  Figure  5-15,  allows  for  generating a distribution over words as we have seen before. We can use scikit-learn's CountVec torizer to generate the bag-of-words (or term frequency) representation. Here, each cluster is considered a topic that has a specific ranking of the corpus's vocabulary.\n",
      "\n",
      "\n",
      "\n",
      "Putting the two steps together, clustering and representing topics, results in the full pipeline of BERTopic, as illustrated in Figure 5-16. With this pipeline, we can cluster semantically similar documents and from the clusters generate topics represented by several keywords. The higher a word's weight in a topic, the more representative it is of that topic.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "A major advantage of this pipeline is that the two steps, clustering and topic representation,  are  largely  independent  of  one  another.  For  instance,  with  c-TF-IDF,  we are  not  dependent  on  the  models  used  in  clustering  the  documents.  This  allows for  significant  modularity  throughout  every  component of the pipeline. And as we will  explore  later  in  this  chapter,  it  is  a  great  starting  point  to  fine-tune  the  topic representations.\n",
      "\n",
      "As illustrated in Figure 5-17, although sentence-transformers is used as the default embedding model, we can swap it with any other embedding technique. The same applies to all other steps. If you do not want outliers generated with HDBSCAN, you can use k-means instead.\n",
      "\n",
      "\n",
      "\n",
      "You  can  think  of  this  modularity  as  building  with  Lego  blocks;  each  part  of  the pipeline  is  completely  replaceable  with  another,  similar  algorithm.  Through  this modularity, newly released models can be integrated within its architecture. As the field of Language AI grows, so does BERTopic!\n",
      "\n",
      "|\n",
      "\n",
      "The Modularity of BERTopic\n",
      "\n",
      "The  modularity  of  BERTopic  has  another  advantage:  it  allows  it  to  be  used  and adapted  to  different  use  cases  using  the  same  base  model.  For  instance,  BERTopic supports a wide variety of algorithmic variants:\n",
      "\n",
      " · Guided topic modeling\n",
      "\n",
      " · (Semi-)supervised topic modeling\n",
      "\n",
      " · Hierarchical topic modeling\n",
      "\n",
      " · Dynamic topic modeling\n",
      "\n",
      " · Multimodal topic modeling\n",
      "\n",
      " · Multi-aspect topic modeling\n",
      "\n",
      " · Online and incremental topic modeling\n",
      "\n",
      " · Zero-shot topic modeling\n",
      "\n",
      " · Etc.\n",
      "\n",
      "The modularity and algorithmic flexibility are the foundation of the author's aim to make BERTopic the one-stop-shop for topic modeling. You can find a full overview of its capabilities in the documentation or the repository.\n",
      "\n",
      "To run BERTopic with our ArXiv dataset, we can use our previously defined models and embeddings (although it is not mandatory):\n",
      "\n",
      "```\n",
      "from bertopic import BERTopic # Train our model with our previously defined models topic_model = BERTopic( embedding_model=embedding_model, umap_model=umap_model, hdbscan_model=hdbscan_model, verbose= True ).fit(abstracts, embeddings)\n",
      "```\n",
      "\n",
      "Let us start by exploring the topics that were created. The get_topic_info() method is useful to get a quick description of the topics that we found:\n",
      "\n",
      "```\n",
      "topic_model.get_topic_info()\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Each of these topics is represented by several keywords, which are concatenated with a '_' in the Name column. This Name column allows us to quickly get a feeling of what the topic is about as it shows the four keywords that best represent it.\n",
      "\n",
      "\n",
      "\n",
      "You might also have noticed that the very first topic is labeled -1. That topic contains all documents that could not be fitted within a topic and are considered outliers. This is a result of the clustering algorithm, HDBSCAN, which does not force all points to be clustered. To remove outliers, we could either use a non-outlier algorithm like k-means or use BERTopic's reduce_outliers() function to reassign the outliers to topics.\n",
      "\n",
      "We can inspect individual topics  and  explore  which  keywords  best  represent  them with the get_topic function. For example, topic 0 contains the following keywords:\n",
      "\n",
      "topic_model.get_topic(0)\n",
      "\n",
      "```\n",
      "[('speech', 0.028177697715245358), ('asr', 0.018971184497453525), ('recognition', 0.013457745472471012), ('end', 0.00980445092749381), ('acoustic', 0.009452082794507863), ('speaker', 0.0068822647060204885), ('audio', 0.006807649923681604), ('the', 0.0063343444687017645), ('error', 0.006320144717019838), ('automatic', 0.006290216996043161)]\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "For example, topic 0 contains the keywords 'speech,' 'asr, ' and 'recognition. ' Based on  these  keywords,  it  seems  that  the  topic  is  about  automatic  speech  recognition (ASR).\n",
      "\n",
      "We  can  use  the find_topics() function  to  search  for  specific  topics  based  on  a search term. Let's search for a topic about topic modeling:\n",
      "\n",
      "```\n",
      "topic_model.find_topics(\"topic modeling\") ([22, -1, 1, 47, 32],\n",
      "```\n",
      "\n",
      "```\n",
      "[0.95456535, 0.91173744, 0.9074769, 0.9067007, 0.90510106])\n",
      "```\n",
      "\n",
      "This returns that topic 22 has a relatively high similarity (0.95) with our search term. If we then inspect the topic, we can see that it is indeed a topic about topic modeling:\n",
      "\n",
      "topic_model.get_topic(22)\n",
      "\n",
      "```\n",
      "[('topic', 0.06634619076655907), ('topics', 0.035308535091932707), ('lda', 0.016386314730705634), ('latent', 0.013372311924864435), ('document', 0.012973600191120576), ('documents', 0.012383715497143821), ('modeling', 0.011978375291037142), ('dirichlet', 0.010078277589545706), ('word', 0.008505619415413312), ('allocation', 0.007930890698168108)]\n",
      "```\n",
      "\n",
      "Although we know that this topic is about topic modeling, let's see if the BERTopic abstract is also assigned to this topic:\n",
      "\n",
      "```\n",
      "topic_model.topics_[titles.index(\"BERTopic: Neural topic modeling with a classbased TF-IDF procedure\")] 22\n",
      "```\n",
      "\n",
      "It is! These functionalities allow us to quickly find the topics that we are interested in.\n",
      "\n",
      "\n",
      "\n",
      "The  modularity  of  BERTopic  gives  you  a  lot  of  choices,  which can be overwhelming. For that purpose, the author created a best practices  guide  that  goes  through  common  practices  to  speed  up training, improve representations, and more.\n",
      "\n",
      "To make exploration of the topics a bit easier, we can look back at our text clustering example. There, we created a static visualization to see the general structure of the created topic. With BERTopic, we can create an interactive variant that allows us to quickly explore which topics exist and which documents they contain.\n",
      "\n",
      "Doing so requires us to use the two-dimensional embeddings, reduced_embeddings , that we created with UMAP. Moreover, when we hover over documents, we will show\n",
      "\n",
      "|\n",
      "\n",
      "the title instead of the abstract to quickly get an understanding of the documents in a topic:\n",
      "\n",
      "```\n",
      "# Visualize topics and documents\n",
      "```\n",
      "\n",
      "```\n",
      "fig = topic_model.visualize_documents( titles, reduced_embeddings=reduced_embeddings, width=1200, hide_annotations= True ) # Update fonts of legend for easier visualization fig.update_layout(font=dict(size=16))\n",
      "```\n",
      "\n",
      "As  we  can  see  in  Figure  5-18,  this  interactive  plot  quickly  gives  us  a  sense  of  the created topics. You can zoom in to view individual documents or double-click a topic on the righthand side to only view it.\n",
      "\n",
      "\n",
      "\n",
      "There is a wide range of visualization options in BERTopic. There are three that are worthwhile to explore to get an idea of the relationships between topics:\n",
      "\n",
      "```\n",
      "# Visualize barchart with ranked keywords topic_model.visualize_barchart() # Visualize relationships between topics\n",
      "```\n",
      "\n",
      "# Visualize the potential hierarchical structure of topics topic_model.visualize_hierarchy()\n",
      "\n",
      "Adding a Special Lego Block\n",
      "\n",
      "The pipeline in BERTopic that we have explored thus far, albeit fast and modular, has a disadvantage: it still represents a topic through a bag-of-words without taking into account semantic structures.\n",
      "\n",
      "The solution is to leverage the strength of the bag-of-words representation, which is its  speed  to  generate  a  meaningful representation. We can use this first meaningful representation and tweak it using more powerful but slower techniques, like embedding models. As shown in Figure 5-19, we can rerank the initial distribution of words to improve the resulting representation. Note that this idea of reranking an initial set of results is a main staple in neural search, a subject that we cover in Chapter 8.\n",
      "\n",
      "\n",
      "\n",
      "As a result, we can design a new Lego block, as shown in Figure 5-20, that takes in this first topic representation and spits out an improved representation.\n",
      "\n",
      "In BERTopic, such reranker models are referred to as representation models . A major benefit of this approach is that the optimization of topic representations only needs to  be  done  as  many  times  as  we  have  topics.  For  instance,  if  we  have  millions  of documents and a hundred topics, the representation block only needs to be applied once for every topic instead of for every document.\n",
      "\n",
      "As shown in Figure 5-21, a wide variety of representation blocks have been designed for  BERTopic  that  allows  you  to  fine-tune  the  representations.  The  representation block can even be stacked multiple times to fine-tune representations using different methodologies.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Before we explore how we can use these representation blocks, we first need to do two things. First, we are going to save our original topic representations so that it will be much easier to compare with and without representation models:\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Second, let's create a short wrapper that we can use to quickly visualize the differences in topic words to compare with and without representation models:\n",
      "\n",
      "```\n",
      "def topic_differences(model, original_topics, nr_topics=5): \"\"\"Show the differences in topic representations between two models \"\"\" df = pd.DataFrame(columns=[\"Topic\", \"Original\", \"Updated\"]) for topic in range(nr_topics): # Extract top 5 words per topic per model og_words = \" | \".join(list(zip(*original_topics[topic]))[:5]) new_words = \" | \".join(list(zip(*model.get_topic(topic)))[:5]) df.loc[len(df)] = [topic, og_words, new_words] return df\n",
      "```\n",
      "\n",
      "KeyBERTInspired\n",
      "\n",
      "The  first  representation  block  that  we  are  going  to  explore  is  KeyBERTInspired. KeyBERTInspired is, as you might have guessed, a method inspired by the keyword extraction package, KeyBERT.  KeyBERT extracts keywords from texts by comparing 7 word and document embeddings through cosine similarity.\n",
      "\n",
      "BERTopic uses  a  similar  approach.  KeyBERTInspired  uses  c-TF-IDF  to  extract  the most  representative  documents  per  topic  by  calculating  the  similarity  between  a document's c-TF-IDF values and those of the topic they correspond to. As shown in Figure 5-22, the average document embedding per topic is calculated and compared to the embeddings of candidate keywords to rerank the keywords.\n",
      "\n",
      "\n",
      "\n",
      "Due to the modular nature of BERTopic, we can update our initial topic representations with KeyBERTInspired without needing to perform the dimensionality reduction and clustering steps:\n",
      "\n",
      "|\n",
      "\n",
      "from bertopic.representation import KeyBERTInspired\n",
      "\n",
      "# Update our topic representations using KeyBERTInspired representation_model = KeyBERTInspired() topic_model.update_topics(abstracts, representation_model=representation_model)\n",
      "\n",
      " # Show topic differences\n",
      "\n",
      "topic_differences(topic_model, original_topics)\n",
      "\n",
      "\n",
      "\n",
      "The updated model shows that the topics are easier to read compared to the original model.  It  also  demonstrates  the  downside  of  using  embedding-based  techniques. Words  in  the  original  model,  like nmt (topic  3),  which  stands  for  neural  machine translation,  are  removed  as  the  model  could  not  properly  represent  the  entity.  For domain experts, these abbreviations are highly informative.\n",
      "\n",
      "Maximal marginal relevance\n",
      "\n",
      "With c-TF-IDF and the previously shown KeyBERTInspired techniques, we still have significant  redundancy  in  the  resulting  topic  representations.  For  instance,  having both  the  words  'summaries'  and  'summary'  in  a  topic  representation  introduces redundancy as they are quite similar.\n",
      "\n",
      "We can use maximal marginal relevance (MMR) to diversify our topic representations.  The  algorithm  attempts  to  find  a  set  of  keywords  that  are  diverse  from  one another but still relate to the documents they are compared to. It does so by embedding  a  set  of  candidate  keywords  and  iteratively  calculating  the  next  best  keyword to add. Doing so requires setting a diversity parameter, which indicates how diverse keywords need to be.\n",
      "\n",
      "In  BERTopic,  we  use  MMR  to  go  from  a  set  of  initial  keywords,  let's  say  30,  to  a smaller but more diverse set of keywords, let's say 10. It filters out redundant words and only keeps words that contribute something new to the topic representation.\n",
      "\n",
      "|\n",
      "\n",
      "Doing so is rather straightforward:\n",
      "\n",
      "from bertopic.representation import MaximalMarginalRelevance\n",
      "\n",
      "# Update our topic representations to MaximalMarginalRelevance representation_model = MaximalMarginalRelevance(diversity=0.2) topic_model.update_topics(abstracts, representation_model=representation_model)\n",
      "\n",
      " # Show topic differences\n",
      "\n",
      "topic_differences(topic_model, original_topics)\n",
      "\n",
      "\n",
      "\n",
      "The  resulting topics demonstrate  more  diversity  in  their representations. For instance, topic 4 only shows one 'summary'-like word and instead adds other words that might contribute more to the overall representation.\n",
      "\n",
      "\n",
      "\n",
      "Both  KeyBERTInspired  and  MMR  are  amazing  techniques  for improving the first set of topic representations. KeyBERTInspired especially tends to remove nearly all stop words since it focuses on the semantic relationships between words and documents.\n",
      "\n",
      "The Text Generation Lego Block\n",
      "\n",
      "The representation block in BERTopic has been acting as a reranking block in our previous examples. However, as we already explored in the previous chapter, generative models have great potential for a wide variety of tasks.\n",
      "\n",
      "We can use generative  models  in  BERTopic  quite  efficiently  by  following  a  part  of the reranking procedure. Instead of using a generative model to identify the topic of all  documents, of which there can potentially be millions, we will use the model to generate a label for our topic. As illustrated in Figure 5-23, instead of generating or reranking keywords, we ask the model to generate a short label based on keywords that were previously generated and a small set of representative documents.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "There are two components to the illustrated prompt. First, the documents that are inserted using the [DOCUMENTS] tag  are  a  small  subset  of  documents,  typically  four, that  best  represent  the  topic.  The  documents  with  the  highest  cosine  similarity  of their c-TF-IDF values with those of the topic are selected. Second, the keywords that make up a topic are also passed to the prompt and referenced using the [KEYWORDS] tag.  The keywords could be generated by c-TF-IDF or any of the other representations we discussed thus far.\n",
      "\n",
      "As a result, we only need to use the generative model once for every topic, of which there  could  be  potentially  hundreds,  instead  of  once  for  each  document,  of  which there  could  potentially  be  millions.  There  are  many  generative  models  that  we  can choose from, both open source and proprietary. Let's start with a model that we have explored in the previous chapter, the Flan-T5 model.\n",
      "\n",
      "We create a prompt that works well with the model and use it in BERTopic through the representation_model parameter:\n",
      "\n",
      "```\n",
      "from transformers import pipeline from bertopic.representation import TextGeneration prompt = \"\"\"I have a topic that contains the following documents: [DOCUMENTS] The topic is described by the following keywords: '[KEYWORDS]'. Based on the documents and keywords, what is this topic about?\"\"\" # Update our topic representations using Flan-T5 generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\") representation_model = TextGeneration(\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "generator, prompt=prompt, doc_length=50, tokenizer=\"whitespace\" ) topic_model.update_topics(abstracts, representation_model=representation_model) # Show topic differences topic_differences(topic_model, original_topics)\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Some  of  these  labels,  like  'Summarization'  seem  to  be  logical  when  comparing them to the original representations. Others, however, like 'Science/Tech, ' seem quite broad and do not do the original topic justice. Let's explore instead how OpenAI's GPT-3.5  would  perform  considering  the  model  is  not  only  larger  but  expected  to have more linguistic capabilities:\n",
      "\n",
      "```\n",
      "import openai from bertopic.representation import OpenAI prompt = \"\"\" I have a topic that contains the following documents: [DOCUMENTS] The topic is described by the following keywords: [KEYWORDS] Based on the information above, extract a short topic label in the following format: topic: <short topic label> \"\"\" # Update our topic representations using GPT-3.5 client = openai.OpenAI(api_key=\"YOUR_KEY_HERE\") representation_model = OpenAI( client, model=\"gpt-3.5-turbo\", exponential_backoff= True , chat= True , prompt=prompt ) topic_model.update_topics(abstracts, representation_model=representation_model) # Show topic differences topic_differences(topic_model, original_topics)\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "The  resulting  labels  are  quite  impressive!  We  are  not  even  using  GPT-4  and  the resulting labels seem to be more informative than our previous example. Note that BERTopic is not confined to only using OpenAI's offering but has local backends as well.\n",
      "\n",
      "\n",
      "\n",
      "Although it seems like we do not need the keywords anymore, they are still representative of the input documents. No model is perfect and  it  is  generally  advised  to  generate  multiple  topic  representations. BERTopic allows for all topics to be represented by different representations.  You  could,  for  example,  use  KeyBERTInspired, MMR, and GPT-3.5 side by side to get different perspectives on the same topic.\n",
      "\n",
      "With these GPT-3.5 generated labels, we can create beautiful illustrations using the datamapplot package (Figure 5-24):\n",
      "\n",
      "```\n",
      "# Visualize topics and documents fig = topic_model.visualize_document_datamap( titles, topics=list(range(20)), reduced_embeddings=reduced_embeddings, width=1200, label_font_size=11, label_wrap_width=20, use_medoids= True , )\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Summary\n",
      "\n",
      "In  this  chapter,  we  explored  how  LLMs,  both  generative  and  representative,  can be  used  in  the  domain  of  unsupervised  learning.  Despite  supervised  methods  like classification  being  prevalent  in  recent  years,  unsupervised  approaches  such  as  text clustering hold immense potential due to their ability to group texts based on semantic content without prior labeling.\n",
      "\n",
      "We  covered  a  common  pipeline  for  clustering  textual  documents  that  starts  with converting  input  text  into  numerical  representations,  which  we  call  embeddings. Then,  dimensionality  reduction  is  applied  to  these  embeddings  to  simplify  highdimensional  data  for  better  clustering  outcomes.  Finally,  a  clustering  algorithm  on\n",
      "\n",
      "|\n",
      "\n",
      "the dimensionality-reduced embeddings is applied to cluster the input text. Manually inspecting the clusters  helped  us  understand  which  documents they contained and how to interpret these clusters.\n",
      "\n",
      "To transition away from this manual inspection, we explored how BERTopic extends this text clustering pipeline with a method for automatically representing the clusters. This methodology is often referred to as topic modeling, which attempts to uncover themes within large amounts of documents. BERTopic generates these topic representations through a bag-of-words approach enhanced with c-TF-IDF, which weighs words based on their cluster relevance and frequency across all clusters.\n",
      "\n",
      "A  major  benefit  of  BERTopic  is  its  modular  nature.  In  BERTopic,  you  can  choose any model in the pipeline, which allows for additional representations of topics that create multiple perspectives of the same topic. We explored maximal marginal relevance and KeyBERTInspired as methodologies to fine-tune the topic representations generated with c-TF-IDF. Additionally, we used the same generative LLMs as in the previous  chapter  (Flan-T5  and  GPT-3.5)  to  further  improve  the  interpretability  of topics by generating highly interpretable labels.\n",
      "\n",
      "In the next chapter, we shift focus and explore a common method for improving the output of generative models, namely prompt engineering.\n",
      "\n",
      "|\n",
      "\n",
      "Prompt Engineering\n",
      "\n",
      "In  the  first  chapters  of  this  book,  we  took  our  first  steps  into  the  world  of  large language models (LLMs). We delved into various applications, such as supervised and unsupervised  classification,  employing  models  that  focus  on  representing  text,  like BERT and its derivatives.\n",
      "\n",
      "As we progressed, we used models trained primarily for text generation, models that are often referred to as generative pre-trained transformers (GPT). These models have the remarkable ability to generate text in response to prompts from the user. Through prompt engineering , we can design these prompts in a way that enhances the quality of the generated text.\n",
      "\n",
      "In this chapter, we will explore these generative models in more detail and dive into the realm of prompt engineering, reasoning with generative models, verification, and even evaluating their output.\n",
      "\n",
      "Using Text Generation Models\n",
      "\n",
      "Before we start with the fundamentals of prompt engineering, it is essential to explore the basics of utilizing a text generation model. How do we select the model to use? Do we use a proprietary or open source model? How can we control the generated output? These questions will serve as our stepping stones into using text generation models.\n",
      "\n",
      "Choosing a Text Generation Model\n",
      "\n",
      "Choosing a text generation model starts with choosing between proprietary models or open source models. Although proprietary models are generally more performant, we focus in this book more on open source models as they offer more flexibility and are free to use.\n",
      "\n",
      "Figure 6-1 shows a small selection of impactful foundation models, LLMs that have been  pretrained  on  vast  amounts  of  text  data  and  are  often  fine-tuned  for  specific applications.\n",
      "\n",
      "\n",
      "\n",
      "From  those  foundation  models,  hundreds  if  not  thousands  of  models  have  been fine-tuned, one more suitable for certain tasks than another. Choosing the model to use can be a daunting task!\n",
      "\n",
      "We advise starting with a small foundation model. So let's continue using Phi-3-mini, which  has  3.8  billion  parameters.  This  makes  it  suitable  for  running  with  devices up  to  8  GB  of  VRAM.  Overall,  scaling  up  to  larger  models  tends  to  be  a  nicer experience than scaling down. Smaller models provide a great introduction and lay a solid foundation for progressing to larger models.\n",
      "\n",
      "Loading a Text Generation Model\n",
      "\n",
      "The most straightforward method of loading a model, as we have done in previous chapters, is by leveraging the transformers library:\n",
      "\n",
      "```\n",
      "import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained( \"microsoft/Phi-3-mini-4k-instruct\", device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code= True , ) tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") # Create a pipeline pipe = pipeline( \"text-generation\", model=model,\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "tokenizer=tokenizer, return_full_text= False , max_new_tokens=500, do_sample= False , )\n",
      "```\n",
      "\n",
      "Compared to previous chapters, we will take a closer look at developing and using the prompt template.\n",
      "\n",
      "To  illustrate,  let's  revisit  the  example  from  Chapter  1  where  we  asked  the  LLM  to make a joke about chickens:\n",
      "\n",
      "```\n",
      "# Prompt messages = [ {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"} ] # Generate the output output = pipe(messages) print(output[\"generated_text\"]) Why don't chickens like to go to the gym? Because they can't crack the eggsistence of it!\n",
      "```\n",
      "\n",
      "Under the hood, transformers.pipeline first converts our messages into a specific prompt template. We can explore this process by accessing the underlying tokenizer:\n",
      "\n",
      "```\n",
      "# Apply prompt template prompt = pipe.tokenizer.apply_chat_template(messages, tokenize= False print(prompt)\n",
      "```\n",
      "\n",
      "```\n",
      ") <s><|user|> Create a funny joke about chickens.<|end|> <|assistant|>\n",
      "```\n",
      "\n",
      "You may recognize the special tokens &lt;|user|&gt; and &lt;|assistant|&gt; from Chapter 2. This prompt template, further illustrated in Figure 6-2, was used during the training of the model. Not only does it provide information about who said what, but it is also used to indicate when the model should stop generating text (see the &lt;|end|&gt; token). This prompt is passed directly to the LLM and processed all at once.\n",
      "\n",
      "In the next chapter, we will customize parts of this template ourselves. Throughout this chapter, we can use transformers.pipeline to handle chat template processing for us. Next, let us explore how we can control the output of the model.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Controlling Model Output\n",
      "\n",
      "Other  than  prompt  engineering,  we  can  control  the  kind  of  output  we  want  by adjusting  the  model  parameters.  In  our  previous  example,  you  might  have  noticed that  we  used  several  parameters  in  the pipe function,  including temperature and top_p .\n",
      "\n",
      "These parameters control the randomness of the output. A part of what makes LLMs exciting  technology  is  that  it  can  generate  different  responses  for  the  exact  same prompt. Each time an LLM needs to generate a token, it assigns a likelihood number to each possible token.\n",
      "\n",
      "As illustrated in Figure 6-3, in the sentence 'I am driving a… ' the likelihood of that sentence  being  followed  by  tokens  like  'car'  or  'truck'  is  generally  higher  than  a token like 'elephant. ' However, there is still a possibility of 'elephant' being generated but it is much lower.\n",
      "\n",
      "\n",
      "\n",
      "When we loaded our model, we purposefully set do_sample=False to make sure the output is somewhat consistent. This means that no sampling will be done and only\n",
      "\n",
      "|\n",
      "\n",
      "the most probable next token is selected. However, to use the temperature and top_p parameters, we will set do_sample=True in order to make use of them.\n",
      "\n",
      "Temperature\n",
      "\n",
      "The temperature controls  the  randomness  or  creativity  of  the  text  generated.  It defines how likely it is to choose tokens that are less probable. The underlying idea is that a temperature of 0 generates the same response every time because it always chooses the most likely word. As illustrated in Figure 6-4, a higher value allows less probable words to be generated.\n",
      "\n",
      "\n",
      "\n",
      "As a result, a higher temperature (e.g., 0.8) generally results in a more diverse output while a lower temperature (e.g., 0.2) creates a more deterministic output.\n",
      "\n",
      "You can use temperature in your pipeline as follows:\n",
      "\n",
      "\n",
      "\n",
      "Why don't chickens like to go on a rollercoaster? Because they're afraid they\n",
      "\n",
      "might suddenly become chicken-soup!\n",
      "\n",
      "Note that every time you rerun this piece of code, the output will change! tempera ture introduces stochastic behavior since the model now randomly selects tokens.\n",
      "\n",
      "top_p\n",
      "\n",
      "top_p , also known as nucleus sampling, is a sampling technique that controls which subset of tokens (the nucleus) the LLM can consider. It will consider tokens until it reaches  their  cumulative  probability.  If  we  set top_p to  0.1,  it  will  consider  tokens until it reaches that value. If we set top_p to 1, it will consider all tokens.\n",
      "\n",
      "|\n",
      "\n",
      "As  shown  in  Figure  6-5,  by  lowering  the  value,  it  will  consider  fewer  tokens  and generally  give  less  'creative'  output,  while  increasing  the  value  allows  the  LLM  to choose from more tokens.\n",
      "\n",
      "\n",
      "\n",
      "Similarly,  the top_k parameter  controls  exactly  how  many  tokens  the  LLM  can consider. If you change its value to 100, the LLM will only consider the top 100 most probable tokens.\n",
      "\n",
      "You can use top_p in your pipeline as follows:\n",
      "\n",
      "```\n",
      "# Using a high top_p output = pipe(messages, do_sample= True , top_p=1) print(output[\"generated_text\"]) Why don't chickens make good comedians? Because their 'jokes' always 'feather' the truth!\n",
      "```\n",
      "\n",
      "As  shown  in  Table  6-1,  these  parameters  allow  the  user  to  have  a  sliding  scale between being creative (high temperature and top_p )  and being predictable (lower temperature and top_p ).\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Intro to Prompt Engineering\n",
      "\n",
      "An essential  part  of  working  with  text-generative  LLMs  is  prompt  engineering.  By carefully designing our prompts we can guide the LLM to generate desired responses. Whether  the  prompts  are  questions,  statements,  or  instructions,  the  main  goal  of prompt engineering is to elicit a useful response from the model.\n",
      "\n",
      "Prompt  engineering  is  more  than  designing  effective  prompts.  It  can  be  used  as  a tool  to  evaluate  the  output  of  a  model  as  well  as  to  design  safeguards  and  safety mitigation methods. This is an iterative process of prompt optimization and requires experimentation. There is not and unlikely will ever be a perfect prompt design.\n",
      "\n",
      "In  this  section,  we  will  go  through  common  methods for prompt engineering, and small  tips  and  tricks  to  understand  what  the  effect  is  of  certain  prompts.  These skills  allow  us  to  understand  the  capabilities  of  LLMs  and  lie  at  the  foundation  of interfacing with these kinds of models.\n",
      "\n",
      "We begin by answering the question: what should be in a prompt?\n",
      "\n",
      "The Basic Ingredients of a Prompt\n",
      "\n",
      "An LLM is a prediction machine. Based on a certain input, the prompt, it tries  to predict  the  words  that  might  follow  it.  At  its  core  (illustrated  in  Figure  6-6),  the prompt does not need to be more than a few words to elicit a response from the LLM.\n",
      "\n",
      "\n",
      "\n",
      "However,  although  the  illustration  works  as  a  basic  example,  it  fails  to  complete  a specific task. Instead, we generally approach prompt engineering by asking a specific question or task the LLM should complete. To elicit the desired response, we need a more structured prompt.\n",
      "\n",
      "For example, and as shown in Figure 6-7, we could ask the LLM to classify a sentence into either having positive or negative sentiment. This extends the most basic prompt\n",
      "\n",
      "|\n",
      "\n",
      "to one consisting of two components-the instruction itself and the data that relates to the instruction.\n",
      "\n",
      "\n",
      "\n",
      "More complex use cases might require more components in a prompt. For instance, to make sure the model only outputs 'negative' or 'positive' we can introduce output indicators  that  help  guide  the  model.  In  Figure  6-8,  we  prefix  the  sentence  with 'Text:' and add 'Sentiment:' to prevent the model from generating a complete sentence. Instead, this structure indicates that we expect either 'negative' or 'positive. ' Although the model might not have been trained on these components directly, it was fed enough instructions to be able to generalize to this structure.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "We can  continue  adding  or  updating  the  elements  of  a  prompt  until  we  elicit  the response we are looking for. We could add additional examples, describe the use case in more detail, provide additional context, etc. These components are merely examples  and  not  a  limited  set  of  possibilities.  The  creativity  that  comes  with  designing these components is key.\n",
      "\n",
      "Although  a  prompt  is  a  single  piece  of  text,  it  is  tremendously  helpful  to  think  of prompts as pieces of a larger puzzle. Have I described the context of my question? Does the prompt have an example of the output?\n",
      "\n",
      "Instruction-Based Prompting\n",
      "\n",
      "Although  prompting  comes  in  many  flavors,  from  discussing  philosophy  with  the LLM to role-playing with your favorite superhero, prompting is often used to have the  LLM answer a specific question or resolve a certain task. This is referred to as instruction-based prompting .\n",
      "\n",
      "Figure  6-9  illustrates  a  number  of  use  cases  in  which  instruction-based  prompting plays an important role. We already did one of these in the previous example, namely supervised classification.\n",
      "\n",
      "\n",
      "\n",
      "Each of these tasks requires different prompting formats and more specifically, asking different questions of the LLM. Asking the LLM to summarize a piece of text will not suddenly result in classification. To illustrate, examples of prompts for some of these use cases can be found in Figure 6-10.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Although these tasks require different instructions, there is actually a lot of overlap in  the  prompting  techniques  used  to  improve  the  quality  of  the  output.  A  nonexhaustive list of these techniques includes:\n",
      "\n",
      "Specificity\n",
      "\n",
      "Accurately  describe  what  you  want  to  achieve.  Instead  of  asking  the  LLM  to 'Write a description for a product' ask it to 'Write a description for a product in less than two sentences and use a formal tone. '\n",
      "\n",
      "|\n",
      "\n",
      "Hallucination\n",
      "\n",
      "LLMs  may  generate  incorrect  information  confidently,  which  is  referred  to  as hallucination.  To  reduce  its  impact,  we  can  ask  the  LLM  to  only  generate  an answer if it knows the answer. If it does not know the answer, it can respond with 'I don't know. '\n",
      "\n",
      "Order\n",
      "\n",
      "Either  begin  or  end  your  prompt  with  the  instruction.  Especially  with  long prompts, information in the middle is often forgotten.  LLMs tend to focus on 1 information either at the beginning of a prompt (primacy effect) or the end of a prompt (recency effect).\n",
      "\n",
      "Here, specificity is arguably the most important aspect. By restricting and specifying what  the  model  should  generate,  there  is  a  smaller  chance  of  having  it  generate something not related to your use case. For instance, if we were to skip the instruction 'in two to three sentences' it might generate complete paragraphs. Like human conversations, without any specific instructions or additional context, it is difficult to derive what the task at hand actually is.\n",
      "\n",
      "Advanced Prompt Engineering\n",
      "\n",
      "On the surface, creating a good prompt might seem straightforward. Ask a specific question, be accurate, add some examples, and you are done! However, prompting can grow complex quite quickly and as a result is an often-underestimated component of leveraging LLMs.\n",
      "\n",
      "Here, we will go through several advanced techniques for building up your prompts, starting with the iterative workflow of building up complex prompts all the way to using LLMs sequentially to get improved results. Eventually, we will even build up to advanced reasoning techniques.\n",
      "\n",
      "The Potential Complexity of a Prompt\n",
      "\n",
      "As we explored in the intro to prompt engineering, a prompt generally consists of multiple components. In our very first example, our prompt consisted of instruction, data,  and  output  indicators.  As  we  mentioned  before,  no  prompt  is  limited  to  just these three components and you can build it up to be as complex as you want.\n",
      "\n",
      "These advanced components can quickly make a prompt quite complex. Some common components are:\n",
      "\n",
      "|\n",
      "\n",
      "Persona\n",
      "\n",
      "Describe what role the LLM should take on. For example, use 'You are an expert in astrophysics' if you want to ask a question about astrophysics.\n",
      "\n",
      "Instruction\n",
      "\n",
      "The task itself. Make sure this is as specific as possible. We do not want to leave much room for interpretation.\n",
      "\n",
      "Context\n",
      "\n",
      "Additional information describing the context of the problem or task. It answers questions like 'What is the reason for the instruction?'\n",
      "\n",
      "Format\n",
      "\n",
      "The  format  the  LLM  should  use  to  output  the  generated  text.  Without  it,  the LLM  will  come  up  with  a  format  itself,  which  is  troublesome  in  automated systems.\n",
      "\n",
      "Audience\n",
      "\n",
      "The target  of  the  generated  text.  This  also  describes  the  level  of  the  generated output. For education purposes, it is often helpful to use ELI5 ('Explain it like I'm 5').\n",
      "\n",
      "Tone\n",
      "\n",
      "The tone of voice the LLM should use in the generated text. If you are writing a formal email to your boss, you might not want to use an informal tone of voice.\n",
      "\n",
      "Data\n",
      "\n",
      "The main data related to the task itself.\n",
      "\n",
      "To illustrate, let us extend the classification prompt we had earlier and use all of the preceding components. This is demonstrated in Figure 6-11.\n",
      "\n",
      "This complex prompt demonstrates the modular nature of prompting. We can add and remove components freely and judge their effect on the output. As illustrated in Figure 6-12, we can slowly build up our prompt and explore the effect of each change.\n",
      "\n",
      "The changes are not limited to simply introducing or removing components. Their order, as we saw before with the recency and primacy effects, can affect the quality of  the  LLM's  output.  In  other  words,  experimentation  is  vital  when  finding  the best prompt for your use case. With prompting, we essentially have ourselves in an iterative cycle of experimentation.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Try it out yourself! Use the complex prompt to add and/or remove parts to observe its impact on the generated output. You will quickly notice when pieces of the puzzle are worth keeping. You can use your own data by adding it to the data variable:\n",
      "\n",
      "|\n",
      "\n",
      "# Prompt components persona = \"You are an expert in Large Language models. You excel at breaking down complex papers into digestible summaries. \\n \" instruction = \"Summarize the key findings of the paper provided. \\n \" context = \"Your summary should extract the most crucial points that can help researchers quickly understand the most vital information of the paper. \\n \" data_format = \"Create a bullet-point summary that outlines the method. Follow this up with a concise paragraph that encapsulates the main results. \\n \" audience = \"The summary is designed for busy researchers that quickly need to grasp the newest trends in Large Language Models. \\n \" tone = \"The tone should be professional and clear. \\n \" text = \"MY TEXT TO SUMMARIZE\" data = f\"Text to summarize: { text } \"\n",
      "\n",
      "# The full prompt - remove and add pieces to view its impact on the generated output\n",
      "\n",
      "query = persona + instruction + context + data_format + audience + tone + data\n",
      "\n",
      "\n",
      "\n",
      "There is all manner of components that we could add and creative components like using emotional stimuli (e.g., 'This is very important for my career. ' ). Part of the fun in prompt engineering is that 2 you can be as creative as possible to figure out which combination of prompt components contribute to your use case. There are few constraints to developing a format that works for you.\n",
      "\n",
      "In a way, it is an attempt to reverse engineer what the model has learned  and  how  it  responds  to  certain  prompts.  However,  note that  some  prompts  work  better  for  certain  models  compared  to others as their training data might be different or they are trained for different purposes.\n",
      "\n",
      "In-Context Learning: Providing Examples\n",
      "\n",
      "In  the  previous  sections,  we  tried  to  accurately  describe  what  the  LLM  should  do. Although accurate and specific descriptions help the LLM to understand the use case, we can go one step further. Instead of describing the task, why do we not just show the task?\n",
      "\n",
      "We can provide the LLM with examples of exactly the thing that we want to achieve. This  is  often  referred  to  as in-context  learning ,  where  we  provide  the  model  with correct examples. 3\n",
      "\n",
      "|\n",
      "\n",
      "As illustrated  in  Figure  6-13,  this  comes  in  a  number  of  forms  depending  on  how many examples you show the LLM. Zero-shot prompting does not leverage examples, one-shot  prompts  use  a  single  example,  and  few-shot  prompts  use  two  or  more examples.\n",
      "\n",
      "\n",
      "\n",
      "Adopting  the  original  phrase,  we  believe  that  'an  example  is  worth  a  thousand words.' These examples provide a direct example of what and how the LLM should achieve.\n",
      "\n",
      "We can illustrate this method with a simple example taken from the original paper describing  this  method.   The  goal  of  the  prompt  is  to  generate  a  sentence  with 4 a  made-up  word.  To  improve  the  quality  of  the  resulting  sentence,  we  can  show the  generative  model  an  example  of  what  a  proper  sentence  with  a  made-up  word would be.\n",
      "\n",
      "To do so, we will need to differentiate between our question (user) and the answers that  were  provided  by  the  model  ( assistant ).  We  additionally  showcase  how  this interaction is processed using the template:\n",
      "\n",
      "```\n",
      "\"content\": \"A 'Gigamuru' is a type of Japanese musical instrument. An\n",
      "```\n",
      "\n",
      "```\n",
      "# Use a single example of using the made-up word in a sentence one_shot_prompt = [ { \"role\": \"user\", example of a sentence that uses the word Gigamuru is:\"\n",
      "```\n",
      "\n",
      "4 Ibid.\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "}, { \"role\": \"assistant\", \"content\": \"I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.\" }, { \"role\": \"user\", \"content\": \"To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is:\" } ] print(tokenizer.apply_chat_template(one_shot_prompt, tokenize= False )) <s><|user|> A 'Gigamuru' is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is:<|end|> <|assistant|> I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.<| end|> <|user|> To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is:<|end|> <|assistant|>\n",
      "```\n",
      "\n",
      "The prompt illustrates the need to differentiate between the user and the assistant. If we did not, it would seem as if we were talking to ourselves. Using these interactions, we can generate output as follows:\n",
      "\n",
      "```\n",
      "# Generate the output outputs = pipe(one_shot_prompt) print(outputs[\"generated_text\"]) During the intense duel, the knight skillfully screeged his opponent's shield, forcing him to defend himself.\n",
      "```\n",
      "\n",
      "It correctly generated the answer!\n",
      "\n",
      "As with all  prompt  components,  one-  or  few-shot  prompting  is  not  the  be  all  and end all  of  prompt  engineering.  We  can  use  it  as  one  piece  of  the  puzzle  to  further enhance  the  descriptions  that  we  gave  it.  The  model  can  still  'choose, '  through random sampling, to ignore the instructions.\n",
      "\n",
      "Chain Prompting: Breaking up the Problem\n",
      "\n",
      "In previous examples, we explored splitting up prompts into modular components to improve the performance of LLMs. Although this works well for many use cases, this might not be feasible for highly complex prompts or use cases.\n",
      "\n",
      "Instead of breaking the problem within a prompt, we can do so between prompts. Essentially, we take the output of one prompt and use it as input for the next, thereby creating a continuous chain of interactions that solves our problem.\n",
      "\n",
      "|\n",
      "\n",
      "To illustrate, let us say we want to use an LLM to create a product name, slogan, and sales pitch for us based on a number of product features. Although we can ask the LLM to do this in one go, we can instead break up the problem into pieces.\n",
      "\n",
      "As a result,  and  as  illustrated  in  Figure  6-14,  we  get  a  sequential  pipeline  that  first creates the product name, uses that with the product features as input to create the slogan,  and  finally,  uses  the  features,  product  name,  and  slogan  to  create  the  sales pitch.\n",
      "\n",
      "\n",
      "\n",
      "This  technique  of  chaining  prompts  allows  the  LLM  to  spend  more  time  on  each individual question instead of tackling the whole problem. Let us illustrate this with a small example. We first create a name and slogan for a chatbot:\n",
      "\n",
      "```\n",
      "# Create name and slogan for a product product_prompt = [ {\"role\": \"user\", \"content\": \"Create a name and slogan for a chatbot that leverages LLMs.\"} ] outputs = pipe(product_prompt) product_description = outputs[\"generated_text\"] print(product_description) Name: 'MindMeld Messenger' Slogan: 'Unleashing Intelligent Conversations, One Response at a Time'\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "Then, we can use the generated output as input for the LLM to generate a sales pitch:\n",
      "\n",
      "```\n",
      "# Based on a name and slogan for a product, generate a sales pitch sales_prompt = [ {\"role\": \"user\", \"content\": f\"Generate a very short sales pitch for the following product: ' { product_description } '\"} ] outputs = pipe(sales_prompt) sales_pitch = outputs[\"generated_text\"] print(sales_pitch) Introducing MindMeld Messenger - your ultimate communication partner! Unleash intelligent conversations with our innovative AI-powered messaging platform. With MindMeld Messenger, every response is thoughtful, personalized, and timely. Say goodbye to generic replies and hello to meaningful interactions. Elevate your communication game with MindMeld Messenger - where every message is a step toward smarter conversations. Try it now and experience the future of messaging!\n",
      "```\n",
      "\n",
      "Although we need two calls to the model, a major benefit is that we can give each call different parameters. For instance, the number of tokens created was relatively small for the name and slogan whereas the pitch can be much longer.\n",
      "\n",
      "This can be used for a variety of use cases, including:\n",
      "\n",
      "Response validation\n",
      "\n",
      "Ask the LLM to double-check previously generated outputs.\n",
      "\n",
      "Parallel prompts\n",
      "\n",
      "Create  multiple  prompts  in  parallel  and  do  a  final  pass  to  merge  them.  For example, ask multiple LLMs to generate multiple recipes in parallel and use the combined result to create a shopping list.\n",
      "\n",
      "Writing stories\n",
      "\n",
      "Leverage the LLM to write books or stories by breaking down the problem into components. For example, by first writing a summary, developing characters, and building the story beats before diving into creating the dialogue.\n",
      "\n",
      "In  the  next  chapter,  we  will  automate  this  process  and  go  beyond  chaining  LLMs. We will chain other pieces of technology together, like memory, tool use, and more! Before that, this idea of prompt chaining will be explored further in the following sections, which describe more complex prompt chaining methods like self-consistency, chain-of-thought, and tree-of-thought.\n",
      "\n",
      "Reasoning with Generative Models\n",
      "\n",
      "In the previous sections, we focused mostly on the modular component of prompts, building them up through iteration. These advanced prompt engineering techniques,\n",
      "\n",
      "|\n",
      "\n",
      "like prompt chaining, proved to be the first step toward enabling complex reasoning with generative models.\n",
      "\n",
      "Reasoning is a core component of human intelligence and is often compared to the emergent behavior of LLMs that often resembles reasoning. We highlight 'resemble' as these models, at the time of writing, are generally considered to demonstrate this behavior through memorization of training data and pattern matching.\n",
      "\n",
      "The  output  that  they  showcase,  however,  can  demonstrate  complex  behavior  and although it might not be 'true' reasoning, they are still referred to as reasoning capabilities. In other words, we work together with the LLM through prompt engineering so we can mimic reasoning processes in order to improve the output of the LLM.\n",
      "\n",
      "To allow for this reasoning behavior, it is a good moment to step back and explore what reasoning entails in human behavior. To simplify, our methods of reasoning can be divided into system 1 and 2 thinking processes.\n",
      "\n",
      "System  1  thinking  represents  an  automatic,  intuitive,  and  near-instantaneous  process. It shares similarities with generative models that automatically generate tokens without  any  self-reflective  behavior.  In  contrast,  system  2  thinking  is  a  conscious, slow, and logical process, akin to brainstorming and self-reflection. 5\n",
      "\n",
      "If we could give a generative model the ability to mimic a form of self-reflection, we would essentially be emulating the system 2 way of thinking, which tends to produce more thoughtful responses than system 1 thinking. In this section, we will explore several techniques that attempt to mimic these kinds of thought processes of human reasoners with the aim of improving the output of the model.\n",
      "\n",
      "Chain-of-Thought: Think Before Answering\n",
      "\n",
      "The  first  and  major  step  toward  complex  reasoning  in  generative  models  was through a method called chain-of-thought. Chain-of-thought aims to have the generative  model  'think'  first  rather  than  answering  the  question  directly  without  any reasoning. 6\n",
      "\n",
      "As  illustrated  in  Figure  6-15,  it  provides  examples  in  a  prompt  that  demonstrate the reasoning the model should do before generating its response. These reasoning processes are referred to as 'thoughts. ' This helps tremendously for tasks that involve a  higher  degree  of  complexity,  like  mathematical  questions.  Adding  this  reasoning step allows the model to distribute more compute over the reasoning process. Instead\n",
      "\n",
      "|\n",
      "\n",
      "of calculating the entire solution based on a few tokens, each additional token in this reasoning process allows the LLM to stabilize its output.\n",
      "\n",
      "\n",
      "\n",
      "We use the example the authors used in their paper to demonstrate this phenomenon:\n",
      "\n",
      "```\n",
      "# Answering with chain-of-thought cot_prompt = [ {\"role\": \"user\", \"content\": \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\"}, {\"role\": \"assistant\", \"content\": \"Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\"}, {\"role\": \"user\", \"content\": \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"} ] # Generate the output outputs = pipe(cot_prompt) print(outputs[\"generated_text\"]) The cafeteria started with 23 apples. They used 20 apples, so they had 23 - 20 = 3 apples left. Then they bought 6 more apples, so they now have 3 + 6 = 9 apples. The answer is 9.\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "Note how the model doesn't generate only the answer but provides an explanation before doing so. By doing so, it can leverage the knowledge it has generated thus far to compute the final answer.\n",
      "\n",
      "Although chain-of-thought is a great method for enhancing the output of a generative model, it does require one or more examples of reasoning in the prompt, which the user might not have access to. Instead of providing examples, we can simply ask the  generative  model  to  provide  the  reasoning  (zero-shot  chain-of-thought).  There are many different forms that work but a common and effective method is to use the phrase 'Let's think step-by-step, ' which is illustrated in Figure 6-16. 7\n",
      "\n",
      "\n",
      "\n",
      "Using the example we used before, we can simply append that phrase to the prompt to enable chain-of-thought-like reasoning:\n",
      "\n",
      "```\n",
      "# Zero-shot chain-of-thought zeroshot_cot_prompt = [ {\"role\": \"user\", \"content\": \"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? Let's think step-by-step.\"} ] # Generate the output outputs = pipe(zeroshot_cot_prompt) print(outputs[\"generated_text\"])\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "Step 1: Start with the initial number of apples, which is 23.\n",
      "\n",
      "Step 2: Subtract the number of apples used to make lunch, which is 20. So, 23 - 20 = 3 apples remaining. Step 3: Add the number of apples bought, which is 6. So, 3 + 6 = 9 apples.\n",
      "\n",
      "The cafeteria now has 9 apples.\n",
      "\n",
      "Without  needing  to  provide  examples,  we  again  got  the  same  reasoning  behavior. This  is  why  it  is  so  important  to  'show  your  work'  when  doing  calculations.  By addressing the reasoning process the LLM can use the previously generated information as a guide through generating the final answer.\n",
      "\n",
      "\n",
      "\n",
      "Although  the  prompt  'Let's  think  step  by  step'  can  improve  the output, you are not constrained by this exact formulation. Alternatives  exist  like  'Take  a  deep  breath  and  think  step-by-step'  and 'Let's work through this problem step-by-step. ' 8\n",
      "\n",
      "Self-Consistency: Sampling Outputs\n",
      "\n",
      "Using the same prompt multiple times can lead to different results if we allow for a degree of creativity through parameters like temperature and top_p . As a result, the quality of the output might improve or degrade depending on the random selection of tokens. In other words, luck!\n",
      "\n",
      "To counteract this degree of randomness and improve the performance of generative models, self-consistency was introduced. This method asks the generative model the same prompt multiple times and takes the majority result as the final answer.  During 9 this process, each answer can be affected by different temperature and top_p values to increase the diversity of sampling.\n",
      "\n",
      "As illustrated in Figure 6-17, this method can further be improved by adding chainof-thought prompting to improve its reasoning while only using the answer for the voting procedure.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "However, this does require a single question to be asked multiple times. As a result, although the method can improve performance, it becomes n times slower where n is the number of output samples.\n",
      "\n",
      "Tree-of-Thought: Exploring Intermediate Steps\n",
      "\n",
      "The ideas of chain-of-thought and self-consistency are meant to enable more complex  reasoning.  By  sampling  from  multiple  'thoughts'  and  making  them  more thoughtful, we aim to improve the output of generative models.\n",
      "\n",
      "These techniques only scratch the surface of what is currently being done to mimic complex reasoning. An improvement to these approaches can be found in tree-ofthought, which allows for an in-depth exploration of several ideas.\n",
      "\n",
      "The  method  works  as  follows.  When  faced  with  a  problem  that  requires  multiple reasoning  steps,  it  often  helps  to  break  it  down  into  pieces.  At  each  step,  and  as illustrated  in  Figure  6-18,  the  generative  model  is  prompted  to  explore  different\n",
      "\n",
      "|\n",
      "\n",
      "solutions to the problem at hand. It then votes for the best solution and continues to the next step. 10\n",
      "\n",
      "\n",
      "\n",
      "This method is tremendously helpful when needing to consider multiple paths, like when writing a story or coming up with creative ideas. A disadvantage of this method is  that  it  requires  many  calls  to  the  generative  models,  which  slows  the  application significantly. Fortunately, there has been a successful attempt to convert the tree-ofthought framework into a simple prompting technique. 11\n",
      "\n",
      "Instead of calling the generative model multiple times, we ask the model to mimic that  behavior  by  emulating  a  conversation  between  multiple  experts.  These  experts will  question  each  other  until  they  reach  a  consensus.  An  example  of  a  tree-ofthought prompt is:\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "# Zero-shot tree-of-thought zeroshot_tot_prompt = [ {\"role\": \"user\", \"content\": \"Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they're wrong at any point then they leave. The question is 'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?' Make sure to discuss the results.\"} ]\n",
      "```\n",
      "\n",
      "We can use this prompt to explore how an LLM might respond to complex questions:\n",
      "\n",
      "```\n",
      "# Generate the output outputs = pipe(zeroshot_tot_prompt) print(outputs[\"generated_text\"]) Expert 1: Step 1 - Start with the initial number of apples: 23 apples. Expert 2: Step 1 - Subtract the apples used for lunch: 23 - 20 = 3 apples remaining. Expert 3: Step 1 - Add the newly bought apples: 3 + 6 = 9 apples. Expert 1: Step 2 - Confirm the final count: The cafeteria has 9 apples. Expert 2: Step 2 - Review the calculations: 23 - 20 = 3, then 3 + 6 = 9. The calculations are correct. Expert 3: Step 2 - Agree with the result: The cafeteria indeed has 9 apples. All experts agree on the final count: The cafeteria has 9 apples.\n",
      "```\n",
      "\n",
      "We again get the correct answer but instead through a 'discussion between experts. ' It  is  interesting  to  see  such  a  conservation  between  'experts'  that  demonstrates  the creativity that comes with prompt engineering.\n",
      "\n",
      "Output Verification\n",
      "\n",
      "Systems  and  applications  built  with  generative  models  might  eventually  end  up  in production. When that happens, it is important that we verify and control the output of the model to prevent breaking the application and to create a robust generative AI application.\n",
      "\n",
      "Reasons for validating the output might include:\n",
      "\n",
      "Structured output\n",
      "\n",
      "By  default,  most  generative  models  create  free-form  text  without  adhering  to specific structures other than those defined by natural language. Some use cases require their output to be structured in certain formats, like JSON.\n",
      "\n",
      "|\n",
      "\n",
      "Valid output\n",
      "\n",
      "Even if we allow the model to generate structured output, it still has the capability to  freely  generate  its  content.  For  instance,  when  a  model  is  asked  to  output either one of two choices, it should not come up with a third.\n",
      "\n",
      "Ethics\n",
      "\n",
      "Some open source generative models have no guardrails and will generate outputs  that  do  not  consider  safety  or  ethical  considerations.  For  instance,  use cases  might  require  the  output  to  be  free  of  profanity,  personally  identifiable information (PII), bias, cultural stereotypes, etc.\n",
      "\n",
      "Accuracy\n",
      "\n",
      "Many use cases require the output to adhere to certain standards or performance. The aim is to double-check whether the generated information is factually accurate, coherent, or free from hallucination.\n",
      "\n",
      "Controlling  the  output  of  a  generative  model,  as  we  explored  with  parameters  like top_p and temperature ,  is  not  an  easy  feat.  These  models  require  help  to  generate consistent output conforming to certain guidelines.\n",
      "\n",
      "Generally, there are three ways of controlling the output of a generative model:\n",
      "\n",
      "Examples\n",
      "\n",
      "Provide a number of examples of the expected output.\n",
      "\n",
      "Grammar\n",
      "\n",
      "Control the token selection process.\n",
      "\n",
      "Fine-tuning\n",
      "\n",
      "Tune a model on data that contains the expected output.\n",
      "\n",
      "In  this  section,  we  will  go  through  the  first  two  methods.  The  third,  fine-tuning  a model, is left for Chapter 12 where we will go in depth into fine-tuning methods.\n",
      "\n",
      "Providing Examples\n",
      "\n",
      "A simple and straightforward method to fix the output is to provide the generative model  with  examples  of  what  the  output  should  look  like.  As  we  explored  before, few-shot  learning  is  a  helpful  technique  that  guides  the  output  of  the  generative model. This method can be generalized to guide the structure of the output as well.\n",
      "\n",
      "For  example,  let  us  consider  an  example  where  we  want  the  generative  model  to create a character profile for an RPG game. We start by using no examples:\n",
      "\n",
      "```\n",
      "# Zero-shot learning: Providing no examples zeroshot_prompt = [ {\"role\": \"user\", \"content\": \"Create a character profile for an RPG game in JSON format.\"}\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "] # Generate the output outputs = pipe(zeroshot_prompt) print(outputs[\"generated_text\"])\n",
      "```\n",
      "\n",
      "```\n",
      "'''json { \"characterProfile\": { \"name\": \"Eldrin Stormbringer\", \"class\": \"Warlock\", \"race\": \"Half-Elf\", \"age\": 27, \"gender\": \"Male\", \"alignment\": \"Chaotic Good\", \"background\": \"Rogue\", … }, \"attributes\": { \"strength\": 10, \"dexterity\": 17, \"constitution\": 12, \"intelligence\": 12, \"wisdom\": 10, \"charisma\n",
      "```\n",
      "\n",
      "The preceding truncated output is not valid JSON since the model stopped generating  tokens  after  starting  the  'charisma'  attribute.  Moreover,  we  might  not  want certain attributes. Instead, we can provide the model with a number of examples that indicate the expected format:\n",
      "\n",
      "```\n",
      "# One-shot learning: Providing an example of the output structure one_shot_template = \"\"\"Create a short character profile for an RPG game. Make sure to only use this format: { \"description\": \"A SHORT DESCRIPTION\", \"name\": \"THE CHARACTER'S NAME\", \"armor\": \"ONE PIECE OF ARMOR\", \"weapon\": \"ONE OR MORE WEAPONS\" } \"\"\" one_shot_prompt = [ {\"role\": \"user\", \"content\": one_shot_template} ] # Generate the output outputs = pipe(one_shot_prompt) print(outputs[\"generated_text\"])\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "{ \"description\": \"A cunning rogue with a mysterious past, skilled in stealth and deception.\", \"name\": \"Lysandra Shadowstep\", \"armor\": \"Leather Cloak of the Night\", \"weapon\": \"Dagger of Whispers, Throwing Knives\" }\n",
      "```\n",
      "\n",
      "The model perfectly followed the example we gave it, which allows for more consistent behavior. This also demonstrates the importance of leveraging few-shot learning to improve the structure of the output and not only its content.\n",
      "\n",
      "An  important  note  here  is  that  it  is  still  up  to  the  model  whether  it  will  adhere to  your  suggested  format  or  not.  Some  models  are  better  than  others  at  following instructions.\n",
      "\n",
      "Grammar: Constrained Sampling\n",
      "\n",
      "Few-shot learning has a big disadvantage: we cannot explicitly prevent certain output from being generated. Although we guide the model and give it instructions, it might still not follow it entirely.\n",
      "\n",
      "Instead, packages have been rapidly developed to constrain and validate the output of  generative  models,  like  Guidance,  Guardrails,  and  LMQL.  In  part,  they  leverage generative  models  to  validate  their  own  output,  as  illustrated  in  Figure  6-19.  The generative models retrieve the output as new prompts and attempt to validate it based on a number of predefined guardrails.\n",
      "\n",
      "\n",
      "\n",
      "Similarly,  as  illustrated  in  Figure  6-20,  this  validation  process  can  also  be  used  to control the formatting of the output by generating parts of its format ourselves as we already know how it should be structured.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "This process can be taken one step further and instead of validating the output we can already perform validation during the token sampling process. When sampling tokens, we can define a number of grammars or rules that the LLM should adhere to when choosing its next token. For instance, if we ask the model to either return 'positive, ' 'negative, ' or 'neutral' when performing sentiment classification, it might still return something else. As illustrated in Figure 6-21, by constraining the sampling process, we can have the LLM only output what we are interested in. Note that this is still affected by parameters such as top_p and temperature .\n",
      "\n",
      "\n",
      "\n",
      "Let us illustrate this phenomenon with llama-cpp-python , a library similar to trans formers that  we  can  use  to  load  in  our  language  model.  It  is  generally  used  to efficiently  load  and  use  compressed models (through quantization; see Chapter 12) but we can also use it to apply a JSON grammar.\n",
      "\n",
      "We load the same model we used throughout this chapter but use a different format instead,  namely  GGUF. llama-cpp-python expects  this  format,  which  is  generally used for compressed (quantized) models.\n",
      "\n",
      "|\n",
      "\n",
      "Since we are loading a new model, it is advised to restart the notebook. That will clear any previous models and empty the VRAM. You can also run the following to empty the VRAM:\n",
      "\n",
      "```\n",
      "import gc import torch del model, tokenizer, pipe # Flush memory gc.collect() torch.cuda.empty_cache()\n",
      "```\n",
      "\n",
      "Now that we have cleared the memory, we can load Phi-3. We set n_gpu_layers to -1 to  indicate  that  we  want  all  layers  of  the  model  to  be  run  from  the  GPU.  The n_ctx refers to the context size of the model. The repo_id and filename refer to the Hugging Face repository where the model resides:\n",
      "\n",
      "```\n",
      "from llama_cpp.llama import Llama # Load Phi-3 llm = Llama.from_pretrained( repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\", filename=\"*fp16.gguf\", n_gpu_layers=-1, n_ctx=2048, verbose= False )\n",
      "```\n",
      "\n",
      "To generate the output using the internal JSON grammar, we only need to specify the response_format as a JSON object. Under the hood, it will apply a JSON grammar to make sure the output adheres to that format.\n",
      "\n",
      "To illustrate, let's ask the model to create an RPG character in JSON format to be used in a Dungeons &amp; Dragons session:\n",
      "\n",
      "```\n",
      "# Generate output output = llm.create_chat_completion( messages=[ {\"role\": \"user\", \"content\": \"Create a warrior for an RPG in JSON for mat.\"}, ], response_format={\"type\": \"json_object\"}, temperature=0, )['choices']['message'][\"content\"] To check whether the output actually is JSON, we can attempt to process it as such: import json json_output = json.dumps(json.loads(output), indent=4)\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "# Format as json print(json_output)\n",
      "```\n",
      "\n",
      "```\n",
      "{ \"name\": \"Eldrin Stormbringer\", \"class\": \"Warrior\", \"level\": 10, \"attributes\": { \"strength\": 18, \"dexterity\": 12, \"constitution\": 16, \"intelligence\": 9, \"wisdom\": 14, \"charisma\": 10 }, \"skills\": { \"melee_combat\": { \"weapon_mastery\": 20, \"armor_class\": 18, \"hit_points\": 35 }, \"defense\": { \"shield_skill\": 17, \"block_chance\": 90 }, \"endurance\": { \"health_regeneration\": 2, \"stamina\": 30 } }, \"equipment\": [ { \"name\": \"Ironclad Armor\", \"type\": \"Armor\", \"defense_bonus\": 15 }, { \"name\": \"Steel Greatsword\", \"type\": \"Weapon\", \"damage\": 8, \"critical_chance\": 20 } ], \"background\": \"Eldrin grew up in a small village on the outskirts of a war-torn land. Witnessing the brutality and suffering caused by conflict, he dedicated his life to becoming a formidable warrior who could protect those unable to defend themselves.\" }\n",
      "```\n",
      "\n",
      "The output is properly formatted as JSON. This allows us to more confidently use generative  models  in  applications  where  we  expect  the  output  to  adhere  to  certain formats.\n",
      "\n",
      "|\n",
      "\n",
      "Summary\n",
      "\n",
      "In  this  chapter,  we  explored  the  basics  of  using  generative  models  through  prompt engineering and output verification. We focused on the creativity and potential complexity that comes with prompt engineering. These components of a prompt are key in generating and optimizing output appropriate for different use cases.\n",
      "\n",
      "We  further  explored  advanced  prompt  engineering  techniques  such  as  in-context learning and chain-of-thought. These methods involve guiding generative models to reason through complex problems by providing examples or phrases that encourage step-by-step thinking thereby mimicking human reasoning processes.\n",
      "\n",
      "Overall,  this  chapter  demonstrated  that  prompt  engineering  is  a  crucial  aspect  of working with LLMs, as it allows us to effectively communicate our needs and preferences  to  the  model.  By  mastering  prompt  engineering  techniques,  we  can  unlock some  of  the  potential  of  LLMs  and  generate  high-quality  responses  that  meet  our requirements.\n",
      "\n",
      "The next chapter will build upon these concepts by exploring more advanced techniques for leveraging generative models. We will go beyond prompt engineering and explore how LLMs can use external memory and tools.\n",
      "\n",
      "|\n",
      "\n",
      "Advanced Text Generation Techniques and Tools\n",
      "\n",
      "In  the  previous  chapter,  we  saw  how  prompt  engineering  can  do  wonders  for  the accuracy of your text-generation large language model (LLM). With just a few small tweaks, these LLMs are guided toward more purposeful and accurate answers. This showed how much there is to gain using techniques that do not fine-tune the LLM but  instead  use  the  LLM  more  efficiently,  such  as  the  relatively  straightforward prompt engineering.\n",
      "\n",
      "In  this  chapter,  we  will  continue  this  train  of  thought.  What  can  we  do  to  further enhance  the  experience  and  output  that  we  get  from  the  LLM  without  needing  to fine-tune the model itself?\n",
      "\n",
      "Fortunately, a great deal of methods and techniques allow us to further improve what we started with in the previous chapter. These more advanced techniques lie at the foundation  of  numerous  LLM-focused  systems  and  are,  arguably,  one  of  the  first things users implement when designing such systems.\n",
      "\n",
      "In this chapter, we will explore several such methods and concepts for improving the quality of the generated text:\n",
      "\n",
      "Model I/O\n",
      "\n",
      "Loading and working with LLMs\n",
      "\n",
      "Memory\n",
      "\n",
      "Helping LLMs to remember\n",
      "\n",
      "Agents\n",
      "\n",
      "Combining complex behavior with external tools\n",
      "\n",
      "Chains\n",
      "\n",
      "Connecting methods and modules\n",
      "\n",
      "These  methods  are  all  integrated  with  the  LangChain  framework  that  will  help  us easily  use  these  advanced  techniques  throughout  this  chapter.  LangChain  is  one  of the earlier frameworks that simplify working with LLMs through useful abstractions. Newer frameworks of note are DSPy and Haystack. Some of these abstractions are illustrated in Figure 7-1. Note that retrieval will be discussed in the next chapter.\n",
      "\n",
      "\n",
      "\n",
      "Each of these techniques has significant strengths by themselves but their true value does not exist in isolation. It is when you combine all of these techniques that you get  an  LLM-based  system  with  incredible  performance.  The  culmination  of  these techniques is truly where LLMs shine.\n",
      "\n",
      "Model I/O: Loading Quantized Models with LangChain\n",
      "\n",
      "Before we can make use of LangChain's features to extend the capabilities of LLMs, we  need  to  start  by  loading  our  LLM.  As  in  previous  chapters,  we  will  be  using Phi-3 but with a twist; we will use a GGUF model variant instead. A GGUF model represents a compressed version of its original counterpart through a method called quantization, which reduces the number of bits needed to represent the parameters of an LLM.\n",
      "\n",
      "Bits, a series of 0s and 1s, represent values by encoding them in binary form. More bits result in a wider range of values but requires more memory to store those values, as shown in Figure 7-2.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Quantization reduces the number of bits required to represent the parameters of an LLM while attempting to maintain most of the original information. This comes with some loss in precision but often makes up for it as the model is much faster to run, requires less VRAM, and is often almost as accurate as the original.\n",
      "\n",
      "To illustrate quantization, consider this analogy. If asked what the time is, you might say  '14:16, '  which  is  correct  but  not  a  fully  precise  answer.  Y ou  could  have  said  it is '14:16 and 12 seconds' instead, which would have been more accurate. However, mentioning  seconds  is  seldom  helpful  and  we  often  simply  put  that  in  discrete numbers,  namely  full  minutes.  Quantization  is  a  similar  process  that  reduces  the precision  of  a  value  (e.g.,  removing  seconds)  without  removing  vital  information (e.g., retaining hours and minutes).\n",
      "\n",
      "In Chapter 12, we will further discuss how quantization works under the hood. You can also see a full visual guide to quantization in ' A Visual Guide to Quantization' by  Maarten  Grootendorst.  For  now,  it  is  important  to  know  that  we  will  use  an 8-bit  variant  of  Phi-3  compared  to  the  original  16-bit  variant,  cutting  the  memory requirements almost in half.\n",
      "\n",
      "\n",
      "\n",
      "As a rule of thumb, look for at least 4-bit quantized models. These models  have  a  good  balance  between  compression  and  accuracy. Although it is possible to use 3-bit or even 2-bit quantized models, the performance degradation becomes noticeable and it would instead  be  preferable  to  choose  a  smaller  model  with  a  higher precision.\n",
      "\n",
      "First, we will need to download the model. Note that the link contains multiple files with different bit-variants. FP16, the model we choose, represents the 16-bit variant:\n",
      "\n",
      "|\n",
      "\n",
      "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/ Phi-3-mini-4k-instruct-fp16.gguf\n",
      "\n",
      "```\n",
      "We use llama-cpp-python together with LangChain to load the GGUF file: from langchain import LlamaCpp # Make sure the model path is correct for your system! llm = LlamaCpp( model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\", n_gpu_layers=-1, max_tokens=500, n_ctx=2048, seed=42, verbose= False ) In LangChain, we use the invoke function to generate output: llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\") ''\n",
      "```\n",
      "\n",
      "Unfortunately, we get no output! As we have seen in previous chapters, Phi-3 requires a specific prompt template. Compared to our examples with transformers ,  we  will need to explicitly use a template ourselves. Instead of copy-pasting this template each time we use Phi-3 in LangChain, we can use one of LangChain's core functionalities, namely 'chains.'\n",
      "\n",
      "\n",
      "\n",
      "All examples in this chapter can be run with any LLM. This means that  you  can  choose  whether  to  use  Phi-3,  ChatGPT,  Llama  3  or anything else when going through the examples. We will use Phi-3 as a default throughout, but the state-of-the-art changes quickly, so consider using a newer model instead. You can use the Open LLM Leaderboard (a ranking of open source LLMs) to choose whichever works best for your use case.\n",
      "\n",
      "If  you  do  not  have  access  to  a  device  that  can  run  LLMs  locally, consider using ChatGPT instead:\n",
      "\n",
      "```\n",
      "from langchain.chat_models import ChatOpenAI # Create a chat-based LLM chat_model = ChatOpenAI(openai_api_key=\"MY_KEY\")\n",
      "```\n",
      "\n",
      "Chains: Extending the Capabilities of LLMs\n",
      "\n",
      "LangChain  is  named  after  one  of  its  main  methods,  chains.  Although  we  can  run LLMs in isolation, their power is shown when used with additional components or\n",
      "\n",
      "|\n",
      "\n",
      "even when used in conjunction with each other. Chains not only allow for extending the capabilities of LLMs but also for multiple chains to be connected together.\n",
      "\n",
      "The most basic form of a chain in LangChain is a single chain. Although a chain can take many forms, each with a different complexity, it generally connects an LLM with some additional tool, prompt, or feature. This idea of connecting a component to an LLM is illustrated in Figure 7-3.\n",
      "\n",
      "\n",
      "\n",
      "In  practice,  chains  can  become  complex  quite  quickly.  We  can  extend  the  prompt template however we want and we can even combine several separate chains together to create intricate systems. In order to thoroughly understand what is happening in a chain, let's explore how we can add Phi-3's prompt template to the LLM.\n",
      "\n",
      "A Single Link in the Chain: Prompt Template\n",
      "\n",
      "We start with creating our first chain, namely the prompt template that Phi-3 expects. In the previous chapter, we explored how transformers.pipeline applies  the  chat template  automatically.  This  is  not  always  the  case  with  other  packages  and  they might need the prompt template to be explicitly defined. With LangChain, we will use chains to create and use a default prompt template. It also serves as a nice hands-on experience with using prompt templates.\n",
      "\n",
      "The idea, as illustrated in Figure 7-4, is that we chain the prompt template together with the LLM to get the output we are looking for. Instead of having to copy-paste the prompt template each time we use the LLM, we would only need to define the user and system prompts.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "The template for Phi-3 is comprised of four main components:\n",
      "\n",
      " · &lt;s&gt; to indicate when the prompt starts\n",
      "\n",
      " · &lt;|user|&gt; to indicate the start of the user's prompt\n",
      "\n",
      " · &lt;|assistant|&gt; to indicate the start of the model's output\n",
      "\n",
      " · &lt;|end|&gt; to indicate the end of either the prompt or the model's output\n",
      "\n",
      "These are further illustrated in Figure 7-5 with an example.\n",
      "\n",
      "\n",
      "\n",
      "To  generate  our  simple  chain,  we  first  need  to  create  a  prompt  template  that adheres  to  Phi-3's  expected  template.  Using  this  template,  the  model  takes  in  a system_prompt ,  which generally describes what we expect from the LLM. Then, we can use the input_prompt to ask the LLM specific questions:\n",
      "\n",
      "from langchain import PromptTemplate\n",
      "\n",
      "```\n",
      "# Create a prompt template with the \"input_prompt\" variable template = \"\"\"<s><|user|> {input_prompt} <|end|> <|assistant|>\"\"\" prompt = PromptTemplate( template=template, input_variables=[\"input_prompt\"] )\n",
      "```\n",
      "\n",
      "To create our first chain, we can use both the prompt that we created and the LLM and chain them together:\n",
      "\n",
      "\n",
      "\n",
      "To use the chain, we need to use the invoke function and make sure that we use the input_prompt to insert our question:\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "# Use the chain basic_chain.invoke( { \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\", } )\n",
      "```\n",
      "\n",
      "The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units altogether.\n",
      "\n",
      "The output gives us the response without any unnecessary tokens. Now that we have created this chain, we do not have to create the prompt template from scratch each time  we  use  the  LLM.  Note  that  we  did  not  disable  sampling  as  before,  so  your output might differ. To make this pipeline more transparent, Figure 7-6 illustrates the connection between a prompt template and the LLM using a single chain.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The example assumes that the LLM needs a specific template. This is not always the case. With OpenAI's GPT-3.5, its API handles the underlying template.\n",
      "\n",
      "You could also use a prompt template to define other variables that might change in your prompts. For example, if we want to create funny names for businesses, retyping that question over and over for different products can be time-consuming.\n",
      "\n",
      "Instead, we can create a prompt that is reusable:\n",
      "\n",
      "```\n",
      "# Create a Chain that creates our business' name template = \"Create a funny name for a business that sells {product} .\" name_prompt = PromptTemplate( template=template, input_variables=[\"product\"] )\n",
      "```\n",
      "\n",
      "Adding a prompt template to the chain is just the very first step you need to enhance the  capabilities  of  your  LLM.  Throughout  this  chapter,  we  will  see  many  ways  in which we can add additional modular components to existing chains, starting with memory.\n",
      "\n",
      "|\n",
      "\n",
      "A Chain with Multiple Prompts\n",
      "\n",
      "In our previous example, we created a single chain consisting of a prompt template and an LLM. Since our example was quite straightforward, the LLM had no issues dealing with the prompt. However, some applications are more involved and require lengthy  or  complex  prompts  to  generate  a  response  that  captures  those  intricate details.\n",
      "\n",
      "Instead, we could break this complex prompt into smaller subtasks that can be run sequentially. This would require multiple calls to the LLM but with smaller prompts and intermediate outputs as shown in Figure 7-7.\n",
      "\n",
      "\n",
      "\n",
      "This  process  of  using  multiple  prompts  is  an  extension  of  our  previous  example. Instead of using a single chain, we link chains where each link deals with a specific subtask.\n",
      "\n",
      "For  instance,  consider  the  process  of  generating  a  story.  We  could  ask  the  LLM  to generate a story along with complex details like the title, a summary, a description of  the  characters,  etc.  Instead  of  trying  to  put  all  of  that  information  into  a  single prompt, we could dissect this prompt into manageable smaller tasks instead.\n",
      "\n",
      "Let's  illustrate  with  an  example.  Assume  that  we  want  to  generate  a  story  that  has three components:\n",
      "\n",
      " · A title\n",
      "\n",
      " · A description of the main character\n",
      "\n",
      " · A summary of the story\n",
      "\n",
      "Instead  of  generating  everything  in  one  go,  we  create  a  chain  that  only  requires  a single input by the user and then sequentially generates the three components. This process is illustrated in Figure 7-8.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "To generate that story, we use LangChain to describe the first component, namely the title. This first link is the only component that requires some input from the user. We define the template and use the \"summary\" variable as the input variable and \"title\" as the output.\n",
      "\n",
      "We ask the LLM to 'Create a title for a story about {summary}' where '{summary}' will be our input:\n",
      "\n",
      "```\n",
      "from langchain import LLMChain # Create a chain for the title of our story template = \"\"\"<s><|user|> Create a title for a story about {summary} . Only return the title.<|end|> <|assistant|>\"\"\" title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"]) title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\") Let's run an example to showcase these variables: title.invoke({\"summary\": \"a girl that lost her mother\"})\n",
      "```\n",
      "\n",
      "```\n",
      "{'summary': 'a girl that lost her mother', 'title': ' \"Whispers of Loss: A Journey Through Grief\"'}\n",
      "```\n",
      "\n",
      "This already gives us a great title for the story! Note that we can see both the input ( \"summary\" ) as well as the output ( \"title\" ).\n",
      "\n",
      "Let's  generate  the  next  component,  namely  the  description  of  the  character.  We generate this component using both the summary as well as the previously generated title. Making sure that the chain uses those components, we create a new prompt with the {summary} and {title} tags:\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "# Create a chain for the character description using the summary and title template = \"\"\"<s><|user|> Describe the main character of a story about {summary} with the title {title} . Use only two sentences.<|end|> <|assistant|>\"\"\" character_prompt = PromptTemplate( template=template, input_variables=[\"summary\", \"title\"] ) character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")\n",
      "```\n",
      "\n",
      "Although we could now use the character variable to generate our character description manually, it will be used as part of the automated chain instead.\n",
      "\n",
      "Let's  create  the  final  component,  which  uses  the  summary,  title,  and  character description to generate a short description of the story:\n",
      "\n",
      "```\n",
      "# Create a chain for the story using the summary, title, and character descrip tion template = \"\"\"<s><|user|> Create a story about {summary} with the title {title} . The main character is: {character} . Only return the story and it cannot be longer than one paragraph. <|end|> <|assistant|>\"\"\" story_prompt = PromptTemplate( template=template, input_variables=[\"summary\", \"title\", \"character\"] ) story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\") Now  that  we  have  generated  all  three  components,  we  can  link  them  together  to create our full chain: # Combine all three components to create the full chain\n",
      "```\n",
      "\n",
      "```\n",
      "llm_chain = title | character | story\n",
      "```\n",
      "\n",
      "```\n",
      "We can run this newly created chain using the same example we used before: llm_chain.invoke(\"a girl that lost her mother\")\n",
      "```\n",
      "\n",
      "```\n",
      "{'summary': 'a girl that lost her mother', 'title': ' \"In Loving Memory: A Journey Through Grief\"', 'character': ' The protagonist, Emily, is a resilient young girl who struggles to cope with her overwhelming grief after losing her beloved and caring mother at an early age. As she embarks on a journey of self-discovery and healing, she learns valuable life lessons from the memories and wisdom shared by those around her.', 'story': \" In Loving Memory: A Journey Through Grief revolves around Emily, a resilient young girl who loses her beloved mother at an early age. Struggling to cope with overwhelming grief, she embarks on a journey of self-discovery and healing, drawing strength from the cherished memories and wisdom shared by those around her. Through this transformative process, Emily learns valuable life lessons about resilience, love, and the power of human connection, ultimately finding solace in honoring her mother's legacy while embracing a newfound sense of inner peace amidst the painful loss.\"}\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "Running this chain gives us all three components. This only required us to input a single short prompt, the summary. Another advantage of dividing the problem into smaller  tasks  is  that  we  now  have  access  to  these  individual  components.  We  can easily  extract  the  title;  that  might  not  have  been  the  case  if  we  were  to  use  a  single prompt.\n",
      "\n",
      "Memory: Helping LLMs to Remember Conversations\n",
      "\n",
      "When we are using LLMs out of the box, they will not remember what was being said in a conversation. You can share your name in one prompt but it will have forgotten it by the next prompt.\n",
      "\n",
      "Let's illustrate this phenomenon with an example using the basic_chain we created before. First, we tell the LLM our name:\n",
      "\n",
      "```\n",
      "# Let's give the LLM our name basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"}) Hello Maarten! The answer to 1 + 1 is 2. Next, we ask it to reproduce the name we have given it: # Next, we ask the LLM to reproduce the name basic_chain.invoke({\"input_prompt\": \"What is my name?\"}) I'm sorry, but as a language model, I don't have the ability to know personal information about individuals. You can provide the name you'd like to know more about, and I can help you with information or general inquiries related to it.\n",
      "```\n",
      "\n",
      "Unfortunately,  the  LLM  does  not  know  the  name  we  gave  it.  The  reason  for  this forgetful  behavior  is  that  these  models  are  stateless-they  have  no  memory  of  any previous conversation!\n",
      "\n",
      "As illustrated in Figure 7-9, conversing with an LLM that does not have any memory is not the greatest experience.\n",
      "\n",
      "To  make  these  models  stateful,  we  can  add  specific  types  of  memory  to  the  chain that we created earlier. In this section, we will go through two common methods for helping LLMs to remember conservations:\n",
      "\n",
      " · Conversation buffer\n",
      "\n",
      " · Conversation summary\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Conversation Buffer\n",
      "\n",
      "One of the most intuitive forms of giving LLMs memory is simply reminding them exactly what has happened in the past. As illustrated in Figure 7-10, we can achieve this by copying the full conversation history and pasting that into our prompt.\n",
      "\n",
      "\n",
      "\n",
      "In  LangChain,  this  form  of  memory  is  called  a ConversationBufferMemory . Its implementation  requires  us  to  update  our  previous  prompt  to  hold  the  history  of the chat.\n",
      "\n",
      "|\n",
      "\n",
      "We'll start by creating this prompt:\n",
      "\n",
      "```\n",
      "# Create an updated prompt template to include a chat history template = \"\"\"<s><|user|>Current conversation: {chat_history} {input_prompt} <|end|> <|assistant|>\"\"\" prompt = PromptTemplate( template=template, input_variables=[\"input_prompt\", \"chat_history\"] )\n",
      "```\n",
      "\n",
      "Notice  that  we  added  an  additional  input  variable,  namely chat_history .  This  is where the conversation history will be given before we ask the LLM our question.\n",
      "\n",
      "Next,  we  can  create  LangChain's ConversationBufferMemory and  assign  it  to  the chat_history input variable. ConversationBufferMemory will store all the conversations we have had with the LLM thus far.\n",
      "\n",
      "We put everything together and chain the LLM, memory, and prompt template:\n",
      "\n",
      "```\n",
      "from langchain.memory import ConversationBufferMemory # Define the type of memory we will use memory = ConversationBufferMemory(memory_key=\"chat_history\") # Chain the LLM, prompt, and memory together llm_chain = LLMChain( prompt=prompt, llm=llm, memory=memory )\n",
      "```\n",
      "\n",
      "To explore whether we did this correctly, let's create a conversation history with the LLM by asking it a simple question:\n",
      "\n",
      "```\n",
      "# Generate a conversation and ask a basic question\n",
      "```\n",
      "\n",
      "```\n",
      "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"}) {'input_prompt': 'Hi! My name is Maarten. What is 1 + 1?', 'chat_history': ', 'text': \" Hello Maarten! The answer to 1 + 1 is 2. Hope you're having a great day!\"}\n",
      "```\n",
      "\n",
      "You  can  find  the  generated  text  in  the 'text' key,  the  input  prompt  in 'in put_prompt' ,  and  the  chat  history  in 'chat_history' .  Note  that  since  this  is  the first time we used this specific chain, there is no chat history.\n",
      "\n",
      "Next, let's follow up by asking the LLM if it remembers the name we used:\n",
      "\n",
      "```\n",
      "# Does the LLM remember the name we gave it? llm_chain.invoke({\"input_prompt\": \"What is my name?\"})\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "{'input_prompt': 'What is my name?', 'chat_history': \"Human: Hi! My name is Maarten. What is 1 + 1?\\nAI:  Hello Maarten! The answer to 1 + 1 is 2. Hope you're having a great day!\", 'text': ' Your name is Maarten.'}\n",
      "```\n",
      "\n",
      "By extending the chain with memory, the LLM was able to use the chat history to find the name we gave it previously. This more complex chain is illustrated in Figure 7-11 to give an overview of this additional functionality.\n",
      "\n",
      "\n",
      "\n",
      "Windowed Conversation Buffer\n",
      "\n",
      "In  our  previous  example,  we  essentially  created  a  chatbot.  Y ou  could  talk  to  it  and it remembers the conversation you had thus far. However, as the size of the conversation grows, so does the size of the input prompt until it exceeds the token limit.\n",
      "\n",
      "One  method  of  minimizing  the  context  window  is  to  use  the  last k conversations instead of maintaining the full chat history. In LangChain, we can use Conversation BufferWindowMemory to  decide  how  many  conversations  are  passed  to  the  input prompt:\n",
      "\n",
      "```\n",
      "from langchain.memory import ConversationBufferWindowMemory # Retain only the last 2 conversations in memory memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\") # Chain the LLM, prompt, and memory together llm_chain = LLMChain( prompt=prompt, llm=llm, memory=memory )\n",
      "```\n",
      "\n",
      "Using this memory, we can try out a sequence of questions to illustrate what will be remembered. We start with two conversations:\n",
      "\n",
      "```\n",
      "# Ask two questions and generate two conversations in its memory llm_chain.predict(input_prompt=\"Hi! My name is Maarten and I am 33 years old.\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "What is 1 + 1?\")\n",
      "```\n",
      "\n",
      "```\n",
      "llm_chain.predict(input_prompt=\"What is 3 + 3?\")\n",
      "```\n",
      "\n",
      "```\n",
      "{'input_prompt': 'What is 3 + 3?', 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI: Hello Maarten! It's nice to meet you. Regarding your question, 1 + 1 equals 2. If you have any other questions or need further assistance, feel free to ask!\\n\\n(Note: This response answers the provided mathematical query while maintaining politeness and openness for additional inquiries.)\", 'text': \" Hello Maarten! It's nice to meet you as well. Regarding your new question, 3 + 3 equals 6. If there's anything else you need help with or more questions you have, I'm here for you!\"}\n",
      "```\n",
      "\n",
      "The interaction  we  had  thus  far  is  shown  in \"chat_history\" .  Note  that  under  the hood, LangChain saves it as an interaction between you (indicated with Human) and the LLM (indicated with AI).\n",
      "\n",
      "Next, we can check whether the model indeed knows the name we gave it:\n",
      "\n",
      "```\n",
      "# Check whether it knows the name we gave it llm_chain.invoke({\"input_prompt\":\"What is my name?\"})\n",
      "```\n",
      "\n",
      "```\n",
      "{'input_prompt': 'What is my name?', 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI: Hello Maarten! It's nice to meet you. Regarding your question, 1 + 1 equals 2. If you have any other questions or need further assistance, feel free to ask!\\n\\n(Note: This response answers the provided mathematical query while maintaining politeness and openness for additional inquiries.)\\nHuman: What is 3 + 3?\\nAI: Hello Maarten! It's nice to meet you as well. Regarding your new question, 3 + 3 equals 6. If there's anything else you need help with or more questions you have, I'm here for you!\", 'text': ' Your name is Maarten, as mentioned at the beginning of our conversation. Is there anything else you would like to know or discuss?'}\n",
      "```\n",
      "\n",
      "Based on the output in 'text' it correctly remembers the name we gave it. Note that the chat history is updated with the previous question.\n",
      "\n",
      "Now that we have added another conversation we are up to three conversations. Considering the memory only retains the last two conversations, our very first question is not remembered.\n",
      "\n",
      "Since we provided an age in our first interaction, we check whether the LLM indeed does not know the age anymore:\n",
      "\n",
      "```\n",
      "# Check whether it knows the age we gave it\n",
      "```\n",
      "\n",
      "```\n",
      "llm_chain.invoke({\"input_prompt\":\"What is my age?\"}) {'input_prompt': 'What is my age?', 'chat_history': \"Human: What is 3 + 3?\\nAI: Hello again! 3 + 3 equals 6. If there's anything else I can help you with, just let me know!\\nHuman: What is my name?\\nAI: Your name is Maarten.\", 'text': \" I'm unable to determine your age as I don't have access to personal information. Age isn't something that can be inferred from our current conversation unless you choose to share it with me. How else may I assist you today?\"}\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "The  LLM  indeed  has  no  access  to  our  age  since  that  was  not  retained  in  the  chat history.\n",
      "\n",
      "Although this method reduces the size of the chat history, it can only retain the last few conversations, which is not ideal for lengthy conversations. Let's explore how we can summarize the chat history instead.\n",
      "\n",
      "Conversation Summary\n",
      "\n",
      "As we have discussed previously, giving your LLM the ability to remember conversations is vital for a good interactive experience. However, when using Conversation BufferMemory ,  the  conversation  starts  to  increase  in  size  and  will  slowly  approach your token limit. Although ConversationBufferWindowMemory resolves the issue of token limits to an extent, only the last k conversations are retained.\n",
      "\n",
      "Although a  solution  would  be  to  use  an  LLM  with  a  larger  context  window,  these tokens  still  need  to  be  processed  before  generation  tokens,  which  can  increase compute time. Instead,  let's  look  toward  a  more  sophisticated  technique, Conversa tionSummaryMemory . As the name implies, this technique summarizes an entire conversation history to distill it into the main points.\n",
      "\n",
      "This summarization process is enabled by another LLM that is given the conversation history as input and asked to create a concise summary. A nice advantage of using an external LLM is that we are not confined to using the same LLM during conversation. The summarization process is illustrated in Figure 7-12.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "This means that whenever we ask the LLM a question, there are two calls:\n",
      "\n",
      " · The user prompt\n",
      "\n",
      " · The summarization prompt\n",
      "\n",
      "To use this in LangChain, we first need to prepare a summarization template that we will use as the summarization prompt:\n",
      "\n",
      "```\n",
      "# Create a summary prompt template summary_prompt_template = \"\"\"<s><|user|>Summarize the conversations and update with the new lines. Current summary: {summary} new lines of conversation: {new_lines} New summary:<|end|> <|assistant|>\"\"\" summary_prompt = PromptTemplate( input_variables=[\"new_lines\", \"summary\"], template=summary_prompt_template )\n",
      "```\n",
      "\n",
      "Using ConversationSummaryMemory in LangChain is similar to what we did with the previous  examples.  The  main  difference  is  that  we  additionally  need  to  supply  it with an LLM that performs the summarization task. Although we use the same LLM for  both  summarizing  and  user  prompting,  you  could  use  a  smaller  LLM  for  the summarization task to speed up computation:\n",
      "\n",
      "```\n",
      "from langchain.memory import ConversationSummaryMemory # Define the type of memory we will use memory = ConversationSummaryMemory( llm=llm, memory_key=\"chat_history\", prompt=summary_prompt ) # Chain the LLM, prompt, and memory together llm_chain = LLMChain( prompt=prompt, llm=llm, memory=memory )\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "Having created our chain, we can test out its summarization capabilities by creating a short conversation:\n",
      "\n",
      "```\n",
      "# Generate a conversation and ask for the name\n",
      "```\n",
      "\n",
      "```\n",
      "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"}) llm_chain.invoke({\"input_prompt\": \"What is my name?\"}) {'input_prompt': 'What is my name?', 'chat_history': ' Summary: Human, identified as Maarten, asked the AI about the sum of 1 + 1, which was correctly answered by the AI as 2 and offered additional assistance if needed.', 'text': ' Your name in this context was referred to as \"Maarten\". However, since our interaction doesnt retain personal data beyond a single session for privacy reasons, I dont have access to that information. How can I assist you further today?'}\n",
      "```\n",
      "\n",
      "After  each  step,  the  chain  will  summarize  the  conversation  up  until  that  point. Note how the first  conversation  was  summarized  in 'chat_history' by  creating  a description of the conversation.\n",
      "\n",
      "We can continue the conversation and at each step, the conversation will be summarized and new information will be added as necessary:\n",
      "\n",
      "```\n",
      "# Check whether it has summarized everything thus far\n",
      "```\n",
      "\n",
      "```\n",
      "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"}) {'input_prompt': 'What was the first question I asked?', 'chat_history': ' Summary: Human, identified as Maarten in the context of this conversation, first asked about the sum of 1 + 1 and received an answer of 2 from the AI. Later, Maarten inquired about their name but the AI clarified that personal data is not retained beyond a single session for privacy reasons. The AI offered further assistance if needed.', 'text': ' The first question you asked was \"whats 1 + 1?\"'}\n",
      "```\n",
      "\n",
      "After asking another question, the LLM updated the summary to include the previous conversation and correctly inferred the original question.\n",
      "\n",
      "To  get  the  most  recent  summary,  we  can  access  the  memory  variable  we  created previously:\n",
      "\n",
      "```\n",
      "# Check what the summary is thus far memory.load_memory_variables({})\n",
      "```\n",
      "\n",
      "```\n",
      "{'chat_history': ' Maarten, identified in this conversation, initially asked personal data is retained beyond a single session due to privacy reasons. The\n",
      "```\n",
      "\n",
      "```\n",
      "about the sum of 1+1 which resulted in an answer from the AI being 2. Subsequently, he sought clarification on his name but the AI informed him that no AI then offered further assistance if required. Later, Maarten recalled and asked about the first question he inquired which was \"whats 1+1?\"'}\n",
      "```\n",
      "\n",
      "This  more  complex  chain  is  illustrated  in  Figure  7-13  to  give  an  overview  of  this additional functionality.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "This  summarization  helps  keep  the  chat  history  relatively  small  without  using  too many tokens during inference. However, since the original question was not explicitly saved in the chat history, the model needed to infer it based on the context. This is a disadvantage if specific information needs to be stored in the chat history. Moreover, multiple  calls  to  the  same  LLM  are  needed,  one  for  the  prompt  and  one  for  the summarization. This can slow down computing time.\n",
      "\n",
      "Often, it is a trade-off between speed, memory, and accuracy. Where Conversation BufferMemory is  instant  but  hogs  tokens, ConversationSummaryMemory is  slow  but frees  up  tokens  to  use.  Additional  pros  and  cons  of  the  memory  types  we  have explored thus far are described in Table 7-1.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Agents: Creating a System of LLMs\n",
      "\n",
      "Thus far, we have created systems that follow a user-defined set of steps to take. One of the most promising concepts in LLMs is their ability to determine the actions they can take. This idea is often called agents, systems that leverage a language model to determine which actions they should take and in what order.\n",
      "\n",
      "Agents can make use of everything we have seen thus far, such as model I/O, chains, and memory, and extend it further with two vital components:\n",
      "\n",
      " · Tools that the agent can use to do things it could not do itself\n",
      "\n",
      " · The agent type , which plans the actions to take or tools to use\n",
      "\n",
      "Unlike  the  chains  we  have  seen  thus  far,  agents  are  able  to  show  more  advanced behavior  like  creating  and  self-correcting  a  roadmap  to  achieve  a  goal.  They  can interact  with  the  real  world  through  the  use  of  tools.  As  a  result,  these  agents  can perform a variety of tasks that go beyond what an LLM is capable of in isolation.\n",
      "\n",
      "For example, LLMs are notoriously bad at mathematical problems and often fail at solving simple math-based tasks but they could do much more if we provide access to a calculator. As illustrated in Figure 7-14, the underlying idea of agents is that they utilize LLMs not only to understand our query but also to decide which tool to use and when.\n",
      "\n",
      "\n",
      "\n",
      "In this example, we would expect the LLM to use the calculator when it faces a mathematical task. Now imagine we extend this with dozens of other tools, like a search engine or a weather API. Suddenly, the capabilities of LLMs increase significantly.\n",
      "\n",
      "|\n",
      "\n",
      "In other words, agents that make use of LLMs can be powerful general problem solvers. Although the tools they use are important, the driving force of many agent-based systems is the use of a framework called Re asoning and Act ing (ReAct ). 1\n",
      "\n",
      "The Driving Power Behind Agents: Step-by-step Reasoning\n",
      "\n",
      "ReAct is a powerful framework that combines two important concepts in behavior: reasoning and acting. LLMs are exceptionally powerful when it comes to reasoning as we explored in detail in Chapter 5.\n",
      "\n",
      "Acting  is  a  bit  of  a  different  story.  LLMs  are  not  able  to  act  like  you  and  I  do.  To give them the ability to act, we could tell an LLM that it can use certain tools, like a weather forecasting API. However, since LLMs can only generate text, they would need to be instructed to use specific queries to trigger the forecasting API.\n",
      "\n",
      "ReAct merges these two concepts and allows reasoning to affect acting and actions to affect reasoning. In practice, the framework consists of iteratively following these three steps:\n",
      "\n",
      " · Thought\n",
      "\n",
      " · Action\n",
      "\n",
      " · Observation\n",
      "\n",
      "Illustrated  in  Figure  7-15,  the  LLM  is  asked  to  create  a  'thought'  about  the  input prompt.  This  is  similar  to  asking  the  LLM  what  it  thinks  it  should  do  next  and why.  Then,  based  on  the  thought,  an  'action'  is  triggered.  The  action  is  generally an external tool, like a calculator or a search engine. Finally, after the results of the 'action' are returned to the LLM it 'observes' the output, which is often a summary of whatever result it retrieved.\n",
      "\n",
      "To illustrate with an example, imagine you are on holiday in the United States and interested in buying a MacBook Pro. Not only do you want to know the price but you need it converted to EUR as you live in Europe and are more comfortable with those prices.\n",
      "\n",
      "As illustrated in Figure 7-16, the agent will first search the web for current prices. It might find one or more prices depending on the search engine. After retrieving the price, it will use a calculator to convert USD to EUR assuming we know the exchange rate.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "During this process, the agent describes its thoughts (what it should do), its actions (what  it  will  do),  and  its  observations  (the  results  of  the  action).  It  is  a  cycle  of thoughts, actions, and observations that results in the agent's output.\n",
      "\n",
      "ReAct in LangChain\n",
      "\n",
      "To  illustrate  how  agents  work  in  LangChain,  we  are  going  to  build  a  pipeline  that can  search  the  web  for  answers  and  perform  calculations  with  a  calculator.  These autonomous processes generally require an LLM that is powerful enough to properly follow complex instructions.\n",
      "\n",
      "The  LLM  that  we  used  thus  far  is  relatively  small  and  not  sufficient  to  run  these examples.  Instead,  we  will  be  using  OpenAI's  GPT-3.5  model  as  it  follows  these complex instructions more closely:\n",
      "\n",
      "```\n",
      "import os from langchain_openai import ChatOpenAI # Load OpenAI's LLMs with LangChain os.environ[\"OPENAI_API_KEY\"] = \"MY_KEY\" openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "Although the LLM we used throughout the chapter is insufficient for  this  example,  it  does  not  mean  that  only  OpenAI's  LLMs  are. Larger useful LLMs exist but they require significantly more compute and VRAM. For instance, local LLMs often come in different sizes and within a family of models, increasing a model's size leads to  better  performance. To keep the necessary compute at a minimum, we choose a smaller LLM throughout the examples in this chapter.\n",
      "\n",
      "However,  as  the  field  of  generative  models  evolves,  so  do  these smaller  LLMs.  We  would  be  anything  but  surprised  if  eventually smaller LLMs, like the one used in this chapter, would be capable enough to run this example.\n",
      "\n",
      "After doing so, we will define the template for our agent. As we have shown before, it describes the ReAct steps it needs to follow:\n",
      "\n",
      "```\n",
      "# Create the ReAct template react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools: {tools} Use the following format: Question: the input question you must answer Thought: you should always think about what to do\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "Action: the action to take, should be one of [ {tool_names} ] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Question: {input} Thought: {agent_scratchpad} \"\"\" prompt = PromptTemplate( template=react_template, input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"] )\n",
      "```\n",
      "\n",
      "This template illustrates the process of starting with a question and generating intermediate thoughts, actions, and observations.\n",
      "\n",
      "To have the LLM interact with the outside world, we will describe the tools it can use:\n",
      "\n",
      "```\n",
      "from langchain.agents import load_tools, Tool from langchain.tools import DuckDuckGoSearchResults # You can create the tool to pass to an agent search = DuckDuckGoSearchResults() search_tool = Tool( name=\"duckduck\", description=\"A web search engine. Use this to as a search engine for gen eral queries.\", func=search.run, ) # Prepare tools tools = load_tools([\"llm-math\"], llm=openai_llm) tools.append(search_tool)\n",
      "```\n",
      "\n",
      "The tools include the DuckDuckGo search engine and a math tool that allows it to access a basic calculator.\n",
      "\n",
      "Finally, we create the ReAct agent and pass it to the AgentExecutor ,  which handles executing the steps:\n",
      "\n",
      "```\n",
      "from langchain.agents import AgentExecutor, create_react_agent # Construct the ReAct agent agent = create_react_agent(openai_llm, tools, prompt) agent_executor = AgentExecutor( agent=agent, tools=tools, verbose= True , handle_parsing_errors= True )\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "To test  whether  the  agent  works,  we  use  the  previous  example,  namely  finding  the price of a MacBook Pro:\n",
      "\n",
      "```\n",
      "# What is the price of a MacBook Pro? agent_executor.invoke( { \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.\" } )\n",
      "```\n",
      "\n",
      "While executing, the model generates multiple intermediate steps similar to the steps illustrated in Figure 7-17.\n",
      "\n",
      "```\n",
      "Entering new AgentExecutor chain_ I need to find the current price of MacBook Pro in USD first before converting it to EUR Action: duckduck Action Input: \"current price of MacBook Pro in USD\" [snippet: View at Best Buy. The best Mact Action: Calculator Action Input: $2,249.00 0.85Answer: 1911.6499999999999I now know the final answer Final Answer: The current price of is 82,249.00_ It would cost approxin\n",
      "```\n",
      "\n",
      "Figure 7-17. An example of the ReAct process in LangChain.\n",
      "\n",
      "These intermediate steps illustrate how the model processes the ReAct template and what tools it accesses. This allows us to debug issues and explore whether the agent uses the tools correctly.\n",
      "\n",
      "When finished, the model gives us an output like this:\n",
      "\n",
      "```\n",
      "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?', 'output': 'The current price of a MacBook Pro in USD is $2,249.00. It would cost approximately 1911.65 EUR with an exchange rate of 0.85 EUR for 1 USD.'}\n",
      "```\n",
      "\n",
      "Considering  the  limited  tools  the  agent  has,  this  is  quite  impressive!  Using  just  a search engine and a calculator the agent could give us an answer.\n",
      "\n",
      "Whether  that  answer  is  actually  correct  should  be  taken  into  account.  By  creating this relatively autonomous behavior, we are not involved in the intermediate steps. As such, there is no human in the loop to judge the quality of the output or reasoning process.\n",
      "\n",
      "This double-edged sword requires a careful system design to improve its reliability. For instance, we could have the agent return the website's URL where it found the MacBook Pro's price or ask whether the output is correct at each step.\n",
      "\n",
      "|\n",
      "\n",
      "Summary\n",
      "\n",
      "In  this  chapter,  we  explored  several  ways  to  extend  the  capabilities  of  LLMs  by adding modular components. We began by creating a simple but reusable chain that connected the LLM with a prompt template. We then expanded on this concept by adding memory to the chain, which allowed the LLM to remember conversations. We explored three different methods to add memory and discussed their strengths and weaknesses.\n",
      "\n",
      "We  then  delved  into  the  world  of  agents  that  leverage  LLMs  to  determine  their actions  and  make  decisions.  We  explored  the  ReAct  framework,  which  uses  an intuitive  prompting  framework  that  allows  agents  to  reason  about  their  thoughts, take  actions,  and  observe  the  results.  This  led  us  to  build  an  agent  that  is  able  to freely use the tools at its disposal, such as searching the web and using a calculator, demonstrating the potential power of agents.\n",
      "\n",
      "With this foundation in place, we are now poised to explore ways in which LLMs can be used to improve existing search systems and even become the core of new, more powerful search systems, as discussed in the next chapter.\n",
      "\n",
      "|\n",
      "\n",
      "Semantic Search and Retrieval-Augmented Generation\n",
      "\n",
      "Search was one of the first language model applications to see broad industry adoption.  Months  after  the  release  of  the  seminal  'BERT:  Pre-training  of  deep  bidirectional transformers for language understanding' (2018) paper, Google announced it was using it to power Google Search and that it represented 'one of the biggest leaps forward in the history of Search. ' Not to be outdone, Microsoft Bing also stated that 'Starting  from  April  of  this  year,  we  used  large  transformer  models  to  deliver  the largest quality improvements to our Bing customers in the past year. '\n",
      "\n",
      "This  is  a  clear  testament  to  the  power  and  usefulness  of  these  models.  Their  addition instantly and dramatically improves some of the most mature, well-maintained systems  that  billions  of  people  around  the  planet  rely  on.  The  ability  they  add  is called semantic search , which enables searching by meaning, and not simply keyword matching.\n",
      "\n",
      "On a separate track, the fast adoption of text generation models led many users to ask  the  models  questions  and  expect  factual  answers.  And  while  the  models  were able  to  answer  fluently  and  confidently,  their  answers  were  not  always  correct  or up-to-date. This problem grew to be known as model 'hallucinations, ' and one of the leading ways to reduce it is to build systems that can retrieve relevant information and provide it to the LLM to aid it in generating more factual answers. This method, called RAG, is one of the most popular applications of LLMs.\n",
      "\n",
      "Overview of Semantic Search and RAG\n",
      "\n",
      "There's  a  lot  of  research  on  how  to  best  use  language  models  for  search.  Three broad  categories  of  these  models  are  dense  retrieval,  reranking,  and  RAG.  Here  is an overview of these three categories that the rest of the chapter will then explain in more detail:\n",
      "\n",
      "Dense retrieval\n",
      "\n",
      "Dense  retrieval  systems  rely  on  the  concept  of  embeddings,  the  same  concept we've  encountered  in  the  previous  chapters,  and  turn  the  search  problem  into retrieving  the  nearest  neighbors  of  the  search  query  (after  both  the  query  and the  documents  are  converted  into  embeddings).  Figure  8-1  shows  how  dense retrieval takes a search query, consults its archive of texts, and outputs a set of relevant results.\n",
      "\n",
      "\n",
      "\n",
      "Reranking\n",
      "\n",
      "|\n",
      "\n",
      "Search systems are often pipelines of multiple steps. A reranking language model is one of these steps and is tasked with scoring the relevance of a subset of results against  the  query;  the  order  of  results  is  then  changed  based  on  these  scores. Figure 8-2 shows how rerankers are different from dense retrieval in that they take an additional input: a set of search results from a previous step in the search pipeline.\n",
      "\n",
      "\n",
      "\n",
      "RAG\n",
      "\n",
      "The  growing  LLM  capability  of  text  generation  led  to  a  new  type  of  search systems that  include  a  model  that  generates  an  answer  in  response  to  a  query. Figure 8-3 shows an example of such a generative search system.\n",
      "\n",
      "Generative  search  is  a  subset  of  a  broader  type  of  category  of  systems  better called  RAG systems. These are text generation systems that incorporate search capabilities to reduce hallucinations, increase factuality, and/or ground the generation model on a specific dataset.\n",
      "\n",
      "\n",
      "\n",
      "The rest of the chapter covers these three types of systems in more detail. While these are  the  major  categories,  they  are  not  the  only  LLM  applications  in  the  domain  of search.\n",
      "\n",
      "|\n",
      "\n",
      "Semantic Search with Language Models\n",
      "\n",
      "Let's now dive into more detail on the major categories of systems that can upgrade the  search  capabilities  of  our  language  models.  We'll  start  with  dense  retrieval  and then move on through reranking and RAG.\n",
      "\n",
      "Dense Retrieval\n",
      "\n",
      "Recall that embeddings turn text into numeric representations. Those can be thought of as points in space, as we can see in Figure 8-4. Points that are close together mean that the text they represent is similar. So in this example, text 1 and text 2 are more similar  to  each  other  (because  they  are  near  each  other)  than  text  3  (because  it's farther away).\n",
      "\n",
      "\n",
      "\n",
      "This is the property that is used to build search systems. In this scenario, when a user enters a search query, we embed the query, thus projecting it into the same space as our  text  archive.  Then  we  simply  find  the  nearest  documents  to  the  query  in  that space, and those would be the search results (Figure 8-5).\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Judging  by  the  distances  in  Figure  8-5,  'text  2'  is  the  best  result  for  this  query, followed by 'text 1. ' Two questions could arise here, however:\n",
      "\n",
      " · Should text 3 even be returned as a result? That's a decision for you, the system designer. It's sometimes desirable to have a max threshold of similarity score to filter  out  irrelevant  results  (in  case  the  corpus  has  no  relevant  results  for  the query).\n",
      "\n",
      " · Are  a  query  and  its  best  result  semantically  similar?  Not  always.  This  is  why language models need to be trained on question-answer pairs to become better at retrieval. This process is explained in more detail in Chapter 10.\n",
      "\n",
      "Figure 8-6 shows how we chunk a document before proceeding to embed each chunk. Those embedding vectors are then stored in the vector database and are ready for retrieval.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Dense retrieval example\n",
      "\n",
      "Let's take a look at a dense retrieval example by using Cohere to search the Wikipedia page for the film Interstellar . In this example, we will do the following:\n",
      "\n",
      " 1. Get  the  text  we  want  to  make  searchable  and  apply  some  light  processing  to chunk it into sentences.\n",
      "\n",
      " 2. Embed the sentences.\n",
      "\n",
      " 3. Build the search index.\n",
      "\n",
      " 4. Search and see the results.\n",
      "\n",
      "Get  your  Cohere  API  key  by  signing  up  at https://oreil.ly/GxrQ1 . Paste  it  in  the following code. You will not have to pay anything to run through this example.\n",
      "\n",
      "Let's import the libraries we'll need:\n",
      "\n",
      "```\n",
      "import cohere import numpy as np import pandas as pd from tqdm import tqdm # Paste your API key here. Remember to not share publicly api_key = '' # Create and retrieve a Cohere API key from os.cohere.ai co = cohere.Client(api_key)\n",
      "```\n",
      "\n",
      "Getting the text archive and chunking it. Let's use the first section of the Wikipedia article on the film Interstellar . We'll get the text, then break it into sentences:\n",
      "\n",
      "|\n",
      "\n",
      "text = \"\"\"\n",
      "\n",
      "Interstellar is a 2014 epic science fiction film co-written, directed, and pro duced by Christopher Nolan.\n",
      "\n",
      "It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine.\n",
      "\n",
      "Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind.\n",
      "\n",
      "Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007.\n",
      "\n",
      "Caltech theoretical physicist and 2017 Nobel laureate in Physics Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar.\n",
      "\n",
      "Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm.\n",
      "\n",
      "Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles.\n",
      "\n",
      "Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects.\n",
      "\n",
      "Interstellar premiered on October 26, 2014, in Los Angeles.\n",
      "\n",
      "In the United States, it was first released on film stock, expanding to venues using digital projectors.\n",
      "\n",
      "The film had a worldwide gross over $677 million (and $773 million with subse quent re-releases), making it the tenth-highest grossing film of 2014. It received acclaim for its performances, direction, screenplay, musical score, visual effects, ambition, themes, and emotional weight.\n",
      "\n",
      "It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics. Since its premiere, Interstellar gained a cult following, and now is regarded by many sci-fi experts as one of the best science-fiction films of all time.\n",
      "\n",
      "Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades\"\"\"\n",
      "\n",
      "```\n",
      "# Split into a list of sentences texts = text.split('.') # Clean up to remove empty spaces and new lines texts = [t.strip(' \\n ') for t in texts]\n",
      "```\n",
      "\n",
      "Embedding the text chunks. Let's now embed the texts. We'll send them to the Cohere API, and get back a vector for each text:\n",
      "\n",
      "```\n",
      "# Get the embeddings response = co.embed( texts=texts, input_type=\"search_document\", ).embeddings embeds = np.array(response) print(embeds.shape)\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "This outputs (15, 4096) ,  which indicates that we have 15 vectors, each one of size 4,096.\n",
      "\n",
      "Building the search index. Before  we  can  search,  we  need  to  build  a  search  index. An  index  stores  the  embeddings  and  is  optimized  to  quickly  retrieve  the  nearest neighbors even if we have a very large number of points:\n",
      "\n",
      "```\n",
      "import faiss dim = embeds.shape index = faiss.IndexFlatL2(dim) print(index.is_trained) index.add(np.float32(embeds))\n",
      "```\n",
      "\n",
      "Search the index. We can now search the dataset using any query we want. We simply embed the query and present its embedding to the index, which will retrieve the most similar sentence from the Wikipedia article.\n",
      "\n",
      "Let's define our search function:\n",
      "\n",
      "```\n",
      "def search(query, number_of_results=3): # 1. Get the query's embedding query_embed = co.embed(texts=[query], input_type=\"search_query\",).embeddings # 2. Retrieve the nearest neighbors distances , similar_item_ids = index.search(np.float32([query_embed]), num ber_of_results) # 3. Format the results texts_np = np.array(texts) # Convert texts list to numpy for easier indexing results = pd.DataFrame(data={'texts': texts_np[similar_item_ids], 'distance': distances}) # 4. Print and return the results print(f\"Query:' { query } ' \\n Nearest neighbors:\") return results We are now ready to write a query and search the texts! query = \"how precise was the science\" results = search(query) results This produces the following output:\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "The first result has the least distance, and so is the most similar to the query. Looking at it, it answers the question perfectly. Notice that this wouldn't have been possible if we were only doing keyword search because the top result did not include the same keywords in the query.\n",
      "\n",
      "We can actually  verify  that  by  defining  a  keyword  search  function  to  compare  the two. We'll use the BM25 algorithm, which is one of the leading lexical search methods. See this notebook for the source of these code snippets:\n",
      "\n",
      "```\n",
      "from rank_bm25 import BM25Okapi from sklearn.feature_extraction import _stop_words import string def bm25_tokenizer(text): tokenized_doc = [] for token in text.lower().split(): token = token.strip(string.punctuation) if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS: tokenized_doc.append(token) return tokenized_doc tokenized_corpus = [] for passage in tqdm(texts): tokenized_corpus.append(bm25_tokenizer(passage)) bm25 = BM25Okapi(tokenized_corpus) def keyword_search(query, top_k=3, num_candidates=15): print(\"Input question:\", query) ##### BM25 search (lexical search) ##### bm25_scores = bm25.get_scores(bm25_tokenizer(query)) top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:] bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n] bm25_hits = sorted(bm25_hits, key= lambda x: x['score'], reverse= True )\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "print(f\"Top-3 lexical search (BM25) hits\") for hit in bm25_hits[0:top_k]: print(\" \\t{:.3f}\\t{} \".format(hit['score'], texts[hit['cor pus_id']].replace(\" \\n \", \" \")))\n",
      "```\n",
      "\n",
      "Now when we search for the same query, we get a different set of results from the dense retrieval search:\n",
      "\n",
      "```\n",
      "keyword_search(query = \"how precise was the science\")\n",
      "```\n",
      "\n",
      "```\n",
      "Results:\n",
      "```\n",
      "\n",
      "```\n",
      "Input question: how precise was the science Top-3 lexical search (BM25) hits 1.789 Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan 1.373 Caltech theoretical physicist and 2017 Nobel laureate in Physics Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar 0.000 It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine\n",
      "```\n",
      "\n",
      "Note that the first  result  does  not  really  answer  the  question  despite  it  sharing  the word 'science' with the query. In the next section, we'll see how adding a reranker can improve this search system. But before that, let's complete our overview of dense retrieval by looking at its caveats and go over some methods of breaking down texts into chunks.\n",
      "\n",
      "Caveats of dense retrieval\n",
      "\n",
      "It' s useful to be aware of some of the drawbacks of dense retrieval and how to address them. What happens, for example, if the texts don't contain the answer? We still get results and their distances. For example:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In  cases  like  this,  one  possible  heuristic  is  to  set  a  threshold  level-a  maximum distance  for  relevance,  for  example.  A  lot  of  search  systems  present  the  user  with the best info they can get and leave it up to the user to decide if it's relevant or not.\n",
      "\n",
      "|\n",
      "\n",
      "Tracking the information of whether the user clicked on a result (and were satisfied by it) can improve future versions of the search system.\n",
      "\n",
      "Another caveat of dense retrieval is when a user wants to find an exact match for a specific phrase. That's a case that's perfect for keyword matching. That's one reason why  hybrid  search,  which  includes  both  semantic  search  and  keyword  search,  is advised instead of relying solely on dense retrieval.\n",
      "\n",
      "Dense retrieval systems also find it challenging to work properly in domains other than the ones that they were trained on. So, for example, if you train a retrieval model on  internet  and  Wikipedia  data,  and  then  deploy  it  on  legal  texts  (without  having enough legal data as part of the training set), the model will not work as well in that legal domain.\n",
      "\n",
      "The  final  thing  we' d  like  to  point  out  is  that  this  is  a  case  where  each  sentence contained  a  piece  of  information,  and  we  showed  queries  that  specifically  ask  for that information. What about questions whose answers span multiple sentences? This highlights one of the important design parameters of dense retrieval systems: what is the best way to chunk long texts? And why do we need to chunk them in the first place?\n",
      "\n",
      "Chunking long texts\n",
      "\n",
      "One limitation  of  Transformer  language  models  is  that  they  are  limited  in  context sizes,  meaning  we  cannot  feed  them  very  long  texts  that  go  above  the  number  of words or tokens that the model supports. So how do we embed long texts?\n",
      "\n",
      "There are  several  possible  ways,  and  two  possible  approaches  shown  in  Figure  8-7 include  indexing  one  vector  per  document  and  indexing  multiple  vectors  per document.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "One vector per document. In  this  approach,  we  use  a  single  vector  to  represent  the whole document. The possibilities here include:\n",
      "\n",
      " · Embedding only a representative part of the document and ignoring the rest of the text. This may mean embedding only the title, or only the beginning of the document. This is useful to get quickly started with building a demo but it leaves a  lot  of  information  unindexed  and  therefore  unsearchable.  As  an  approach,  it may work better for documents where the beginning captures the main points of a document (think: Wikipedia article). But it's really not the best approach for a real system because a lot of information would be left out of the index and would be unsearchable.\n",
      "\n",
      " · Embedding the document in chunks, embedding those chunks, and then aggregating those chunks into a single vector. The usual method of aggregation here is to average those vectors. A downside of this approach is that it results in a highly compressed vector that loses a lot of the information in the document.\n",
      "\n",
      "This approach can satisfy some information needs, but not others. A lot of the time, a search is for a specific piece of information contained in an article, which is better captured if the concept had its own vector.\n",
      "\n",
      "Multiple vectors per document. In this approach, we chunk the document into smaller pieces,  and  embed  those  chunks.  Our  search  index  then  becomes  that  of  chunk embeddings, not entire document embeddings. Figure 8-8 shows a number of possible text chunking approaches.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "The chunking approach is better because it has full coverage of the text and because the vectors tend to capture individual concepts inside the text. This leads to a more expressive search index. Figure 8-9 shows a number of possible approaches.\n",
      "\n",
      "\n",
      "\n",
      "The best way of chunking a long text will depend on the types of texts and queries your system anticipates. Approaches include:\n",
      "\n",
      " · Each sentence is a chunk. The issue here is this could be too granular and the vectors don't capture enough of the context.\n",
      "\n",
      " · Each  paragraph  is  a  chunk.  This  is  great  if  the  text  is  made  up  of  short  paragraphs. Otherwise, it may be that every 3-8 sentences is a chunk.\n",
      "\n",
      " · Some chunks derive a lot of their meaning from the text around them. So we can incorporate some context via:\n",
      "\n",
      " -Adding the title of the document to the chunk.\n",
      "\n",
      " -Adding some of the text before and after them to the chunk. This way, the chunks can overlap so they include some surrounding text that also appears in adjacent chunks. This is what we can see in Figure 8-10.\n",
      "\n",
      "Expect more chunking strategies to arise as the field develops-some of which may even use LLMs to dynamically split a text into meaningful chunks.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Nearest neighbor search versus vector databases\n",
      "\n",
      "Once  the  query  is  embedded,  we  need  to  find  the  nearest  vectors  to  it  from  our text archive as we can see in Figure 8-11. The most straightforward way to find the nearest  neighbors  is  to  calculate  the  distances  between  the  query  and  the  archive. That  can  easily  be  done  with  NumPy  and  is  a  reasonable  approach  if  you  have thousands or tens of thousands of vectors in your archive.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "As you scale beyond to the millions of vectors, an optimized approach for retrieval is to rely on approximate nearest neighbor search libraries like Annoy or FAISS. These allow you to retrieve results from massive indexes in milliseconds and some of them can improve their performance by utilizing GPUs and scaling to clusters of machines to serve very large indices.\n",
      "\n",
      "Another class of vector retrieval systems are vector databases like Weaviate or Pinecone. A vector database allows you to add or delete vectors without having to rebuild the index. They also provide ways to filter your search or customize it in ways beyond merely vector distances.\n",
      "\n",
      "Fine-tuning embedding models for dense retrieval\n",
      "\n",
      "Just as we discussed in Chapter 4 on text classification, we can improve the performance  of  an  LLM  on  a  task  using  fine-tuning.  As  in  that  case,  retrieval  needs  to optimize  text  embeddings  and  not  simply  token  embeddings.  The  process  for  this fine-tuning is to get training data composed of queries and relevant results.\n",
      "\n",
      "Let's  look  at  one  example from our dataset, the sentence 'Interstellar premiered on October 26, 2014, in Los Angeles. ' Two possible queries where this is a relevant result are:\n",
      "\n",
      " · Relevant query 1: 'Interstellar release date'\n",
      "\n",
      " · Relevant query 2: 'When did Interstellar premier'\n",
      "\n",
      "The fine-tuning process aims to make the embeddings of these queries close to the embedding of the resulting sentence. It also needs to see negative examples of queries that are not relevant to the sentence, for example:\n",
      "\n",
      " · Irrelevant query: 'Interstellar cast'\n",
      "\n",
      "With these examples, we now have three pairs-two positive pairs and one negative pair.  Let's  assume,  as  we  can  see  in  Figure  8-12,  that  before  fine-tuning,  all  three queries  have  the  same  distance  from  the  result  document.  That's  not  far-fetched because they all talk about Interstellar .\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "The fine-tuning step works to make the relevant queries closer to the document and at the same time make irrelevant queries farther from the document. We can see this effect in Figure 8-13.\n",
      "\n",
      "\n",
      "\n",
      "Reranking\n",
      "\n",
      "A  lot  of  organizations  have  already  built  search  systems.  For  those  organizations, an  easier  way  to  incorporate  language  models  is  as  a  final  step  inside  their  search\n",
      "\n",
      "|\n",
      "\n",
      "pipeline. This step is tasked with changing the order of the search results based on relevance  to  the  search  query.  This  one  step  can  vastly  improve  search  results  and it' s  in fact what Microsoft Bing added to achieve the improvements to search results using BERT-like models. Figure 8-14 shows the structure of a rerank search system serving as the second stage in a two-stage search system.\n",
      "\n",
      "\n",
      "\n",
      "Reranking example\n",
      "\n",
      "A  reranker  takes  in  the  search  query  and  a  number  of  search  results,  and  returns the optimal ordering of these documents so the most relevant ones to the query are higher  in  ranking.  Cohere's  Rerank  endpoint  is  a  simple  way  to  start  using  a  first reranker.  We  simply  pass  it  the  query  and  texts  and  get  the  results  back.  We  don't need to train or tune it:\n",
      "\n",
      "```\n",
      "query = \"how precise was the science\" results = co.rerank(query=query, documents=texts, top_n=3, return_docu ments= True ) results.results\n",
      "```\n",
      "\n",
      "We can print these results:\n",
      "\n",
      "```\n",
      "for idx, result in enumerate(results.results): print(idx, result.relevance_score , result.document.text)\n",
      "```\n",
      "\n",
      "Output:\n",
      "\n",
      "```\n",
      "0 0.1698185 It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics 1 0.07004896 The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014 2 0.0043994132 Caltech theoretical physicist and 2017 Nobel laureate in Physics Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "This shows the reranker is much more confident about the first result, assigning it a relevance score of 0.16, while the other results are scored much lower in relevance.\n",
      "\n",
      "In this basic example, we passed our reranker all 15 of our documents. More often, however,  our  index  would  have  thousands  or  millions  of  entries,  and  we  need  to shortlist,  say  one  hundred  or  one  thousand  results  and  then  present  those  to  the reranker. This shortlisting step is called the first stage of the search pipeline.\n",
      "\n",
      "The first-stage retriever can be keyword search, dense retrieval, or better yet-hybrid search that uses both of them. We can revisit our previous example to see how adding a reranker after a keyword search system improves its performance.\n",
      "\n",
      "Let's tweak our keyword search function so it retrieves a list of the top 10 results using keyword search, then use rerank to choose the top 3 results from those 10:\n",
      "\n",
      "```\n",
      "def keyword_and_reranking_search(query, top_k=3, num_candidates=10): print(\"Input question:\", query) ##### BM25 search (lexical search) ##### bm25_scores = bm25.get_scores(bm25_tokenizer(query)) top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:] bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n] bm25_hits = sorted(bm25_hits, key= lambda x: x['score'], reverse= True ) print(f\"Top-3 lexical search (BM25) hits\") for hit in bm25_hits[0:top_k]: print(\" \\t{:.3f}\\t{} \".format(hit['score'], texts[hit['cor pus_id']].replace(\" \\n \", \" \"))) #Add re-ranking docs = [texts[hit['corpus_id']] for hit in bm25_hits] print(f\" \\n Top-3 hits by rank-API ( { len(bm25_hits) } BM25 hits re-ranked)\") results = co.rerank(query=query, documents=docs, top_n=top_k, return_docu ments= True ) # print(results.results) for hit in results.results: # print(hit) print(\" \\t{:.3f}\\t{} \".format(hit.relevance_score, hit.docu ment.text.replace(\" \\n \", \" \")))\n",
      "```\n",
      "\n",
      "Now we can send our query and check the results of keyword search and then the result  of  keyword  search  shortlisting  its  top  10  results,  then  pass  them  on  to  the reranker:\n",
      "\n",
      "```\n",
      "keyword_and_reranking_search(query = \"how precise was the science\")\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "Results:\n",
      "\n",
      "```\n",
      "Input question: how precise was the science Top-3 lexical search (BM25) hits 1.789 Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan 1.373 Caltech theoretical physicist and 2017 Nobel laureate in Physics Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar 0.000 Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects Top-3 hits by rank-API (10 BM25 hits re-ranked) 0.004 Caltech theoretical physicist and 2017 Nobel laureate in Physics Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar 0.004 Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind 0.003 Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007\n",
      "```\n",
      "\n",
      "We  see  that  keyword  search  assigns  scores  to  only  two  results  that  share  some  of the  keywords.  In  the  second  set  of  results,  the  reranker  elevates  the  second  result appropriately  as  the  most  relevant  result  for  the  query.  This  is  a  toy  example  that gives us a glimpse of the effect, but in practice, such a pipeline significantly improves search  quality.  On  a  multilingual  benchmark  like  MIRACL,  a  reranker  can  boost performance from 36.5 to 62.8, measured as nDCG@10 (more on evaluation later in this chapter).\n",
      "\n",
      "Open source retrieval and reranking with sentence transformers\n",
      "\n",
      "If  you  want  to  locally  set  up  retrieval  and  reranking  on  your  own  machine,  then you can use the Sentence Transformers library. Refer to the documentation at https:// oreil.ly/jJOhV for setup. Check the 'Retrieve &amp; Re-Rank' section for instructions and code examples for how to conduct these steps in the library.\n",
      "\n",
      "|\n",
      "\n",
      "How reranking models work\n",
      "\n",
      "One  popular  way  of  building  LLM  search  rerankers  is  to  present  the  query  and each  result  to  an  LLM  working  as  a cross-encoder .  This  means  that  a  query  and possible  result  are  presented  to  the  model  at  the  same  time  allowing  the  model  to view both these texts before it assigns a relevance score, as we can see in Figure 8-15. All of the documents are processed simultaneously as a batch yet each document is evaluated against the query independently. The scores then determine the new order of the results. This method is described in more detail in a paper titled 'Multi-stage document ranking with BERT' and is sometimes referred to as monoBERT.\n",
      "\n",
      "\n",
      "\n",
      "This  formulation  of  search  as  relevance  scoring  basically  boils  down  to  being  a classification problem. Given those inputs, the model outputs a score from 0-1 where 0 is irrelevant and 1 is highly relevant. This should be familiar from our classification discussions in Chapter 4.\n",
      "\n",
      "To learn more about the development of using LLMs for search, \" Pretrained transformers  for  text  tanking:  BERT  and  beyond\"  is  a  highly  recommended  look  at  the developments of these models until about 2021.\n",
      "\n",
      "Retrieval Evaluation Metrics\n",
      "\n",
      "Semantic search is evaluated using metrics from the Information Retrieval (IR) field. Let's discuss one of these popular metrics: mean average precision (MAP).\n",
      "\n",
      "Evaluating  search  systems  needs  three  major  components:  a  text  archive,  a  set  of queries, and relevance judgments indicating which documents are relevant for each query. We see these components in Figure 8-16.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Using this test suite, we can proceed to explore evaluating search systems. Let's start with a simple example. Let's assume we pass query 1 to two different search systems. And get two sets of results. Say we limit the number of results to three, as we can see in Figure 8-17.\n",
      "\n",
      "\n",
      "\n",
      "To tell which is a better system, we turn to the relevance judgments that we have for the query. Figure 8-18 shows which of the returned results are relevant.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "This shows us a clear case where system 1 is better than system 2. Intuitively, we may just count how many relevant results each system retrieved. System 1 got two out of three correct, and system 2 got only one out of three correct. But what about a case like  Figure  8-19  where  both  systems  only  get  one  relevant  result  out  of  three,  but they're in different positions?\n",
      "\n",
      "\n",
      "\n",
      "In this case, we can intuit that system 1 did a better job than system 2 because the result in the first position (the most important position) is correct. But how can we assign a number or score to how much better that result is? Mean average precision is a measure that is able to quantify this distinction.\n",
      "\n",
      "|\n",
      "\n",
      "One  common  way  to  assign  numeric  scores  in  this  scenario  is  average  precision, which evaluates system 1's result for the query to be 1 and system 2's to be 0.3. So let's see how average precision is calculated to evaluate one set of results, and then how it's aggregated to evaluate a system across all the queries in the test suite.\n",
      "\n",
      "Scoring a single query with average precision\n",
      "\n",
      "To score a search system on this query, we can focus on scoring the relevant documents. Let's start by looking at a query that only has one relevant document in the test suite.\n",
      "\n",
      "The first one is easy: the search system placed the relevant result (the only available one for this query) at the top. This gets the system the perfect score of 1. Figure 8-20 shows this calculation: looking at the first position, we have a relevant result leading to  a  precision  at  position  1  of  1.0  (calculated  as  the  number  of  relevant  results  at position 1, divided by the position we're currently looking at).\n",
      "\n",
      "\n",
      "\n",
      "Since we're only scoring relevant documents we can ignore the scores of nonrelevant documents  and  stop  our  calculation  here.  What  if  the  system  actually  placed  the only relevant result at the third position, however? How would that affect the score? Figure 8-21 shows how that results in a penalty.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Let's now look at a query with more than one relevant document. Figure 8-22 shows that calculation and how averaging now comes into the picture.\n",
      "\n",
      "\n",
      "\n",
      "Scoring across multiple queries with mean average precision\n",
      "\n",
      "Now that we're familiar with precision at k and average precision, we can extend this knowledge to a metric that can score a search system against all the queries in our test  suite.  That  metric  is  called  mean  average  precision.  Figure  8-23  shows  how  to calculate this metric by taking the mean of the average precisions of each query.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "You may be wondering why the same operation is called 'mean' and 'average.' It's likely an aesthetic choice because MAP sounds better than average average precision.\n",
      "\n",
      "Now we have a single metric that we can use to compare different systems. If you want  to  learn  more  about  evaluation  metrics,  see  the  'Evaluation  in  Information Retrieval'  chapter  of Introduction  to  Information  Retrieval (Cambridge  University Press) by Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze.\n",
      "\n",
      "In  addition  to  mean  average  precision,  another  metric  commonly  used  for  search systems is normalized discounted cumulative gain (nDCG), which is more nuanced in  that  the  relevance  of  documents is not binary (relevant versus not relevant) and one  document  can  be  labeled  as  more  relevant  than  another  in  the  test  suite  and scoring mechanism.\n",
      "\n",
      "Retrieval-Augmented Generation (RAG)\n",
      "\n",
      "The  mass  adoption  of  LLMs  quickly  led  to  people  asking  them  questions  and expecting  factual  answers.  While  the  models  can  answer  some  questions  correctly, they  also  confidently  answer  lots  of  questions  incorrectly.  The  leading  method  the industry turned to remedy this behavior is RAG, described in the paper 'RetrievalAugmented Generation for Knowledge-Intensive NLP Tasks' (2020)  and illustrated 1 in Figure 8-24.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "RAG systems  incorporate  search  capabilities  in  addition  to  generation  capabilities. They  can  be  seen  as  an  improvement  to  generation  systems  because  they  reduce their hallucinations and improve their factuality. They also enable use cases of 'chat with my data' that consumers and companies can use to ground an LLM on internal company data, or a specific data source of interest (e.g., chatting with a book).\n",
      "\n",
      "This also extends to search systems. More search engines are incorporating an LLM to summarize results or answer questions submitted to the search engine. Examples include Perplexity, Microsoft Bing AI, and Google Gemini.\n",
      "\n",
      "From Search to RAG\n",
      "\n",
      "Let's now turn our search system into a RAG system. We do that by adding an LLM to  the  end  of  the  search  pipeline.  We  present  the  question  and  the  top  retrieved documents to the LLM, and ask it to answer the question given the context provided by the search results. We can see an example in Figure 8-25.\n",
      "\n",
      "This  generation  step  is  called grounded  generation because  the  retrieved  relevant information we provide the LLM establishes a certain context that grounds the LLM in the domain we're interested in. Figure 8-26 shows how grounded generation fits after search if we continue our embeddings search example from earlier.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Example: Grounded Generation with an LLM API\n",
      "\n",
      "Let's look at how to add a grounded generation step after the search results to create our  first  RAG  system.  For  this  example,  we'll  use  Cohere's  managed  LLM,  which builds on the search systems we've seen earlier in the chapter. We'll use embedding search to retrieve the top documents, then we'll pass those to the co.chat endpoint along with the questions to provide a grounded answer:\n",
      "\n",
      "```\n",
      "query = \"income generated\" # 1- Retrieval # We'll use embedding search. But ideally we'd do hybrid results = search(query) # 2- Grounded Generation docs_dict = [{'text': text} for text in results['texts']] response = co.chat( message = query, documents=docs_dict ) print(response.text)\n",
      "```\n",
      "\n",
      "Result:\n",
      "\n",
      "```\n",
      "The film generated a worldwide gross of over $677 million, or $773 million with subsequent re-releases.\n",
      "```\n",
      "\n",
      "We are highlighting some of the text because the model indicated the source for these spans of text to be the first document we passed in:\n",
      "\n",
      "```\n",
      "citations=[ChatCitation(start=21, end=36, text='worldwide gross', docu-ment_ids=['doc_0']), ChatCitation(start=40, end=57, text='over $677 million', document_ids=['doc_0']), ChatCitation(start=62, end=103, text='$773 million with subsequent re-releases.', document_ids=['doc_0'])] documents=[{'id': 'doc_0', 'text': 'The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenthhighest grossing film of 2014'}]\n",
      "```\n",
      "\n",
      "Example: RAG with Local Models\n",
      "\n",
      "Let  us  now  replicate  this  basic  functionality  with  local  models.  We  will  lose  the ability  to  do  span  citations  and  the  smaller  local  model  isn't  going  to  work  as  well as  the  larger  managed  model,  but  it's  useful  to  demonstrate  the  flow.  We'll  start  by downloading a quantized model.\n",
      "\n",
      "|\n",
      "\n",
      "Loading the generation model\n",
      "\n",
      "We start by downloading our model:\n",
      "\n",
      "```\n",
      "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/ Phi-3-mini-4k-instruct-fp16.gguf\n",
      "```\n",
      "\n",
      "Using llama.cpp , llama-cpp-python ,  and  LangChain,  we  load  the  text  generation model:\n",
      "\n",
      "```\n",
      "from langchain import LlamaCpp # Make sure the model path is correct for your system! llm = LlamaCpp( model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\", n_gpu_layers=-1, max_tokens=500, n_ctx=2048, seed=42, verbose= False )\n",
      "```\n",
      "\n",
      "Loading the embedding model\n",
      "\n",
      "Let's  now  load  an  embedding  language  model.  In  this  example,  we  will  choose  the BAAI/bge-small-en-v1.5  model.  At  the  time  of  writing,  it  is  high  on  the  MTEB leaderboard for embedding models and relatively small:\n",
      "\n",
      "```\n",
      "from langchain.embeddings.huggingface import HuggingFaceEmbeddings # Embedding model for converting text to numerical representations embedding_model = HuggingFaceEmbeddings( model_name='thenlper/gte-small' ) We can now use the embedding model to set up our vector database: from langchain.vectorstores import FAISS # Create a local vector database db = FAISS.from_texts(texts, embedding_model)\n",
      "```\n",
      "\n",
      "The RAG prompt\n",
      "\n",
      "A prompt template plays a vital part in the RAG pipeline. It is the central place where we  communicate  the  relevant  documents  to  the  LLM.  To  do  so,  we  will  create  an additional input variable named context that can provide the LLM with the retrieved documents:\n",
      "\n",
      "|\n",
      "\n",
      "from langchain import PromptTemplate\n",
      "\n",
      "# Create a prompt template template = \"\"\"&lt;|user|&gt; Relevant information: {context} Provide a concise answer the following question using the relevant information provided above: {question}&lt;|end|&gt; &lt;|assistant|&gt;\"\"\" prompt = PromptTemplate( template=template, input_variables=[\"context\", \"question\"] ) from langchain.chains import RetrievalQA # RAG pipeline rag = RetrievalQA.from_chain_type( llm=llm, chain_type='stuff', retriever=db.as_retriever(), chain_type_kwargs={ \"prompt\": prompt }, verbose= True ) Now we're ready to call the model and ask it a question: rag.invoke('Income generated') Result: The Income generated by the film in 2014 was over $677 million worldwide. This made it the tenth-highest grossing film of that year. It should be noted, however, this figure includes both initial ticket sales as well as any subsequent re-releases. With these additional releases, total earnings surged to approximately $773 million. The release format transitioned from traditional film stock projection in theaters to digital projectors once it was expanded to various venues in the United States. This shift might have contributed to wider audience reach and potentially higher grossing figures over time. However, specific data on how this affected total earnings isn't provided in the information above.\n",
      "\n",
      "As always, we can adjust the prompt to control the model's generation (e.g., answer length and tone).\n",
      "\n",
      "|\n",
      "\n",
      "Advanced RAG Techniques\n",
      "\n",
      "There are several additional techniques to improve the performance of RAG systems. Some of them are laid out here.\n",
      "\n",
      "Query rewriting\n",
      "\n",
      "If  the  RAG  system  is  a  chatbot,  the  preceding  simple  RAG  implementation  would likely struggle with the search step if a question is too verbose, or to refer to context in  previous  messages  in  the  conversation.  This  is  why  it's  a  good  idea  to  use  an LLM to  rewrite  the  query  into  one  that  aids  the  retrieval  step  in  getting  the  right information. An example of this is a message such as:\n",
      "\n",
      "User Question: 'We have an essay due tomorrow. We have to write about some animal. I love penguins. I could write about them. But I could also write about dolphins. Are they animals? Maybe. Let's do dolphins. Where do they live for example?'\n",
      "\n",
      "This should actually be rewritten into a query like:\n",
      "\n",
      "Query: 'Where do dolphins live'\n",
      "\n",
      "This  rewriting  behavior  can  be  done  through  a  prompt  (or  through  an  API  call). Cohere's API, for example, has a dedicated query-rewriting mode for co.chat .\n",
      "\n",
      "Multi-query RAG\n",
      "\n",
      "The next improvement we can introduce is to extend the query rewriting to be able to search multiple queries if more than one is needed to answer a specific question. Take for example:\n",
      "\n",
      "User Question: 'Compare the financial results of Nvidia in 2020 vs. 2023'\n",
      "\n",
      "We may find one document that contains the results for both years, but more likely, we're better off making two search queries:\n",
      "\n",
      "Query 1: 'Nvidia 2020 financial results'\n",
      "\n",
      "Query 2: 'Nvidia 2023 financial results'\n",
      "\n",
      "We then present the top results of both queries to the model for grounded generation. An additional small improvement here is to also give the query rewriter the option to determine if no search is required and if it can directly generate a confident answer without searching.\n",
      "\n",
      "|\n",
      "\n",
      "Multi-hop RAG\n",
      "\n",
      "A  more  advanced  question  may  require  a  series  of  sequential  queries.  Take  for example a question like:\n",
      "\n",
      "User Question: 'Who are the largest car manufacturers in 2023? Do they each make EVs or not?'\n",
      "\n",
      "To answer this, the system must first search for:\n",
      "\n",
      "Step 1, Query 1: 'largest car manufacturers 2023'\n",
      "\n",
      "Then after it gets this information (the result being Toyota, Volkswagen, and Hyundai), it should ask follow-up questions:\n",
      "\n",
      "Step 2, Query 1: 'Toyota Motor Corporation electric vehicles'\n",
      "\n",
      "Step 2, Query 2: 'Volkswagen AG electric vehicles'\n",
      "\n",
      "Step 2, Query 3: 'Hyundai Motor Company electric vehicles'\n",
      "\n",
      "Query routing\n",
      "\n",
      "An additional enhancement is to give the model the ability to search multiple data sources. We can, for example, specify for the model that if it gets a question about HR,  it  should  search  the  company's  HR  information  system  (e.g.,  Notion)  but  if the question is about customer data, that it should search the customer relationship management (CRM) (e.g., Salesforce).\n",
      "\n",
      "Agentic RAG\n",
      "\n",
      "You may be able to now see that the list of previous enhancements slowly delegates more and more responsibility to the LLM to solve more and more complex problems. This relies on the LLM's capability to gauge the required information needs as well as  its  ability  to  utilize  multiple  data  sources.  This  new  nature  of  the  LLM  starts  to become closer and closer to an agent that acts on the world. The data sources can also now be abstracted into tools. We saw, for example, that we can search Notion, but by the same token, we should be able to post to Notion as well.\n",
      "\n",
      "Not all LLMs will have the RAG capabilities mentioned here. At the time of writing, likely only the largest managed models may be able to attempt this behavior. Thankfully, Cohere's Command R+ excels at these tasks and is available as an open-weights model as well.\n",
      "\n",
      "|\n",
      "\n",
      "RAG Evaluation\n",
      "\n",
      "There are still ongoing developments in how to evaluate RAG models. A good paper to read on this topic is 'Evaluating verifiability in generative search engines' (2023), which runs human evaluations on different generative search systems. 2\n",
      "\n",
      "It evaluates results along four axes:\n",
      "\n",
      "Fluency\n",
      "\n",
      "Whether the generated text is fluent and cohesive.\n",
      "\n",
      "Perceived utility\n",
      "\n",
      "Whether the generated answer is helpful and informative.\n",
      "\n",
      "Citation recall\n",
      "\n",
      "The proportion of generated statements about the external world that are fully supported by their citations.\n",
      "\n",
      "Citation precision\n",
      "\n",
      "The proportion of generated citations that support their associated statements.\n",
      "\n",
      "While  human  evaluation  is  always  preferred,  there  are  approaches  that  attempt  to automate these evaluations by having a capable LLM act as a judge (called LLM-as-ajudge ) and score the different generations along the different axes. Ragas is a software library that does exactly this. It also scores some additional useful metrics like:\n",
      "\n",
      "Faithfulness\n",
      "\n",
      "Whether the answer is consistent with the provided context\n",
      "\n",
      "Answer relevance\n",
      "\n",
      "How relevant the answer is to the question\n",
      "\n",
      "The Ragas documentation site provides more details about the formulas to actually calculate these metrics.\n",
      "\n",
      "|\n",
      "\n",
      "Summary\n",
      "\n",
      "In  this  chapter,  we  looked  at  different  ways  of  using  language  models  to  improve existing search systems and even be the core of new, more powerful search systems. These include:\n",
      "\n",
      " · Dense  retrieval,  which  relies  on  the  similarity  of  text  embeddings.  These  are systems that embed a search query and retrieve the documents with the nearest embeddings to the query's embedding.\n",
      "\n",
      " · Rerankers, systems (like monoBERT) that look at a query and candidate results and score the relevance of each document to that query. These relevance scores are then used to order the shortlisted results according to their relevance to the query, often producing an improved results ranking.\n",
      "\n",
      " · RAG, where search systems have a generative LLM at the end of the pipeline to formulate an answer based on retrieved documents while citing sources.\n",
      "\n",
      "We also looked at one of the possible methods of evaluating search systems. Mean average precision allows us to score search systems to be able to compare across a test  suite  of  queries  and  their  known  relevance to the test queries. Evaluating RAG systems requires multiple axes, however, like faithfulness, fluency, and others that can be evaluated by humans or by LLM-as-a-judge.\n",
      "\n",
      "In the next chapter, we will explore how language models can be made multimodal and reason not just about text but images as well.\n",
      "\n",
      "|\n",
      "\n",
      "Multimodal Large Language Models\n",
      "\n",
      "When you think about large language models (LLMs), multimodality might not be the first  thing  that  comes to mind. After all, they are language models! But we can quickly see that models can be much more useful if they're able to handle types of data other than text. It's very useful, for example, if a language model is able to glance at  a  picture  and answer questions about it. A model that is able to handle text and images (each of which is called a modality ) is said to be multimodal, as we can see in Figure 9-1.\n",
      "\n",
      "\n",
      "\n",
      "We have seen all manner of emerging behaviors rising from LLMs, from generalization capabilities and reasoning to arithmetic and linguistics. As models grow larger and smarter, so do their skill sets. 1\n",
      "\n",
      "The ability to receive and reason with multimodal input might further increase and help emerge capabilities that were previously locked. In practice, language does not solely  live  in  a  vacuum.  As  an  example,  your  body  language,  facial  expressions, intonation, etc. are all methods of communication that enhance the spoken word.\n",
      "\n",
      "The same thing applies to LLMs; if we can enable them to reason about multimodal information, their capabilities might increase and we become able to deploy them to solve new kinds of problems.\n",
      "\n",
      "In  this  chapter,  we  will  explore  a  number  of  different  LLMs  that  have  multimodal capabilities  and  what  that  means  for  practical  use  cases.  We  will  start  by  exploring how images are converted to numerical representations using an adaptation of the original Transformer technique. Then, we will show how LLMs can be extended to include vision tasks using this Transformer.\n",
      "\n",
      "Transformers for Vision\n",
      "\n",
      "Throughout the chapters of this book, we have seen the success of using Transformerbased  models  for  a  variety  of  language  modeling  tasks,  from  classification  and clustering  to  search  and  generative  modeling.  So  it  might  not  be  surprising  that researchers  have  been  looking  at  a  way  to  generalize  some  of  the  Transformer's success to the field of computer vision.\n",
      "\n",
      "The method they came up with is called the Vision Transformer (ViT), which has been  shown  to  do  tremendously  well  on  image  recognition  tasks  compared  to  the previously default convolutional neural networks (CNNs).  Like the original Trans2 former, ViT is used to transform unstructured data, an image, into representations that can be used for a variety of tasks, like classification, as illustrated in Figure 9-2.\n",
      "\n",
      "ViT relies on an important component of the Transformer architecture, namely the encoder. As we saw in Chapter 1, the encoder is responsible for converting textual input  into  numerical  representations  before  being  passed  to  the  decoder.  However, before  the  encoder  can  perform  its  duties,  the  textual  input  needs  to  be  tokenized first, as is illustrated in Figure 9-3.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Since an image does not consist of words this tokenization process cannot be used for visual data. Instead, the authors of ViT came up with a method for tokenizing images into 'words, ' which allowed them to use the original encoder structure.\n",
      "\n",
      "Imagine  that  you  have  an  image  of  a  cat.  This  image  is  represented  by  a  number of  pixels,  let's  say  512  ×  512  pixels.  Each  individual  pixel  does  not  convey  much information but when you combine patches of pixels, you slowly start to see more information.\n",
      "\n",
      "|\n",
      "\n",
      "ViT uses a principle much like that. Instead of splitting up text into tokens, it converts the  original  image  into  patches  of  images.  In  other  words,  it  cuts  the  image  into  a number of pieces horizontally and vertically as illustrated in Figure 9-4.\n",
      "\n",
      "\n",
      "\n",
      "Just  like  we  are  converting  text  into  tokens  of  text,  we  are  converting  an  image into  patches  of  images.  The  flattened  input  of  image  patches  can  be  thought  of  as the  tokens  in  a  piece  of  text.  However,  unlike  tokens,  we  cannot  just  assign  each patch with an ID since these patches will rarely be found in other images, unlike the vocabulary of a text.\n",
      "\n",
      "Instead,  the  patches  are  linearly  embedded  to  create  numerical  representations, namely embeddings. These can then be used as the input of a Transformer model. That way, the patches of images are treated the same way as tokens. The full process is illustrated in Figure 9-5.\n",
      "\n",
      "For illustrative purposes, the images in the examples were patched into 3 × 3 patches but the original implementation used 16 × 16 patches. After all, the paper is called ' An Image is Worth 16x16 Words.'\n",
      "\n",
      "What is so interesting about this approach is that the moment the embeddings are passed to the encoder, they are treated as if they were textual tokens. From that point forward, there is no difference in how a text or image trains.\n",
      "\n",
      "Due to these similarities, the ViT is often used to make all kinds of language models multimodal. One of the most straightforward ways to use it is during the training of embedding models.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Multimodal Embedding Models\n",
      "\n",
      "In  previous  chapters,  we  used  embedding  models  to  capture  the  semantic  content of  textual  representations,  such  as  papers  and  documents.  We  saw  that  we  could use these embeddings or numerical representations to find similar documents, apply classification tasks, and even perform topic modeling.\n",
      "\n",
      "As  we  have  seen  many  times  before,  embeddings  often  are  an  important  driver behind  LLM  applications.  They  are  an  efficient  method  for  capturing  large-scale information and searching for the needle in the haystack of information.\n",
      "\n",
      "That  said,  we  have  looked  at  text-only  embedding  models  thus  far,  which  focus on generating embeddings for textual representations. Although embedding models exist  for  solely  embedding  imagery,  we  will  look  at  embedding  models  that  can capture both textual as well as visual representations. We illustrate this in Figure 9-6.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "An advantage is that this allows for comparing multimodal representations since the resulting embeddings lie in the same vector space (Figure 9-7). For instance, using such a multimodal embedding model, we can find images based on input text. What images would we find if we search for images similar to 'pictures of a puppy'? Vice versa would also be possible. Which documents are best related to this question?\n",
      "\n",
      "\n",
      "\n",
      "There are a number of multimodal embedding models, but the most well-known and currently most-used model is Contrastive Language-Image Pre-training (CLIP).\n",
      "\n",
      "|\n",
      "\n",
      "CLIP: Connecting Text and Images\n",
      "\n",
      "CLIP is an embedding model that can compute embeddings of both images and texts. The resulting embeddings lie in the same vector space, which means that the embeddings  of  images  can  be  compared  with  the  embeddings  of  text.   This  comparison 3 capability makes CLIP, and similar models, usable for tasks such as:\n",
      "\n",
      "Zero-shot classification\n",
      "\n",
      "We can compare the embedding of an image with that of the description of its possible classes to find which class is most similar.\n",
      "\n",
      "Clustering\n",
      "\n",
      "Cluster both images and a collection of keywords to find which keywords belong to which sets of images.\n",
      "\n",
      "Search\n",
      "\n",
      "Across billions of texts or images, we can quickly find what relates to an input text or image.\n",
      "\n",
      "Generation\n",
      "\n",
      "Use  multimodal  embeddings  to  drive  the  generation  of  images  (e.g.,  stable diffusion ). 4\n",
      "\n",
      "How Can CLIP Generate Multimodal Embeddings?\n",
      "\n",
      "The  procedure  of  CLIP  is  actually  quite  straightforward.  Imagine  that  you  have  a dataset with millions of images alongside captions as we illustrate in Figure 9-8.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "This dataset can be used to create two representations for each pair, the image and its caption. To do so, CLIP uses a text encoder to embed text and an image encoder to embed images. As is shown in Figure 9-9, the result is an embedding for both the image and its corresponding caption.\n",
      "\n",
      "\n",
      "\n",
      "The pair of embeddings that are generated are compared through cosine similarity. As we saw in Chapter 4, cosine similarity is the cosine of the angle between vectors, which is calculated through the dot product of the embeddings and divided by the product of their lengths.\n",
      "\n",
      "When we start training, the similarity between the image embedding and text embedding will  be  low  as  they  are  not  yet  optimized  to  be  within  the  same  vector  space. During training,  we  optimize  for  the  similarity  between  the  embeddings  and  want to maximize them for similar image/caption pairs and minimize them for dissimilar image/caption pairs (Figure 9-10).\n",
      "\n",
      "After calculating their similarity,  the  model  is  updated  and  the  process  starts  again with  new  batches  of  data  and  updated  representations  (Figure  9-11).  This  method is  called contrastive  learning ,  and  we  will  go  in  depth  into  its  inner  workings  in Chapter 10 where we will create our own embedding model.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Eventually,  we  expect  the  embedding  of  an  image  of  a  cat  would  be  similar  to  the embedding of the phrase 'a picture of a cat. ' As we will see in Chapter 10, to make sure  the  representations  are  as  accurate  as  possible,  negative  examples  of  images and  captions  that  are  not  related  should  also  be  included  in  the  training  process. Modeling similarity is not only knowing what makes things similar to one another, but also what makes them different and dissimilar.\n",
      "\n",
      "OpenCLIP\n",
      "\n",
      "For our next example, we are going to be using models from the open source variant of  CLIP ,  namely  OpenCLIP.  Using  OpenCLIP ,  or  any  CLIP  model,  boils  down  to two things: processing the textual and image inputs before passing them to the main model.\n",
      "\n",
      "Before  doing  so,  let's  take  a  look  at  a  small  example  where  we  will  be  using  one of  the  images  we  have  seen  before,  namely,  an  AI-generated  image  (through  stable diffusion) of a puppy playing in the snow, as illustrated in Figure 9-12:\n",
      "\n",
      "puppy_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-LargeLanguage-Models/main/chapter09/images/puppy.png\" image = Image.open(urlopen(puppy_path)).convert(\"RGB\")\n",
      "\n",
      "```\n",
      "from urllib.request import urlopen from PIL import Image # Load an AI-generated image of a puppy playing in the snow caption = \"a puppy playing in the snow\"\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "Since we have a caption for this image, we can use OpenCLIP to generate embeddings for both.\n",
      "\n",
      "|\n",
      "\n",
      "To do so, we load in three models:\n",
      "\n",
      " · A tokenizer for tokenizing the textual input\n",
      "\n",
      " · A preprocessor to preprocess and resize the image\n",
      "\n",
      " · The main model that converts the previous outputs to embeddings\n",
      "\n",
      "from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel\n",
      "\n",
      "```\n",
      "model_id = \"openai/clip-vit-base-patch32\" # Load a tokenizer to preprocess the text # Load a processor to preprocess the images clip_processor = CLIPProcessor.from_pretrained(model_id) # Main model for generating text and image embeddings\n",
      "```\n",
      "\n",
      "```\n",
      "clip_tokenizer = CLIPTokenizerFast.from_pretrained(model_id) model = CLIPModel.from_pretrained(model_id)\n",
      "```\n",
      "\n",
      "After having loaded in the models, preprocessing our input is straightforward. Let's start with the tokenizer and see what happens if we preprocess our input:\n",
      "\n",
      "```\n",
      "# Tokenize our input inputs = clip_tokenizer(caption, return_tensors=\"pt\") inputs\n",
      "```\n",
      "\n",
      "This outputs a dictionary that contains the IDs of the input:\n",
      "\n",
      "```\n",
      "{'input_ids': tensor([[49406, 320, 6829, 1629, 530, 518, 2583, 49407]]), 'at-\n",
      "```\n",
      "\n",
      "```\n",
      "tention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "```\n",
      "\n",
      "To  see  what  those  IDs  represent,  we  can  convert  them  to  tokens  using  the  aptly named convert_ids_to_tokens function:\n",
      "\n",
      "```\n",
      "# Convert our input back to tokens clip_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
      "```\n",
      "\n",
      "This gives us the following output:\n",
      "\n",
      "```\n",
      "['<|startoftext|>', 'a</w>', 'puppy</w>', 'playing</w>', 'in</w>', 'the</w>', 'snow</w>', '<|endoftext|>']\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "As we often have seen before, the text is split up into tokens. Additionally, we now also see that the start and end of the text is indicated to separate it from a potential image embedding. You might also notice that the [CLS] token is missing. In CLIP , the [CLS] token is actually used to represent the image embedding.\n",
      "\n",
      "Now that we have preprocessed our caption, we can create the embedding:\n",
      "\n",
      "```\n",
      "# Create a text embedding text_embedding = model.get_text_features(**inputs) text_embedding.shape\n",
      "```\n",
      "\n",
      "This results in an embedding that has 512 values for this single string:\n",
      "\n",
      "```\n",
      "torch.Size([1, 512])\n",
      "```\n",
      "\n",
      "Before we can create our image embedding, like the text embedding, we will need to preprocess it as the model expects the input image to have certain characteristics, like its size and shape.\n",
      "\n",
      "To do so, we can use the processor that we created before:\n",
      "\n",
      "```\n",
      "# Preprocess image processed_image = clip_processor( text= None , images=image, return_tensors=\"pt\" )[\"pixel_values\"] processed_image.shape\n",
      "```\n",
      "\n",
      "The original image was 512 × 512 pixels. Notice that the preprocessing of this image reduced its size to 224 × 224 pixels as that is its expected size:\n",
      "\n",
      "```\n",
      "torch.Size([1, 3, 224, 224])\n",
      "```\n",
      "\n",
      "```\n",
      "Let's visualize the results of this preprocessing as shown in Figure 9-13: import torch img = img.permute(*torch.arange(img.ndim - 1, -1, -1))\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "import numpy as np import matplotlib.pyplot as plt # Prepare image for visualization img = processed_image.squeeze(0) img = np.einsum(\"ijk->jik\", img) # Visualize preprocessed image plt.imshow(img) plt.axis(\"off\")\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "To convert this preprocessed image into embeddings, we can call the model as we did before and explore what shape it returns:\n",
      "\n",
      "```\n",
      "# Create the image embedding image_embedding = model.get_image_features(processed_image) image_embedding.shape\n",
      "```\n",
      "\n",
      "This gives us the following shape:\n",
      "\n",
      "\n",
      "\n",
      "Notice that the shape of the resulting image embedding is the same as that of the text embedding. This is important as it allows us to compare their embeddings and see if they are similar.\n",
      "\n",
      "We can use these embeddings to calculate how similar they are. To do so, we normalize  the  embeddings  first  before  calculating  the  dot  product  to  give  us  a  similarity score:\n",
      "\n",
      "```\n",
      "# Normalize the embeddings\n",
      "```\n",
      "\n",
      "```\n",
      "text_embedding /= text_embedding.norm(dim=-1, keepdim= True ) image_embedding /= image_embedding.norm(dim=-1, keepdim= True ) # Calculate their similarity text_embedding = text_embedding.detach().cpu().numpy() image_embedding = image_embedding.detach().cpu().numpy() score = np.dot(text_embedding, image_embedding.T) score This gives us the following score: array([[0.33149648]], dtype=float32)\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "We get a similarity score of 0.33, which is difficult to interpret considering we don't know  what  the  model  considers  a  low  versus  a  high  similarity  score.  Instead,  let's extend the example with more images and captions as illustrated in Figure 9-14.\n",
      "\n",
      "\n",
      "\n",
      "It  seems  that  a  score  of  0.33  is  indeed  high  considering  the  similarities  with  other images are quite a bit lower.\n",
      "\n",
      "```\n",
      "Using sentence-transformers to Load CLIP sentence-transformers implements  a  few  CLIP-based  models  that  make  it  much easier to create embeddings. It only takes a few lines of code: from sentence_transformers import SentenceTransformer, util # Load SBERT-compatible CLIP model model = SentenceTransformer(\"clip-ViT-B-32\") # Encode the images image_embeddings = model.encode(images) # Encode the captions text_embeddings = model.encode(captions) #Compute cosine similarities sim_matrix = util.cos_sim( image_embeddings, text_embeddings )\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "Making Text Generation Models Multimodal\n",
      "\n",
      "Traditionally,  text  generation  models  have  been,  as  you  might  expect,  models  that interpret textual representations. Models like Llama 2 and ChatGPT excel at reasoning about textual information and responding with natural language.\n",
      "\n",
      "They  are,  however,  limited  to  the  modality  they  were  trained  in,  namely  text.  As we have seen before with multimodal embedding models, the addition of vision can enhance the capabilities of a model.\n",
      "\n",
      "In the case of text generation models, we would like it to reason about certain input images. For example, we could give it an image of a pizza and ask it what ingredients it contains. You could show it a picture of the Eiffel Tower and ask when it was built or where it is located. This conversational ability is further illustrated in Figure 9-15.\n",
      "\n",
      "\n",
      "\n",
      "To bridge the gap between these two domains, attempts have been made to introduce a  form  of  multimodality  to  existing  models.  One  such  method  is  called BLIP-2: Bootstrapping  Language-Image Pre-training for  Unified  Vision-Language  Understanding and Generation 2 . BLIP-2 is an easy-to-use and modular technique that allows for introducing vision capabilities to existing language models.\n",
      "\n",
      "BLIP-2: Bridging the Modality Gap\n",
      "\n",
      "Creating a multimodal language model from scratch requires significant computing power and data. We would have to use billions of images, text, and image-text pairs to create such a model. As you can imagine, this is not easily feasible!\n",
      "\n",
      "|\n",
      "\n",
      "Instead of building the architecture from scratch, BLIP-2 bridges the vision-language gap by building a bridge, named the Querying Transformer (Q-Former), that connects a pretrained image encoder and a pretrained LLM. 5\n",
      "\n",
      "By  leveraging  pretrained  models,  BLIP-2  only  needs  to  train  the  bridge  without needing  to  train  the  image  encoder  and  LLM  from  scratch.  It  makes  great  use  of the  technology  and  models  that  are  already  out  there!  This  bridge  is  illustrated  in Figure 9-16.\n",
      "\n",
      "\n",
      "\n",
      "To connect the two pretrained models, the Q-Former mimics their architectures. It has two modules that share their attention layers:\n",
      "\n",
      " · An Image Transformer to interact with the frozen Vision Transformer for feature extraction\n",
      "\n",
      " · A Text Transformer that can interact with the LLM\n",
      "\n",
      "The  Q-Former  is  trained  in  two  stages,  one  for  each  modality,  as  illustrated  in Figure 9-17.\n",
      "\n",
      "In  step  1,  image-document  pairs  are  used  to  train  the  Q-Former  to  represent  both images and text. These pairs are generally captions of images, as we have seen before when training CLIP.\n",
      "\n",
      "The images are fed to the frozen ViT to extract vision embeddings. These embeddings are used as the input of Q-Former's ViT. The captions are used as the input of Q-Former's Text Transformer.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "With these inputs, the Q-Former is then trained on three tasks:\n",
      "\n",
      "Image-text contrastive learning\n",
      "\n",
      "This  task  attempts  to  align  pairs  of  image  and  text  embeddings  such  that  they maximize their mutual information.\n",
      "\n",
      "Image-text matching\n",
      "\n",
      "A  classification  task  to  predict  whether  an  image  and  text  pair  is  positive (matched) or negative (unmatched).\n",
      "\n",
      "Image-grounded text generation\n",
      "\n",
      "Trains the model to generate text based on information extracted from the input image.\n",
      "\n",
      "These three objectives are jointly optimized to improve the visual representations that are extracted from the frozen ViT. In a way, we are trying to inject textual information into the embeddings of the frozen ViT so that we can use them in the LLM. This first step of BLIP-2 is illustrated in Figure 9-18.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "In step 2, the learnable embeddings derived from step 1 now contain visual information  in  the  same  dimensional  space  as  the  corresponding  textual  information.  The learnable embeddings are then passed to the LLM. In a way, these embeddings serve as soft visual prompts that condition the LLM on the visual representations that were extracted by the Q-Former.\n",
      "\n",
      "There is also  a  fully  connected  linear  layer  in  between  them  to  make  sure  that  the learnable embeddings have the same shape as the LLM expects. This second step of converting vision to language is represented in Figure 9-19.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "When we put these steps together, they make it possible for the Q-Former to learn visual and textual representations in the same dimensional space, which can be used as a soft prompt to the LLM. As a result, the LLM will be given information about the  image  in  a  similar  manner  to  the  context  you  would  provide  an  LLM  when prompting. The full in-depth process is illustrated in Figure 9-20.\n",
      "\n",
      "\n",
      "\n",
      "Since BLIP-2, many other visual LLMs have been released that have similar processes, like  LLaVA,  a  framework  for  making  textual  LLMs  multimodal   or  Idefics  2,  an 6 efficient  visual  LLM  based  on  the  Mistral  7B  LLM.   Both  visual  LLMs,  although 7 having  different  architectures,  connect  pretrained  CLIP-like  visual  encoders  with textual  LLMs.  The  goal  of  these  architectures  is  to  project  visual  features  from  the input images to language embeddings such that they can be used as the input for an LLM. Similar to the Q-Former, they attempt to bridge the gap between images and text.\n",
      "\n",
      "|\n",
      "\n",
      "Preprocessing Multimodal Inputs\n",
      "\n",
      "Now that we know how BLIP-2 is created, there are a number of interesting use cases for such a model, not limited to captioning images, answering visual questions, and even performing prompting.\n",
      "\n",
      "Before we go through some use cases, let's first load the model and explore how you can use it:\n",
      "\n",
      "```\n",
      "from transformers import AutoProcessor, Blip2ForConditionalGeneration import torch # Load processor and main model blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\") model = Blip2ForConditionalGeneration.from_pretrained( \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16 ) # Send the model to GPU to speed up inference device = \"cuda\" if torch.cuda.is_available() else \"cpu\" model.to(device)\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "Using model.vision_model and model.language_model , we  can see which ViT and generative model are used, respectively, in the BLIP-2 model we loaded.\n",
      "\n",
      "We loaded two components that make up our full pipeline: a processor and a model. The  processor  can  be  compared  to  the  tokenizer  of  language  models.  It  converts unstructured input, such as images and text, to representations that the model generally expects.\n",
      "\n",
      "Preprocessing images\n",
      "\n",
      "Let's  start  by  exploring  what  the  processor  does  to  images.  We  start  by  loading  the picture of a very wide image for illustration purposes:\n",
      "\n",
      "```\n",
      "# Load image of a supercar car_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-LargeLanguage-Models/main/chapter09/images/car.png\" image = Image.open(urlopen(car_path)).convert(\"RGB\") image\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "The image has 520 × 492 pixels, which is generally an unusual format. So let's see what our processor does to it:\n",
      "\n",
      "# Preprocess the image\n",
      "\n",
      "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16) inputs[\"pixel_values\"].shape\n",
      "\n",
      "This gives us the following shape:\n",
      "\n",
      "\n",
      "\n",
      "The result is a 224 × 224-sized image. Quite a bit smaller than we initially had! This also means that all the original different shapes of the image will be processed into squares. So be careful inputting very wide or tall images as they might get distorted.\n",
      "\n",
      "Preprocessing text\n",
      "\n",
      "Let's continue this exploration of the processor with text instead. First, we can access the tokenizer used to tokenize the input text:\n",
      "\n",
      "blip_processor.tokenizer\n",
      "\n",
      "This gives us the following output:\n",
      "\n",
      "GPT2TokenizerFast(name_or_path='Salesforce/blip2-opt-2.7b', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, pad-ding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;/ s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;/s&gt;', 'pad_token': '&lt;pad&gt;'}, clean_up_tokenization_spaces=True), added_tokens_decoder={ 1: AddedToken(\"&lt;pad&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), 2: AddedToken(\"&lt;/s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True), }\n",
      "\n",
      "|\n",
      "\n",
      "The  BLIP-2  model  here  uses  a GPT2Tokenizer .  As  we  explored  in  Chapter  2,  how tokenizers deal with input text can differ greatly.\n",
      "\n",
      "To explore how GPT2Tokenizer works, we can try it out with a small sentence. We start by converting the sentence to token IDs before converting them back to tokens:\n",
      "\n",
      "```\n",
      "# Preprocess the text text = \"Her vocalization was remarkably melodic\" token_ids = blip_processor(image, text=text, return_tensors=\"pt\") token_ids = token_ids.to(device, torch.float16)[\"input_ids\"] # Convert input ids back to tokens tokens = blip_processor.tokenizer.convert_ids_to_tokens(token_ids) tokens This gives us the following tokens:\n",
      "```\n",
      "\n",
      "```\n",
      "['</s>', 'Her', 'Ġvocal', 'ization', 'Ġwas', 'Ġremarkably', 'Ġmel', 'odic']\n",
      "```\n",
      "\n",
      "When we inspect the tokens, you might notice a strange symbol at the beginning of some tokens, namely, the Ġ symbol. This is actually supposed to be a space. However, an internal function takes characters in certain code points and moves them up by 256 to make them printable. As a result, the space (code point 32) becomes Ġ (code point 288).\n",
      "\n",
      "```\n",
      "We will convert them to underscores for illustrative purposes: # Replace the space token with an underscore tokens = [token.replace(\"Ġ\", \"_\") for token in tokens] tokens This gives us a nicer output:\n",
      "```\n",
      "\n",
      "```\n",
      "['</s>', 'Her', '_vocal', 'ization', '_was', '_remarkably', '_mel', 'odic']\n",
      "```\n",
      "\n",
      "The output shows that the underscore indicates the beginning of a word. That way, words that are made up of multiple tokens can be recognized.\n",
      "\n",
      "Use Case 1: Image Captioning\n",
      "\n",
      "The most straightforward usage of a model like BLIP-2 is to create captions of images that  you  have  in  your  data.  Y ou  might  be  a  store  that  wants  to  create  descriptions of  its  clothing  or  perhaps  you  are  a  photographer  that  does  not  have  the  time  to manually label the 1,000+ pictures of a wedding.\n",
      "\n",
      "The  process  of  captioning  an  image  closely  follows  the  processing.  An  image  is converted to pixel values that the model can read. These pixel values are passed to BLIP-2 to be converted into soft visual prompts that the LLM can use to decide on a proper caption.\n",
      "\n",
      "|\n",
      "\n",
      "Let's  take  the  image  of  a  supercar  and  use  the  processor  to  derive  pixels  in  the expected shape:\n",
      "\n",
      "```\n",
      "# Load an AI-generated image of a supercar image = Image.open(urlopen(car_path)).convert(\"RGB\") # Convert an image into inputs and preprocess it inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16) image\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "The next step is converting the image into token IDs using the BLIP-2 model. After doing so, we can convert the IDs into text (the generated caption):\n",
      "\n",
      "```\n",
      "# Generate image ids to be passed to the decoder (LLM) generated_ids = model.generate(**inputs, max_new_tokens=20) # Generate text from the image ids generated_text = blip_processor.batch_decode( generated_ids, skip_special_tokens= True ) generated_text = generated_text.strip() generated_text\n",
      "```\n",
      "\n",
      "generated_text contains the caption:\n",
      "\n",
      "```\n",
      "an orange supercar driving on the road at sunset\n",
      "```\n",
      "\n",
      "This seems like a perfect description for this image!\n",
      "\n",
      "Image captioning is a great way to get to learn this model before stepping into more complex use cases. Try it out with a few images yourself and see where it performs well and where it performs poorly. Domain-specific images, like pictures of specific\n",
      "\n",
      "|\n",
      "\n",
      "cartoon  characters  or  imaginary  creations,  may  fail  as  the  model  was  trained  on largely public data.\n",
      "\n",
      "Let's end this use case with a fun example, namely an image from the Rorschach test, which is illustrated in Figure 9-21. It is part of an old psychological experiment that tests the individual's perception of inkblots.  What someone sees in such an inkblot 8 supposedly tells you something about a person's personality characteristics. It is quite a subjective test but that just makes it more fun!\n",
      "\n",
      "\n",
      "\n",
      "Let's take the image illustrated in Figure 9-21 and use that as our input:\n",
      "\n",
      "```\n",
      "# Load Rorschach image url = \"https://upload.wikimedia.org/wikipedia/commons/7/70/Ror schach_blot_01.jpg\" image = Image.open(urlopen(url)).convert(\"RGB\") # Generate caption inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16) generated_ids = model.generate(**inputs, max_new_tokens=20) generated_text = blip_processor.batch_decode( generated_ids, skip_special_tokens= True )\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "generated_text = generated_text.strip() generated_text\n",
      "```\n",
      "\n",
      "As before, when we inspect the generated_text variable, we can take a look at the caption:\n",
      "\n",
      "```\n",
      "a black and white ink drawing of a bat\n",
      "```\n",
      "\n",
      "I can definitely see how the model would caption this image using such a description. Since this is a Rorschach test, what do you think it says about the model?\n",
      "\n",
      "Use Case 2: Multimodal Chat-Based Prompting\n",
      "\n",
      "Although captioning is an important task, we can extend its use case even further. In  the  previous  example,  we  showed  going  from  one  modality,  vision  (image),  to another, text (caption).\n",
      "\n",
      "Instead of following this linear structure, we can try to present both modalities simultaneously by performing what is called visual question answering. In this particular use case, we give the model an image along with a question about that specific image for it to answer. The model needs to process both the image as well as the question at once.\n",
      "\n",
      "To demonstrate, let's  start  with  the  picture  of  a  car  and  ask  BLIP-2  to  describe  the image. To do so, we first need to preprocess the image as we did a few times before:\n",
      "\n",
      "```\n",
      "# Load an AI-generated image of a supercar image = Image.open(urlopen(car_path)).convert(\"RGB\")\n",
      "```\n",
      "\n",
      "To perform our visual question answering we need to give BLIP-2 more than just the image, namely the prompt. Without it, the model would generate a caption as it did before. We will ask the model to describe the image we just processed:\n",
      "\n",
      "```\n",
      "# Visual question answering prompt = \"Question: Write down what you see in this picture. Answer:\" # Process both the image and the prompt inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16) # Generate text generated_ids = model.generate(**inputs, max_new_tokens=30) generated_text = blip_processor.batch_decode( generated_ids, skip_special_tokens= True ) generated_text = generated_text.strip() generated_text\n",
      "```\n",
      "\n",
      "This gives us the following output:\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "A sports car driving on the road at sunset\n",
      "```\n",
      "\n",
      "It  correctly describes the image. However, this is a rather simple example since our question  is  essentially  asking  the  model  to  create  a  caption.  Instead,  we  can  ask follow-up questions in a chat-based manner.\n",
      "\n",
      "To do so, we can give the model our previous conversation, including its answer to our question. We then ask it a follow-up question:\n",
      "\n",
      "```\n",
      "# Chat-like prompting prompt = \"Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset. Question: What would it cost me to drive that car? Answer:\" # Generate output inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16) generated_ids = model.generate(**inputs, max_new_tokens=30) generated_text = blip_processor.batch_decode( generated_ids, skip_special_tokens= True ) generated_text = generated_text.strip() generated_text\n",
      "```\n",
      "\n",
      "This gives us the following answer:\n",
      "\n",
      "```\n",
      "$1,000,000\n",
      "```\n",
      "\n",
      "$1,000,000 is highly specific! This shows more chat-like behavior from BLIP-2, which allows for some interesting conversations.\n",
      "\n",
      "Finally,  we can make this process a bit smoother by creating an interactive chatbot using ipywidgets , an extension for Jupyter notebooks that allows us to make interactive buttons, input text, etc:\n",
      "\n",
      "```\n",
      "from IPython.display import HTML, display import ipywidgets as widgets def text_eventhandler(*args): question = args[\"new\"] if question: args[\"owner\"].value = \"\" # Create prompt if not memory: prompt = \" Question: \" + question + \" Answer:\" else : template = \"Question: {} Answer: {} .\" prompt = \" \".join( [ template.format(memory[i], memory[i])\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "USER: Write down what you see in this picture. for i in range(len(memory)) ] ) + \" Question: \" + question + \" Answer:\" BLIP-2: Because they're fast. # Generate text inputs = blip_processor(image, text=prompt, return_tensors=\"pt\") inputs = inputs.to(device, torch.float16) generated_ids = model.generate(**inputs, max_new_tokens=100) generated_text = blip_processor.batch_decode( generated_ids, skip_special_tokens= True ) generated_text = generated_text.strip().split(\"Question\") # Update memory memory.append((question, generated_text)) # Assign to output output.append_display_data(HTML(\"<b>USER:</b> \" + question)) output.append_display_data(HTML(\"<b>BLIP-2:</b> \" + generated_text)) output.append_display_data(HTML(\"<br>\")) # Prepare widgets in_text = widgets.Text() in_text.continuous_update = False in_text.observe(text_eventhandler, \"value\") output = widgets.Output() memory = [] # Display chat box display( widgets.VBox( children=[output, in_text], layout=widgets.Layout(display=\"inline-flex\", flex_flow=\"columnreverse\"), ) )\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "It seems that we can continue the conversation and ask a bunch of questions. Using this  chat-based  approach,  we  essentially  created  a  chatbot  that  can  reason  about images!\n",
      "\n",
      "Summary\n",
      "\n",
      "In  this  chapter,  we  explored  various  methods  for  making  LLMs  multimodal  by bridging  the  gap  between  textual  and  visual  representations.  We  started  by  discussing Transformers for vision, which are models that convert images into numerical representations.  This  was  achieved  through  the  use  of  image  encoders  and  patch embeddings, which allow the model to process images at various scales.\n",
      "\n",
      "We then explored the creation of embedding models that can convert both images and text to numerical representations using CLIP . We saw how CLIP uses contrastive learning to align image and text embeddings in a shared space, allowing for tasks like zero-shot  classification,  clustering,  and  search.  The  chapter  also  introduced  OpenCLIP, an open source variant of CLIP that is easy to use for multimodal embedding tasks.\n",
      "\n",
      "Finally,  we  explored  how  text  generation  models  could  be  made  multimodal  and dived  into  the  BLIP-2  model.  The  core  idea  of  these  multimodal  text  generation models involves projecting visual features from input images to text embeddings that can be used by LLMs. We saw how this model could be used for image captioning and multimodal chat-based prompting, where both modalities are combined to generate responses. Overall, this chapter highlighted the power of multimodality in LLMs and demonstrated its applications in various areas such as image captioning, search, and chat-based prompting.\n",
      "\n",
      "In Part III of the book, we will cover training and fine-tuning techniques. In Chapter 10, we will explore how to create and fine-tune a text embedding model, which is  a  core  technology  that  drives  many  language  modeling  applications.  This  next chapter serves as an introduction into both training and fine-tuning language models.\n",
      "\n",
      "|\n",
      "\n",
      "PART III\n",
      "\n",
      "Training and Fine-Tuning Language Models\n",
      "\n",
      "Creating Text Embedding Models\n",
      "\n",
      "Text  embedding  models  lie  at  the  foundation  of  many  powerful  natural  language processing  applications.  They  lay  the  groundwork  for  empowering  already  impressive technologies such as text generation models. We have already used embedding models throughout this book in a number of applications, such as supervised classification, unsupervised classification, semantic search, and even giving memory to text generation models like ChatGPT.\n",
      "\n",
      "It is nearly impossible to overstate the importance of embedding models in the field as they are the driving power behind so many applications. As such, in this chapter, we  will  discuss  a  variety  of  ways  that  we  can  create  and  fine-tune  an  embedding model to increase its representative and semantic power.\n",
      "\n",
      "Let's start by discovering what embedding models are and how they generally work.\n",
      "\n",
      "Embedding Models\n",
      "\n",
      "Embeddings and embedding models have already been discussed in quite a number of  chapters  (Chapters  4,  5,  and  8)  thereby  demonstrating  their  usefulness.  Before going into training such a model, let's recap what we have learned with embedding models.\n",
      "\n",
      "Unstructured  textual  data  by  itself  is  often  quite  hard  to  process.  They  are  not values we can directly process, visualize, and create actionable results from. We first have  to  convert  this  textual  data  to  something  that  we  can  easily  process:  numeric representations.  This  process  is  often  referred  to  as embedding the  input  to  output usable vectors, namely embeddings, as shown in Figure 10-1.\n",
      "\n",
      "\n",
      "\n",
      "This  process  of  embedding  the  input  is  typically  performed  by  an  LLM,  which  we refer to as an embedding model . The main purpose of such a model is to be as accurate as possible in representing the textual data as an embedding.\n",
      "\n",
      "However, what does it mean to be accurate in representation? Typically, we want to capture the semantic nature -the meaning-of documents. If we can capture the core of what the document communicates, we hope to have captured what the document is about. In practice, this means that we expect vectors of documents that are similar to one another to be similar, whereas the embeddings of documents that each discuss something  entirely  different  should  be  dissimilar.  We've  seen  this  idea  of  semantic similarity several times already in this book, and it is visualized in Figure 10-2. This figure  is  a  simplified  example.  While  two-dimensional  visualization  helps  illustrate the  proximity  and  similarity  of  embeddings,  these  embeddings  typically  reside  in high-dimensional spaces.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "An  embedding  model,  however,  can  be  trained  for  a  number  of  purposes.  For example, when we are building a sentiment classifier, we are more interested in the sentiment of texts than their semantic similarity. As illustrated in Figure 10-3, we can fine-tune the model such that documents are closer in n-dimensional space based on their sentiment rather than their semantic nature.\n",
      "\n",
      "Either way, an embedding model aims to learn what makes certain documents similar to one another and we can guide this process. By presenting the model with enough examples of semantically similar documents, we can steer toward semantics whereas using examples of sentiment would steer it in that direction.\n",
      "\n",
      "\n",
      "\n",
      "There are many ways in which we can train, fine-tune, and guide embedding models,  but  one  of  the  strongest  and  most  widely  used  techniques  is  called  contrastive learning.\n",
      "\n",
      "What Is Contrastive Learning?\n",
      "\n",
      "One  major  technique  for  both  training  and  fine-tuning  text  embedding  models  is called contrastive  learning . Contrastive  learning  is  a  technique  that  aims  to  train an  embedding model such that similar documents are closer in vector space while dissimilar  documents  are  further  apart.  If  this  sounds  familiar,  it's  because  it's  very\n",
      "\n",
      "|\n",
      "\n",
      "similar to the word2vec method from Chapter 2. We have seen this notion previously in Figures 10-2 and 10-3.\n",
      "\n",
      "The underlying idea of contrastive learning is that the best way to learn and model similarity/dissimilarity between documents is by feeding a model examples of similar and dissimilar pairs.  In  order  to  accurately  capture  the  semantic  nature  of  a  document, it  often  needs  to  be  contrasted  with  another  document  for  a  model  to  learn what makes it different or similar. This contrasting procedure is quite powerful and relates to the context in which documents are written. This high-level procedure is demonstrated in Figure 10-4.\n",
      "\n",
      "\n",
      "\n",
      "Another way to look at contrastive learning is through the nature of explanations. A nice example of this is an anecdotal story of a reporter asking a robber 'Why did you rob a bank?' to which he answers, 'Because that is where the money is.'  Although 1 a  factually  correct  answer,  the  intent  of  the  question  was  not  why  he  robs  banks specifically but why he robs at all. This is called contrastive explanation and refers to understanding a particular case, 'Why P?' in contrast to alternatives, 'Why P and not Q?' 2 In the example, the question could be interpreted in a number of ways and may be best modeled by providing an alternative: 'Why did you rob a bank (P) instead of obeying the law (Q)?'\n",
      "\n",
      "The importance of alternatives to the understanding of a question also applies to how an embedding learns through contrastive learning. By showing a model similar and dissimilar pairs of documents, it starts to learn what makes something similar/dissimilar and more importantly, why.\n",
      "\n",
      "For  example,  you  could  teach  a  model  to  understand  what  a  dog  is  by  letting  it find features such as 'tail, ' 'nose, ' 'four legs, ' etc. This learning process can be quite difficult since features are often not well-defined and can be interpreted in a number of  ways. A being with a 'tail, ' 'nose, ' and 'four legs' can also be a cat. To help the\n",
      "\n",
      "|\n",
      "\n",
      "model steer  toward  what  we  are  interested  in,  we  essentially  ask  it,  'Why  is  this  a dog and not a cat?' By providing the contrast between two concepts, it starts to learn the features that define the concept but also the features that are not related. We get more information when we frame a question as a contrast. We further illustrate this concept of contrastive explanation in Figure 10-5.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "One of the earliest and most popular examples of contrastive learning in NLP is actually word2vec, as we discussed in Chapters 1 and 2. The model learns word representations by training on individual words in a sentence. A word close to a target word in a sentence will  be  constructed  as  a  positive  pair  whereas  randomly  sampled words constitute dissimilar pairs. In other words, positive examples of neighboring words are contrasted with randomly selected words that  are  not  neighbors.  Although  not  widely  known,  it  is  one  of the  first  major  breakthroughs  in  NLP  that  leverages  contrastive learning with neural networks.\n",
      "\n",
      "There  are  many  ways  we  can  apply  contrastive  learning  to  create  text  embedding  models  but  the  most  well-known  technique  and  framework  is sentencetransformers .\n",
      "\n",
      "SBERT\n",
      "\n",
      "Although  there  are  many  forms  of  contrastive  learning,  one  framework  that  has popularized  the  technique  within  the  natural  language  processing  community  is sentence-transformers . 3 Its approach  fixes  a  major  problem  with  the  original BERT implementation for creating sentence embeddings, namely its computational\n",
      "\n",
      "|\n",
      "\n",
      "overhead. Before sentence-transformers , sentence embeddings often used an architectural structure called cross-encoders with BERT.\n",
      "\n",
      "A  cross-encoder  allows  two  sentences  to  be  passed  to  the  Transformer  network simultaneously to predict the extent to which the two sentences are similar. It does so by adding a classification head to the original architecture that can output a similarity score.  However,  the  number  of  computations  rises  quickly  when  you  want  to  find the  highest  pair  in  a  collection  of  10,000  sentences.  That  would  require  n·(n-1)/2 =  49,995,000  inference  computations  and  therefore  generates  significant  overhead. Moreover,  a  cross-encoder  generally  does  not  generate  embeddings,  as  shown  in Figure 10-6. Instead, it outputs a similarity score between the input sentences.\n",
      "\n",
      "A  solution  to  this  overhead  is  to  generate  embeddings  from  a  BERT  model  by averaging its output layer or using the [CLS] token. This, however, has shown to be worse than simply averaging word vectors, like GloVe. 4\n",
      "\n",
      "\n",
      "\n",
      "Instead, the authors of sentence-transformers approached the problem differently and searched for a method that is fast and creates embeddings that can be compared semantically. The result is an elegant alternative to the original cross-encoder architecture. Unlike a cross-encoder, in sentence-transformers the classification head is dropped, and instead mean pooling is used on the final output layer to generate an embedding. This pooling layer averages the word embeddings and gives back a fixed dimensional output vector. This ensures a fixed-size embedding.\n",
      "\n",
      "|\n",
      "\n",
      "The training for sentence-transformers uses a Siamese architecture. In this architecture, as visualized in Figure 10-7, we have two identical BERT models that share the same weights and neural architecture. These models are fed the sentences from which embeddings are generated through the pooling of token embeddings. Then, models are optimized through the similarity of the sentence embeddings. Since the weights are identical for both BERT models, we can use a single model and feed it the sentences one after the other.\n",
      "\n",
      "\n",
      "\n",
      "The optimization process of these pairs of sentences is done through loss functions, which  can  have  a  major  impact  on  the  model's  performance.  During  training,  the embeddings for each sentence are concatenated together with the difference between the  embeddings.  Then,  this  resulting  embedding  is  optimized  through  a  softmax classifier.\n",
      "\n",
      "The resulting architecture is also referred to as a bi-encoder or SBERT for sentenceBERT. Although a bi-encoder is quite fast and creates accurate sentence representations, cross-encoders generally achieve better performance than a bi-encoder but do not generate embeddings.\n",
      "\n",
      "The bi-encoder, like a cross-encoder, leverages contrastive learning; by optimizing the (dis)similarity between pairs of sentences, the model will eventually learn the things that make the sentences what they are.\n",
      "\n",
      "|\n",
      "\n",
      "To perform contrastive learning, we need two things. First, we need data that constitutes similar/dissimilar pairs. Second, we will need to define how the model defines and optimizes similarity.\n",
      "\n",
      "Creating an Embedding Model\n",
      "\n",
      "There  are  many  methods  through  which  an  embedding  model  can  be  created but  generally,  we  look  toward  contrastive  learning.  This  is  an  important  aspect of  many  embedding  models  as  the  process  allows  it  to  efficiently  learn  semantic representations.\n",
      "\n",
      "However,  this  is  not  a  free  process.  We  will  need  to  understand  how  to  generate contrastive examples, how to train the model, and how to properly evaluate it.\n",
      "\n",
      "Generating Contrastive Examples\n",
      "\n",
      "When pretraining your embedding model, you will often see data being used from natural  language  inference  (NLI)  datasets.  NLI  refers  to  the  task  of  investigating whether,  for  a  given  premise,  it  entails  the  hypothesis  (entailment),  contradicts  it (contradiction), or neither (neutral).\n",
      "\n",
      "For  example,  when  the  premise  is  'He  is  in  the  cinema  watching Coco '  and  the hypothesis 'He is watching Frozen at home, ' then these statements are contradictions. In  contrast,  when  the  premise  is  'He  is  in  the  cinema  watching Coco '  and  the hypothesis 'In the movie theater he is watching the Disney movie Coco , '  then these statements are considered entailment. This principle is illustrated in Figure 10-8.\n",
      "\n",
      "\n",
      "\n",
      "If you look closely at entailment and contradiction, then they describe the extent to which  two  inputs  are  similar  to  one  another.  As  such,  we  can  use  NLI  datasets  to generate negative examples (contradictions) and positive examples (entailments) for contrastive learning.\n",
      "\n",
      "The  data  that  we  are  going  to  be  using  throughout  creating  and  fine-tuning embedding models is derived from the General Language Understanding Evaluation\n",
      "\n",
      "|\n",
      "\n",
      "benchmark (GLUE). This GLUE benchmark consists of nine language understanding tasks to evaluate and analyze model performance.\n",
      "\n",
      "One of these tasks is the Multi-Genre Natural Language Inference (MNLI) corpus, which is a collection of 392,702 sentence pairs annotated with entailment (contradiction,  neutral,  entailment).  We  will  be  using  a  subset  of  the  data,  50,000  annotated sentence pairs, to create a minimal example that does not need to be trained for hours on  end.  Do  note,  though,  that  the  smaller  the  dataset,  the  more  unstable  training or  fine-tuning  an  embedding  model  is.  If  possible,  larger  datasets  are  preferred assuming it is still quality data:\n",
      "\n",
      "```\n",
      "from datasets import load_dataset\n",
      "```\n",
      "\n",
      "```\n",
      "# Load MNLI dataset from GLUE # 0 = entailment, 1 = neutral, 2 = contradiction train_dataset = load_dataset( \"glue\", \"mnli\", split=\"train\" ).select(range(50_000)) train_dataset = train_dataset.remove_columns(\"idx\") Next, we take a look at an example: dataset {'premise': 'One of our number will carry out your instructions minutely.', 'hypothesis': 'A member of my team will execute your orders with immense precision.', 'label': 0}\n",
      "```\n",
      "\n",
      "This shows an example of an entailment between the premise and the hypothesis as they are positively related and have near identical meanings.\n",
      "\n",
      "Train Model\n",
      "\n",
      "Now that  we  have  our  dataset  with  training  examples,  we  will  need  to  create  our embedding model. We typically choose an existing sentence-transformers model and fine-tune that model, but in this example, we are going to train an embedding from scratch.\n",
      "\n",
      "This means that we will have to define two things. First, a pretrained Transformer model that serves as embedding individual words. We will use the BERT base model (uncased)  as  it  is  a  great  introduction  model.  However,  many  others  exist  that also  have  been  evaluated  using sentence-transformers .  Most  notably, microsoft/ mpnet-base often gives good results when used as a word embedding model.\n",
      "\n",
      "```\n",
      "from sentence_transformers import SentenceTransformer # Use a base model embedding_model = SentenceTransformer('bert-base-uncased')\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "By  default,  all  layers  of  an  LLM  in sentence-transformers are trainable.  Although  it  is  possible  to  freeze  certain  layers,  it  is generally  not  advised  since  the  performance  is  often  better  when unfreezing all layers.\n",
      "\n",
      "Next, we will need to define a loss function over which we will optimize the model. As mentioned at the beginning of this section, one of the first instances of sentencetransformers uses softmax loss. For illustrative purposes, we are going to be using that for now, but we will go into more performant losses later on:\n",
      "\n",
      "from sentence_transformers import losses\n",
      "\n",
      "```\n",
      "# Define the loss function. In softmax loss, we will also need to explicitly set the number of labels. train_loss = losses.SoftmaxLoss( model=embedding_model, sentence_embedding_dimension=embedding_model.get_sentence_embedding_dimen sion(), num_labels=3 )\n",
      "```\n",
      "\n",
      "Before  we  train  our  model,  we  define  an  evaluator  to  evaluate  the  model's  performance during training, which also determines the best model to save.\n",
      "\n",
      "We  can  perform  evaluation  of  the  performance  of  our  model  using  the  Semantic Textual  Similarity  Benchmark  (STSB).  It  is  a  collection  of  human-labeled  sentence pairs, with similarity scores between 1 and 5.\n",
      "\n",
      "We use this dataset to explore how well our model scores on this semantic similarity task. Moreover, we process the STSB data to make sure all values are between 0 and 1:\n",
      "\n",
      "```\n",
      "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator # Create an embedding similarity evaluator for STSB val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\") evaluator = EmbeddingSimilarityEvaluator( sentences1=val_sts[\"sentence1\"], sentences2=val_sts[\"sentence2\"], scores=[score/5 for score in val_sts[\"label\"]], main_similarity=\"cosine\", )\n",
      "```\n",
      "\n",
      "Now  that  we  have  our  evaluator,  we  create SentenceTransformerTrainingArgu ments ,  similar  to  training  with  Hugging  Face  Transformers  (as  we  will  explore  in the next chapter):\n",
      "\n",
      "from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments\n",
      "\n",
      "```\n",
      "# Define the training arguments\n",
      "```\n",
      "\n",
      "```\n",
      "args = SentenceTransformerTrainingArguments( output_dir=\"base_embedding_model\", num_train_epochs=1, per_device_train_batch_size=32, per_device_eval_batch_size=32, warmup_steps=100, fp16= True , eval_steps=100, logging_steps=100, )\n",
      "```\n",
      "\n",
      "Of note are the following arguments:\n",
      "\n",
      "num_train_epochs\n",
      "\n",
      "The  number  of  training  rounds.  We  keep  this  at  1  for  faster  training  but  it  is generally advised to increase this value.\n",
      "\n",
      "per_device_train_batch_size\n",
      "\n",
      "The number of samples to process simultaneously on each device (e.g., GPU or CPU) during evaluation. Higher values generally means faster training.\n",
      "\n",
      "per_device_eval_batch_size\n",
      "\n",
      "The number of samples to process simultaneously on each device (e.g., GPU or CPU) during evaluation. Higher values generally means faster evaluation.\n",
      "\n",
      "warmup_steps\n",
      "\n",
      "The  number  of  steps  during  which  the  learning  rate  will  be  linearly  increased from zero to the initial learning rate defined for the training process. Note that we did not specify a custom learning rate for this training process.\n",
      "\n",
      "fp16\n",
      "\n",
      "By enabling this parameter we allow for mixed precision training, where computations are performed using 16-bit floating-point numbers (FP16) instead of the default 32-bit (FP32). This reduces memory usage and potentially increases the training speed.\n",
      "\n",
      "Now that we have defined our data, embedding model, loss, and evaluator, we can start training our model. We can do that using SentenceTransformerTrainer :\n",
      "\n",
      "```\n",
      "from sentence_transformers.trainer import SentenceTransformerTrainer # Train embedding model trainer = SentenceTransformerTrainer(\n",
      "```\n",
      "\n",
      "```\n",
      "model=embedding_model, args=args, train_dataset=train_dataset, loss=train_loss, evaluator=evaluator ) trainer.train()\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "After  training  our  model,  we  can  use  the  evaluator  to  get  the  performance  on  this single task:\n",
      "\n",
      "```\n",
      "# Evaluate our trained model evaluator(embedding_model)\n",
      "```\n",
      "\n",
      "```\n",
      "{'pearson_cosine': 0.5982288436666162, 'spearman_cosine': 0.6026682018489217, 'pearson_manhattan': 0.6100690915500567, 'spearman_manhattan': 0.617732600131989, 'pearson_euclidean': 0.6079280934202278, 'spearman_euclidean': 0.6158926913905742, 'pearson_dot': 0.38364924527804595, 'spearman_dot': 0.37008497926991796, 'pearson_max': 0.6100690915500567, 'spearman_max': 0.617732600131989}\n",
      "```\n",
      "\n",
      "We  get  several  different  distance  measures.  The  one  we  are  interested  in  most  is 'pearson_cosine' ,  which  is  the  cosine  similarity  between  centered  vectors.  It  is  a value between 0 and 1 where a higher value indicates higher degrees of similarity. We get a value of 0.59, which we consider a baseline throughout this chapter.\n",
      "\n",
      "\n",
      "\n",
      "Larger batch sizes tend to work better with multiple negative rankings  (MNR)  loss  as  a  larger  batch  makes  the  task  more  difficult. The reason for this is that the model needs to find the best matching sentence from a larger set of potential pairs of sentences. You can adapt the code to try out different batch sizes and get a feeling of its effects.\n",
      "\n",
      "In-Depth Evaluation\n",
      "\n",
      "A good embedding model is more than just a good score on the STSB benchmark! As we observed earlier, the GLUE benchmark has a number of tasks for which we can evaluate  our  embedding  model.  However,  there  exist  many  more  benchmarks  that allow  for  the  evaluation  of  embedding  models.  To  unify  this  evaluation  procedure, the Massive Text Embedding Benchmark (MTEB) was developed.  The MTEB spans 5 8 embedding tasks that cover 58 datasets and 112 languages.\n",
      "\n",
      "To publicly compare state-of-the-art embedding models, a leaderboard was created with the scores of each embedding model across all tasks:\n",
      "\n",
      "```\n",
      "from mteb import MTEB # Choose evaluation task evaluation = MTEB(tasks=[\"Banking77Classification\"])\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "# Calculate results\n",
      "\n",
      "results = evaluation.run(model)\n",
      "\n",
      "```\n",
      "{'Banking77Classification': {'mteb_version': '1.1.2', 'dataset_revision': '0fd18e25b25c072e09e0d92ab615fda904d66300', 'mteb_dataset_name': 'Banking77Classification', 'test': {'accuracy': 0.4926298701298701, 'f1': 0.49083335791288685, 'accuracy_stderr': 0.010217785746224237, 'f1_stderr': 0.010265814957074591, 'main_score': 0.4926298701298701, 'evaluation_time': 31.83}}}\n",
      "```\n",
      "\n",
      "This  gives  us  several  evaluation  metrics  for  this  specific  task  that  we  can  use  to explore its performance.\n",
      "\n",
      "The great thing about this evaluation benchmark is not only the diversity of the tasks and languages but that even the evaluation time is saved. Although many embedding models exist, we typically want those that are both accurate and have low latency. The tasks for which embedding models are used, like semantic search, often benefit from and require fast inference.\n",
      "\n",
      "Since testing your model on the entire MTEB can take a couple of hours depending on your GPU, we will use the STSB benchmark throughout this chapter instead for illustration purposes.\n",
      "\n",
      "\n",
      "\n",
      "Whenever you are done training and evaluating your model, it is important to restart the  notebook. This will clear your VRAM up for the next training examples throughout this chapter. By restarting the notebook, we can be sure that all VRAM is cleared.\n",
      "\n",
      "Loss Functions\n",
      "\n",
      "We trained our model using softmax loss to illustrate how one of the first sentencetransformers models was trained. However, not only is there a large variety of loss functions to choose from, but softmax loss is generally not advised as there are more performant losses.\n",
      "\n",
      "Instead  of  going  through  every  single  loss  function  out  there,  there  are  two  loss functions that are typically used and seem to perform generally well, namely:\n",
      "\n",
      " · Cosine similarity\n",
      "\n",
      " · Multiple negatives ranking (MNR) loss\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "There  are  many  more  loss  functions  to  choose  from  than  just those  discussed  here.  For  example,  a  loss  like  MarginMSE  works great for training or fine-tuning a cross-encoder. There are a number  of  interesting  loss  functions  implemented  in  the sentencetransformers framework.\n",
      "\n",
      "Cosine similarity\n",
      "\n",
      "The cosine similarity loss is an intuitive and easy-to-use loss that works across many different  use  cases  and  datasets.  It  is  typically  used  in  semantic  textual  similarity tasks. In these tasks, a similarity score is assigned to the pairs of texts over which we optimize the model.\n",
      "\n",
      "Instead of having strictly positive or negative pairs of sentences, we assume pairs of sentences that are similar or dissimilar to a certain degree. Typically, this value lies between 0 and 1 to indicate dissimilarity and similarity, respectively (Figure 10-9).\n",
      "\n",
      "\n",
      "\n",
      "Cosine similarity loss is straightforward-it calculates the cosine similarity between the two embeddings of the two texts and compares that to the labeled similarity score. The model will learn to recognize the degree of similarity between sentences.\n",
      "\n",
      "Cosine similarity loss intuitively works best using data where you have pairs of sentences and labels that indicate their similarity between 0 and 1. To use this loss with our NLI dataset, we need to convert the entailment (0), neutral (1), and contradiction (2)  labels  to  values  between  0  and  1.  The  entailment  represents  a  high  similarity\n",
      "\n",
      "|\n",
      "\n",
      "between the sentences, so we give it a similarity score of 1. In contrast, since both neutral  and  contradiction  represent  dissimilarity,  we  give  these  labels  a  similarity score of 0:\n",
      "\n",
      "```\n",
      "from datasets import Dataset, load_dataset # Load MNLI dataset from GLUE # 0 = entailment, 1 = neutral, 2 = contradiction train_dataset = load_dataset( \"glue\", \"mnli\", split=\"train\" ).select(range(50_000)) train_dataset = train_dataset.remove_columns(\"idx\") # (neutral/contradiction)=0 and (entailment)=1 mapping = {2: 0, 1: 0, 0:1} train_dataset = Dataset.from_dict({ \"sentence1\": train_dataset[\"premise\"], \"sentence2\": train_dataset[\"hypothesis\"], \"label\": [float(mapping[label]) for label in train_dataset[\"label\"]] }) As before, we create our evaluator: from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator # Create an embedding similarity evaluator for stsb val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\") evaluator = EmbeddingSimilarityEvaluator( sentences1=val_sts[\"sentence1\"], sentences2=val_sts[\"sentence2\"], scores=[score/5 for score in val_sts[\"label\"]], main_similarity=\"cosine\" ) Then, we follow the same steps as before but select a different loss instead: from sentence_transformers import losses, SentenceTransformer\n",
      "```\n",
      "\n",
      "```\n",
      "from sentence_transformers.trainer import SentenceTransformerTrainer from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments # Define model embedding_model = SentenceTransformer(\"bert-base-uncased\") # Loss function train_loss = losses.CosineSimilarityLoss(model=embedding_model) # Define the training arguments args = SentenceTransformerTrainingArguments( output_dir=\"cosineloss_embedding_model\", num_train_epochs=1, per_device_train_batch_size=32, per_device_eval_batch_size=32,\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "Evaluating the model after training gives us the following score:\n",
      "```\n",
      "\n",
      "```\n",
      "warmup_steps=100, fp16= True , eval_steps=100, logging_steps=100, ) # Train model trainer = SentenceTransformerTrainer( model=embedding_model, args=args, train_dataset=train_dataset, loss=train_loss, evaluator=evaluator ) trainer.train() # Evaluate our trained model\n",
      "```\n",
      "\n",
      "```\n",
      "evaluator(embedding_model)\n",
      "```\n",
      "\n",
      "```\n",
      "{'pearson_cosine': 0.7222322163831805, 'spearman_cosine': 0.7250508271229599, 'pearson_manhattan': 0.7338163436711481, 'spearman_manhattan': 0.7323479193408869, 'pearson_euclidean': 0.7332716434966307, 'spearman_euclidean': 0.7316999722750905, 'pearson_dot': 0.660366792336156, 'spearman_dot': 0.6624167554844425, 'pearson_max': 0.7338163436711481, 'spearman_max': 0.7323479193408869}\n",
      "```\n",
      "\n",
      "A Pearson cosine score of 0.72 is a big improvement compared to the softmax loss example, which scored 0.59. This demonstrates the impact the loss function can have on performance.\n",
      "\n",
      "Make sure to restart your notebook so we can explore a more common and performant loss, namely multiple negatives ranking loss.\n",
      "\n",
      "Multiple negatives ranking loss\n",
      "\n",
      "Multiple  negatives  ranking  (MNR)  loss,   often  referred  to  as  InfoNCE   or  NTXen6 7 tLoss,   is  a  loss  that  uses  either  positive  pairs  of  sentences  or  triplets  that  contain 8 a  pair  of  positive  sentences  and  an  additional  unrelated  sentence.  This  unrelated\n",
      "\n",
      "|\n",
      "\n",
      "sentence  is  called  a  negative  and  represents  the  dissimilarity  between  the  positive sentences.\n",
      "\n",
      "For example, you might have pairs of question/answer, image/image caption, paper title/paper abstract, etc. The great thing about these pairs is that we can be confident they are hard positive pairs. In MNR loss (Figure 10-10), negative pairs are constructed by mixing a positive pair with another positive pair. In the example of a paper title and abstract, you would generate a negative pair by combining the title of a paper with a completely different abstract. These negatives are called in-batch negatives and can also be used to generate the triplets.\n",
      "\n",
      "\n",
      "\n",
      "After having generated these positive and negative pairs, we calculate their embeddings  and  apply  cosine  similarity.  These  similarity  scores  are  then  used  to  answer the  question,  are  these  pairs  negative  or  positive?  In  other  words,  it  is  treated  as  a classification task and we can use cross-entropy loss to optimize the model.\n",
      "\n",
      "To make these triplets we start with an anchor sentence (i.e., labeled as the 'premise'),  which is used to compare other sentences. Then, using the MNLI dataset, we only  select  sentence  pairs  that  are  positive  (i.e.,  labeled  as  'entailment').  To  add negative sentences, we randomly sample sentences as the 'hypothesis. '\n",
      "\n",
      "```\n",
      "import random from tqdm import tqdm from datasets import Dataset, load_dataset # # Load MNLI dataset from GLUE mnli = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(50_000)) mnli = mnli.remove_columns(\"idx\")\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "mnli = mnli.filter( lambda x: True if x[\"label\"] == 0 else False ) # Prepare data and add a soft negative train_dataset = {\"anchor\": [], \"positive\": [], \"negative\": []} soft_negatives = mnli[\"hypothesis\"] random.shuffle(soft_negatives) for row, soft_negative in tqdm(zip(mnli, soft_negatives)): train_dataset[\"anchor\"].append(row[\"premise\"]) train_dataset[\"positive\"].append(row[\"hypothesis\"]) train_dataset[\"negative\"].append(soft_negative) train_dataset = Dataset.from_dict(train_dataset) Since  we  only  selected  sentences  labeled  with  'entailment, '  the  number  of  rows reduced quite a a bit from 50,000 to 16,875 rows. Let's define the evaluator: from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator # Create an embedding similarity evaluator for stsb val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\") evaluator = EmbeddingSimilarityEvaluator( sentences1=val_sts[\"sentence1\"], sentences2=val_sts[\"sentence2\"], scores=[score/5 for score in val_sts[\"label\"]], main_similarity=\"cosine\" ) We then train as before but with MNR loss instead:\n",
      "```\n",
      "\n",
      "```\n",
      "from sentence_transformers import losses, SentenceTransformer from sentence_transformers.trainer import SentenceTransformerTrainer from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments # Define model embedding_model = SentenceTransformer('bert-base-uncased') # Loss function train_loss = losses.MultipleNegativesRankingLoss(model=embedding_model) # Define the training arguments args = SentenceTransformerTrainingArguments( output_dir=\"mnrloss_embedding_model\", num_train_epochs=1, per_device_train_batch_size=32, per_device_eval_batch_size=32, warmup_steps=100, fp16= True , eval_steps=100, logging_steps=100, ) # Train model\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "trainer = SentenceTransformerTrainer( model=embedding_model, args=args, train_dataset=train_dataset, loss=train_loss, evaluator=evaluator ) trainer.train()\n",
      "```\n",
      "\n",
      "Let's see how this dataset and loss function compare to our previous examples:\n",
      "\n",
      "```\n",
      "# Evaluate our trained model evaluator(embedding_model)\n",
      "```\n",
      "\n",
      "```\n",
      "{'pearson_cosine': 0.8093892326162132, 'spearman_cosine': 0.8121064796503025, 'pearson_manhattan': 0.8215001523827565, 'spearman_manhattan': 0.8172161486524246, 'pearson_euclidean': 0.8210391407846718, 'spearman_euclidean': 0.8166537141010816, 'pearson_dot': 0.7473360302629125, 'spearman_dot': 0.7345184137194012, 'pearson_max': 0.8215001523827565, 'spearman_max': 0.8172161486524246}\n",
      "```\n",
      "\n",
      "Compared to our previously trained model with softmax loss (0.72), our model with MNR loss (0.80) seems to be much more accurate!\n",
      "\n",
      "\n",
      "\n",
      "Larger  batch  sizes  tend  to  be  better  with  MNR  loss  as  a  larger batch makes the task more difficult. The reason for this is that the model needs to find the best matching sentence from a larger set of  potential  pairs  of  sentences.  Y ou  can  adapt  the  code  to  try  out different batch sizes and get a feeling of the effects.\n",
      "\n",
      "There is a downside to how we used this loss function. Since negatives are sampled from  other  question/answer  pairs,  these  in-batch  or  'easy'  negatives  that  we  used could potentially be completely unrelated to the question. As a result, the embedding model's  task  of  then  finding  the  right  answer  to  a  question  becomes  quite  easy. Instead, we would like to have negatives that are very related to the question but not the  right  answer.  These  negatives  are  called hard  negatives .  Since  this  would  make the  task  more  difficult  for  the  embedding  model  as  it  has  to  learn  more  nuanced representations, the embedding model's performance generally improves quite a bit.\n",
      "\n",
      "A good example of a hard negative is the following. Let's assume we have the following question: 'How many people live in Amsterdam?' A related answer to this question would be: ' Almost a million people live in Amsterdam. ' To generate a good hard negative, we ideally want the answer to contain something about Amsterdam and the number of people living in this city. For example: 'More than a million people live in Utrecht, which is more than Amsterdam.' This answer relates to the question but is\n",
      "\n",
      "|\n",
      "\n",
      "not the actual answer, so this would be a good hard negative. Figure 10-11 illustrates the differences between easy and hard negatives.\n",
      "\n",
      "\n",
      "\n",
      "Gathering negatives can roughly be divided into the following three processes:\n",
      "\n",
      "Easy negatives\n",
      "\n",
      "Through randomly sampling documents as we did before.\n",
      "\n",
      "Semi-hard negatives\n",
      "\n",
      "Using  a  pretrained  embedding  model,  we  can  apply  cosine  similarity  on  all sentence  embeddings to  find  those  that  are  highly  related.  Generally,  this  does not lead to hard negatives since this method merely finds similar sentences, not question/answer pairs.\n",
      "\n",
      "Hard negatives\n",
      "\n",
      "These often need to be either manually labeled (for instance, by generating semihard  negatives)  or  you  can  use  a  generative  model  to  either  judge  or  generate sentence pairs.\n",
      "\n",
      "Make  sure  to restart your  notebook  so  we  can  explore  the  different  methods  of fine-tuning embedding models.\n",
      "\n",
      "|\n",
      "\n",
      "Fine-Tuning an Embedding Model\n",
      "\n",
      "In the previous section, we went through the basics of training an embedding model from scratch and saw how we could leverage loss functions to further optimize its performance. This approach, although quite powerful, requires creating an embedding model from scratch. This process can be quite costly and time-consuming.\n",
      "\n",
      "Instead, the sentence-transformers framework allows nearly all embedding models to  be  used  as  a  base  for  fine-tuning.  We  can  choose  an  embedding model that was already  trained  on  a  large  amount  of  data  and  fine-tune  it  for  our  specific  data  or purpose.\n",
      "\n",
      "There are several ways to fine-tune your model, depending on the data availability and domain. We will go through two such methods and demonstrate the strength of leveraging pretrained embedding models.\n",
      "\n",
      "Supervised\n",
      "\n",
      "The  most  straightforward  way  to  fine-tune  an  embedding  model  is  to  repeat  the process of training our model as we did before but replace the 'bert-base-uncased' with  a  pretrained sentence-transformers model.  There  are  many  to  choose  from but generally, all-MiniLM-L6-v2 performs well across many use cases and due to its small size is quite fast.\n",
      "\n",
      "We  use  the  same  data  as  we  used  to  train  our  model  in  the  MNR  loss  example but instead use a pretrained embedding model to fine-tune. As always, let's start by loading the data and creating the evaluator:\n",
      "\n",
      "```\n",
      "from datasets import load_dataset from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator # Load MNLI dataset from GLUE # 0 = entailment, 1 = neutral, 2 = contradiction train_dataset = load_dataset( \"glue\", \"mnli\", split=\"train\" ).select(range(50_000)) train_dataset = train_dataset.remove_columns(\"idx\") # Create an embedding similarity evaluator for stsb val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\") evaluator = EmbeddingSimilarityEvaluator( sentences1=val_sts[\"sentence1\"], sentences2=val_sts[\"sentence2\"], scores=[score/5 for score in val_sts[\"label\"]], main_similarity=\"cosine\" )\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "The training steps are similar to our previous examples but instead of using 'bertbase-uncased', we can use a pretrained embedding model instead:\n",
      "\n",
      "```\n",
      "from sentence_transformers import losses, SentenceTransformer from sentence_transformers.trainer import SentenceTransformerTrainer from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments # Define model embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') # Loss function train_loss = losses.MultipleNegativesRankingLoss(model=embedding_model) # Define the training arguments args = SentenceTransformerTrainingArguments( output_dir=\"finetuned_embedding_model\", num_train_epochs=1, per_device_train_batch_size=32, per_device_eval_batch_size=32, warmup_steps=100, fp16= True , eval_steps=100, logging_steps=100, ) # Train model trainer = SentenceTransformerTrainer( model=embedding_model, args=args, train_dataset=train_dataset, loss=train_loss, evaluator=evaluator ) trainer.train() Evaluating this model gives us the following score: # Evaluate our trained model evaluator(embedding_model)\n",
      "```\n",
      "\n",
      "```\n",
      "{'pearson_cosine': 0.8509553350510896, 'spearman_cosine': 0.8484676559567688, 'pearson_manhattan': 0.8503896832470704, 'spearman_manhattan': 0.8475760325664419, 'pearson_euclidean': 0.8513115442079158, 'spearman_euclidean': 0.8484676559567688, 'pearson_dot': 0.8489553386816947, 'spearman_dot': 0.8484676559567688, 'pearson_max': 0.8513115442079158, 'spearman_max': 0.8484676559567688}\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "Although a score of 0.85 is the highest we have seen thus far, the pretrained model that we used for fine-tuning was already trained on the full MNLI dataset, whereas we only used 50,000 examples. It might seem redundant but this example demonstrates how to fine-tune a pretrained embedding model on your own data.\n",
      "\n",
      "\n",
      "\n",
      "Instead  of  using  a  pretrained  BERT  model  like 'bert-baseuncased' or  a  possible  out-of-domain  model  like 'all-mpnetbase-v2' , you can also perform masked language modeling on the pretrained BERT model to first adapt it to your domain. Then, you can use this fine-tuned BERT model as the base for training your embedding model. This is a form of domain adaptation. In the next chapter, we will apply masked language modeling on a pretrained model.\n",
      "\n",
      "Note  that  the  main  difficulty  of  training  or  fine-tuning  your  model  is  finding  the right  data.  With  these  models,  we  not  only  want  to  have  very  large  datasets,  but the  data  in  itself  needs  to  be  of  high  quality.  Developing  positive  pairs  is  generally straightforward but adding hard negative pairs significantly increases the difficulty of creating quality data.\n",
      "\n",
      "As always, restart your notebook to free up VRAM for the following examples.\n",
      "\n",
      "Augmented SBERT\n",
      "\n",
      "A disadvantage of training or fine-tuning these embedding models is that they often require substantial training data. Many of these models are trained with more than a billion sentence pairs. Extracting such a high number of sentence pairs for your use case is generally not possible as in many cases, there are only a couple of thousand labeled data points available.\n",
      "\n",
      "Fortunately,  there  is  a  way  to  augment  your  data  such  that  an  embedding  model can be fine-tuned when there is only a little labeled data available. This procedure is referred to as Augmented SBERT . 9\n",
      "\n",
      "In  this  procedure,  we  aim  to  augment  the  small  amount  of  labeled  data  such  that they  can  be  used  for  regular  training.  It  makes  use  of  the  slow  and  more  accurate cross-encoder architecture (BERT) to augment and label a larger set of input pairs. These newly labeled pairs are then used for fine-tuning a bi-encoder (SBERT).\n",
      "\n",
      "As shown in Figure 10-12, Augmented SBERT involves the following steps:\n",
      "\n",
      "|\n",
      "\n",
      " 1. Fine-tune  a  cross-encoder  (BERT)  using  a  small,  annotated  dataset  (gold dataset).\n",
      "\n",
      " 2. Create new sentence pairs.\n",
      "\n",
      " 3. Label new sentence pairs with the fine-tuned cross-encoder (silver dataset).\n",
      "\n",
      " 4. Train a bi-encoder (SBERT) on the extended dataset (gold + silver dataset).\n",
      "\n",
      "Here, a gold dataset is a small but fully annotated dataset that holds the ground truth. A silver dataset is also fully annotated but is not necessarily the ground truth as it was generated through predictions of the cross-encoder.\n",
      "\n",
      "\n",
      "\n",
      "Before  we  get  into  the  preceding  steps,  let's  first  prepare  the  data.  Instead  of  our original  50,000  documents,  we  take  a  subset  of  10,000  documents  to  simulate  a setting where we have limited annotated data. As we did in our example with cosine similarity loss, give entailment a score of 1 whereas neutral and contradiction get a score of 0:\n",
      "\n",
      "```\n",
      "import pandas as pd from tqdm import tqdm from datasets import load_dataset, Dataset from sentence_transformers import InputExample from sentence_transformers.datasets import NoDuplicatesDataLoader # Prepare a small set of 10000 documents for the cross-encoder dataset = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(10_000)) mapping = {2: 0, 1: 0, 0:1} # Data loader gold_examples = [ InputExample(texts=[row[\"premise\"], row[\"hypothesis\"]], label=map ping[row[\"label\"]]) for row in tqdm(dataset) ]\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "gold_dataloader = NoDuplicatesDataLoader(gold_examples, batch_size=32) # Pandas DataFrame for easier data handling gold = pd.DataFrame( { \"sentence1\": dataset[\"premise\"], \"sentence2\": dataset[\"hypothesis\"], \"label\": [mapping[label] for label in dataset[\"label\"]] } )\n",
      "```\n",
      "\n",
      "This is the gold dataset since it is labeled and represents our ground truth.\n",
      "\n",
      "Using this gold dataset, we train our cross-encoder (step 1):\n",
      "\n",
      "```\n",
      "from sentence_transformers.cross_encoder import CrossEncoder # Train a cross-encoder on the gold dataset cross_encoder = CrossEncoder(\"bert-base-uncased\", num_labels=2) cross_encoder.fit( train_dataloader=gold_dataloader, epochs=1, show_progress_bar= True , warmup_steps=100, use_amp= False )\n",
      "```\n",
      "\n",
      "After training our cross-encoder, we use the remaining 400,000 sentence pairs (from our original dataset of 50,000 sentence pairs) as our silver dataset (step 2):\n",
      "\n",
      "```\n",
      "# Prepare the silver dataset by predicting labels with the cross-encoder silver = load_dataset( \"glue\", \"mnli\", split=\"train\" ).select(range(10_000, 50_000)) pairs = list(zip(silver[\"premise\"], silver[\"hypothesis\"]))\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "If  you  do  not  have  any  additional  unlabeled  sentence  pairs,  you can  randomly  sample  them  from  your  original  gold  dataset.  To illustrate, you can create a new sentence pair by taking the premise from one row and the hypothesis from another. This allows you to easily generate 10 times as many sentence pairs that can be labeled with the cross-encoder.\n",
      "\n",
      "This strategy, however, likely generates significantly more dissimilar than similar pairs. Instead, we can use a pretrained embedding model  to  embed  all  candidate  sentence  pairs  and  retrieve  the top-k  sentences  for  each  input  sentence  using  semantic  search. This rough reranking process allows us to focus on sentence pairs that are likely to be more similar. Although the sentences are still chosen based on an approximation since the pretrained embedding model was not trained on our data, it is much better than random sampling.\n",
      "\n",
      "|\n",
      "\n",
      "Note that we assume that these sentence pairs are unlabeled in this example. We will use our fine-tuned cross-encoder to label these sentence pairs (step 3):\n",
      "\n",
      "```\n",
      "import numpy as np # Label the sentence pairs using our fine-tuned cross-encoder output = cross_encoder.predict( pairs, apply_softmax= True , show_progress_bar= True ) silver = pd.DataFrame( { \"sentence1\": silver[\"premise\"], \"sentence2\": silver[\"hypothesis\"], \"label\": np.argmax(output, axis=1) } )\n",
      "```\n",
      "\n",
      "Now that we have a silver and gold dataset, we simply combine them and train our embedding model as we did before:\n",
      "\n",
      "```\n",
      "# Combine gold + silver data = data.drop_duplicates(subset=[\"sentence1\", \"sentence2\"], keep=\"first\")\n",
      "```\n",
      "\n",
      "```\n",
      "data = pd.concat([gold, silver], ignore_index= True , axis=0) train_dataset = Dataset.from_pandas(data, preserve_index= False ) As always, we need to define our evaluator: from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator # Create an embedding similarity evaluator for stsb val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\") evaluator = EmbeddingSimilarityEvaluator( sentences1=val_sts[\"sentence1\"], sentences2=val_sts[\"sentence2\"], scores=[score/5 for score in val_sts[\"label\"]], main_similarity=\"cosine\" )\n",
      "```\n",
      "\n",
      "We train the model the same as before except now we use the augmented dataset:\n",
      "\n",
      "```\n",
      "from sentence_transformers import losses, SentenceTransformer from sentence_transformers.trainer import SentenceTransformerTrainer from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments # Define model embedding_model = SentenceTransformer(\"bert-base-uncased\") # Loss function train_loss = losses.CosineSimilarityLoss(model=embedding_model) # Define the training arguments args = SentenceTransformerTrainingArguments(\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "output_dir=\"augmented_embedding_model\", num_train_epochs=1, per_device_train_batch_size=32, per_device_eval_batch_size=32, warmup_steps=100, fp16= True , eval_steps=100, logging_steps=100, ) # Train model trainer = SentenceTransformerTrainer( model=embedding_model, args=args, train_dataset=train_dataset, loss=train_loss, evaluator=evaluator ) trainer.train()\n",
      "```\n",
      "\n",
      "```\n",
      "Finally, we evaluate the model: evaluator(embedding_model)\n",
      "```\n",
      "\n",
      "```\n",
      "{'pearson_cosine': 0.7101597020018693, 'spearman_cosine': 0.7210536464320728, 'pearson_manhattan': 0.7296749443525249, 'spearman_manhattan': 0.7284184255293913, 'pearson_euclidean': 0.7293097297208753, 'spearman_euclidean': 0.7282830906742256, 'pearson_dot': 0.6746605824703588, 'spearman_dot': 0.6754486790570754, 'pearson_max': 0.7296749443525249, 'spearman_max': 0.7284184255293913}\n",
      "```\n",
      "\n",
      "The original cosine similarity loss example had a score of 0.72 with the full dataset. Using only 20% of that data, we managed to get a score of 0.71!\n",
      "\n",
      "This method allows us to increase the size of datasets that you already have available without the need to manually label hundreds of thousands of sentence pairs. You can test the quality of your silver data by also training your embedding model only on the gold dataset. The difference in performance indicates how much your silver dataset potentially adds to the quality of the model.\n",
      "\n",
      "You can restart your notebook a final time for the last example, namely unsupervised learning.\n",
      "\n",
      "|\n",
      "\n",
      "Unsupervised Learning\n",
      "\n",
      "To  create  an  embedding  model,  we  typically  need  labeled  data.  However,  not  all real-world  datasets  come  with  a  nice  set  of  labels  that  we  can  use.  We  instead look for techniques to train the model without any predetermined labels-unsupervised learning. Many approaches exist, like Simple Contrastive Learning of Sentence Embeddings (SimCSE), 10 Contrastive  Tension  (CT), 11 Transformer-based Sequential Denoising Auto-Encoder (TSDAE), 12 and Generative Pseudo-Labeling (GPL). 13\n",
      "\n",
      "In  this  section,  we  will  focus  on  TSDAE,  as  it  has  shown  great  performance  on unsupervised tasks as well as domain adaptation.\n",
      "\n",
      "Transformer-Based Sequential Denoising Auto-Encoder\n",
      "\n",
      "TSDAE is a very elegant approach to creating an embedding model with unsupervised learning. The method assumes that we have no labeled data at all and does not require us to artificially create labels.\n",
      "\n",
      "The underlying idea of TSDAE is that we add noise to the input sentence by removing a certain percentage of words from it. This 'damaged' sentence is put through an encoder, with a pooling layer on top of it, to map it to a sentence embedding. From this sentence embedding, a decoder tries to reconstruct the original sentence from the 'damaged' sentence but without the artificial noise. The main concept here is that the  more  accurate  the  sentence  embedding  is,  the  more  accurate  the  reconstructed sentence will be.\n",
      "\n",
      "This method is very similar to masked language modeling, where we try to reconstruct  and  learn  certain  masked  words.  Here,  instead  of  reconstructing  masked words, we try to reconstruct the entire sentence.\n",
      "\n",
      "After training, we can use the encoder to generate embeddings from text since the decoder is only used for judging whether the embeddings can accurately reconstruct the original sentence (Figure 10-13).\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Since we only need a bunch of sentences without any labels, training this model is straightforward. We start by downloading an external tokenizer, which is used for the denoising procedure:\n",
      "\n",
      "```\n",
      "# Download additional tokenizer import nltk nltk.download(\"punkt\")\n",
      "```\n",
      "\n",
      "Then, we create flat sentences from our data and remove any labels that we have to mimic an unsupervised setting:\n",
      "\n",
      "```\n",
      "from tqdm import tqdm from datasets import Dataset, load_dataset from sentence_transformers.datasets import DenoisingAutoEncoderDataset # Create a flat list of sentences mnli = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(25_000)) flat_sentences = mnli[\"premise\"] + mnli[\"hypothesis\"] # Add noise to our input data damaged_data = DenoisingAutoEncoderDataset(list(set(flat_sentences))) # Create dataset train_dataset = {\"damaged_sentence\": [], \"original_sentence\": []}\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "for data in tqdm(damaged_data): train_dataset[\"damaged_sentence\"].append(data.texts) train_dataset[\"original_sentence\"].append(data.texts) train_dataset = Dataset.from_dict(train_dataset)\n",
      "```\n",
      "\n",
      "This creates a dataset of 50,000 sentences. When we inspect the data, notice that the first sentence is the damaged sentence and the second sentence the original:\n",
      "\n",
      "```\n",
      "train_dataset\n",
      "```\n",
      "\n",
      "```\n",
      "{'damaged_sentence': 'Grim jaws are.', 'original_sentence': 'Grim faces and hardened jaws are not people-friendly.'}\n",
      "```\n",
      "\n",
      "The first sentence shows the 'noisy' data whereas the second shows the original input sentence. After creating our data, we define our evaluator as before:\n",
      "\n",
      "```\n",
      "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator # Create an embedding similarity evaluator for stsb val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\") evaluator = EmbeddingSimilarityEvaluator( sentences1=val_sts[\"sentence1\"], sentences2=val_sts[\"sentence2\"], scores=[score/5 for score in val_sts[\"label\"]], main_similarity=\"cosine\" )\n",
      "```\n",
      "\n",
      "Next, we run the training as before but with the [CLS] token as the pooling strategy instead of the mean pooling of the token embeddings. In the TSDAE paper, this was shown to be more effective since mean pooling loses the position information, which is not the case when using the [CLS] token:\n",
      "\n",
      "```\n",
      "from sentence_transformers import models, SentenceTransformer # Create your embedding model word_embedding_model = models.Transformer(\"bert-base-uncased\") pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimen sion(), \"cls\") embedding_model = SentenceTransformer(modules=[word_embedding_model, pool ing_model])\n",
      "```\n",
      "\n",
      "Using our sentence pairs, we will need a loss function that attempts to reconstruct the original  sentence  using  the  noise  sentence,  namely DenoisingAutoEncoderLoss .  By doing so, it will learn how to accurately represent the data. It is similar to masking but without knowing where the actual masks are.\n",
      "\n",
      "Moreover, we tie the parameters of both models. Instead of having separate weights for the encoder's embedding layer and the decoder's output layer, they share the same weights. This means that any updates to the weights in one layer will be reflected in the other layer as well:\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "from sentence_transformers import losses # Use the denoising auto-encoder loss train_loss = losses.DenoisingAutoEncoderLoss( embedding_model, tie_encoder_decoder= True ) train_loss.decoder = train_loss.decoder.to(\"cuda\")\n",
      "```\n",
      "\n",
      "Finally, training our model works the same as we have seen several times before but we lower the batch size as memory increases with this loss function:\n",
      "\n",
      "```\n",
      "from sentence_transformers.trainer import SentenceTransformerTrainer from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments # Define the training arguments args = SentenceTransformerTrainingArguments( output_dir=\"tsdae_embedding_model\", num_train_epochs=1, per_device_train_batch_size=16, per_device_eval_batch_size=16, warmup_steps=100, fp16= True , eval_steps=100, logging_steps=100, ) # Train model trainer = SentenceTransformerTrainer( model=embedding_model, args=args, train_dataset=train_dataset, loss=train_loss, evaluator=evaluator ) trainer.train()\n",
      "```\n",
      "\n",
      "After  training,  we  evaluate  our  model  to  explore  how  well  such  an  unsupervised technique performs:\n",
      "\n",
      "```\n",
      "# Evaluate our trained model evaluator(embedding_model)\n",
      "```\n",
      "\n",
      "```\n",
      "{'pearson_cosine': 0.6991809700971775, 'spearman_cosine': 0.713693213167873, 'pearson_manhattan': 0.7152343356643568, 'spearman_manhattan': 0.7201441944880915, 'pearson_euclidean': 0.7151142243297436, 'spearman_euclidean': 0.7202291660769805, 'pearson_dot': 0.5198066451871277, 'spearman_dot': 0.5104025515225046, 'pearson_max': 0.7152343356643568, 'spearman_max': 0.7202291660769805}\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "After fitting our model, we got a score of 0.70, which is quite impressive considering we did all this training with unlabeled data.\n",
      "\n",
      "Using TSDAE for Domain Adaptation\n",
      "\n",
      "When you have very little or no labeled data available, you typically use unsupervised learning  to  create  your  text  embedding  model.  However,  unsupervised  techniques are  generally  outperformed  by  supervised  techniques  and  have  difficulty  learning domain-specific concepts.\n",
      "\n",
      "This is where domain adaptation comes in. Its goal is to update existing embedding models to a specific textual domain that contains different subjects from the source domain. Figure 10-14 demonstrates how domains can differ in content. The target domain, or out-domain, generally contains words and subjects that were not found in the source domain or in-domain.\n",
      "\n",
      "\n",
      "\n",
      "One method for domain adaptation is called adaptive pretraining .  Y ou  start  by  pretraining your domain-specific corpus using an unsupervised technique, such as the previously  discussed  TSDAE  or  masked  language  modeling.  Then,  as  illustrated  in Figure  10-15,  you  fine-tune  that  model  using  a  training  dataset  that  can  be  either outside or in your target domain. Although data from the target domain is preferred, out-domain data also works since we started with unsupervised training on the target domain.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Using everything you have learned in this chapter, you should be able to reproduce this  pipeline!  First,  you  can  start  with  TSDAE  to  train  an  embedding  model  on your target domain and then fine-tune it using either general supervised training or Augmented SBERT.\n",
      "\n",
      "Summary\n",
      "\n",
      "In  this  chapter,  we  looked  at  creating  and  fine-tuning  embedding  models  through various tasks. We discussed the concept of embeddings and their role in representing textual data in a numerical format. We then explored the foundational technique of many embedding models, namely contrastive learning, which learns primarily from (dis)similar pairs of documents.\n",
      "\n",
      "Using  a  popular  embedding  framework, sentence-transformers ,  we  then  created embedding  models  using  a  pretrained  BERT  model  while  exploring  different  loss functions, such as cosine similarity loss and MNR loss. We discussed how the collection  of  (dis)similar  pairs  or  triples  of  documents  is  vital  to  the  performance  of  the resulting model.\n",
      "\n",
      "In  the  sections  that  followed,  we  explored  techniques  for  fine-tuning  embedding models. Both supervised and unsupervised techniques were discussed such as Augmented SBERT and TSDAE for domain adaptation. Compared to creating an embedding model, fine-tuning generally needs less data and is a great way to adapt existing embedding models to your domain.\n",
      "\n",
      "In the next chapter, methods for fine-tuning representations for classification will be discussed.  Both  BERT  models  and  embedding  models  will  make  an  appearance  as well as a wide range of fine-tuning techniques.\n",
      "\n",
      "|\n",
      "\n",
      "Fine-Tuning Representation Models for Classification\n",
      "\n",
      "In Chapter 4, we used pretrained models to classify our text. We kept the pretrained models  as  they  were  without  any  modifications  to  them.  This  might  make  you wonder, what happens if we were to fine-tune them?\n",
      "\n",
      "If  we have sufficient data, fine-tuning tends to lead to some of the best-performing models possible. In this chapter, we will go through several methods and applications for fine-tuning BERT models. 'Supervised Classification' on page 323 demonstrates the general process of fine-tuning a classification model. Then, in 'Few-Shot Classification' on page 333, we look at SetFit, which is a method for efficiently fine-tuning a high-performing model using a small number of training examples. In 'Continued Pretraining with Masked Language Modeling' on page 340, we will explore how to continue training a pretrained model. Lastly, classification on a token level is explored in 'Named-Entity Recognition' on page 345.\n",
      "\n",
      "We  will  focus  on  nongenerative  tasks,  as  generative  models  will  be  covered  in Chapter 12.\n",
      "\n",
      "Supervised Classification\n",
      "\n",
      "In  Chapter  4,  we  explored  supervised  classification  tasks  by  leveraging  pretrained representation  models  that  were  either  trained  to  predict  sentiment  (task-specific model) or to generate embeddings (embedding model), as shown in Figure 11-1.\n",
      "\n",
      "\n",
      "\n",
      "Both models were kept frozen (nontrainable) to showcase the potential of leveraging pretrained  models  for  classification  tasks.  The  embedding  model  uses  a  separate trainable classification head (classifier) to predict the sentiment of movie reviews.\n",
      "\n",
      "In  this  section,  we  will  take  a  similar  approach  but  allow  both  the  model  and  the classification head to be updated during training. As shown in Figure 11-2, instead of using an embedding model, we will fine-tune a pretrained BERT model to create a  task-specific  model  similar  to  the  one  we  used  in  Chapter  2.  Compared  to  the embedding model approach, we will fine-tune both the representation model and the classification head as a single architecture.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "To  do  so,  instead  of  freezing  the  model,  we  allow  it  to  be  trainable  and  update  its parameters  during  training.  As  illustrated  in  Figure  11-3,  we  will  use  a  pretrained BERT model and add a neural network as a classification head, both of which will be fine-tuned for classification.\n",
      "\n",
      "\n",
      "\n",
      "In practice, this means that the pretrained BERT model and the classification head are  updated jointly.  Instead  of  independent  processes,  they  learn  from  one  another and allow for more accurate representations.\n",
      "\n",
      "Fine-Tuning a Pretrained BERT Model\n",
      "\n",
      "We  will  be  using  the  same  dataset  we  used  in  Chapter  4  to  fine-tune  our  model, namely the Rotten Tomatoes dataset, which contains 5,331 positive and 5,331 negative movie reviews from Rotten Tomatoes:\n",
      "\n",
      "```\n",
      "from datasets import load_dataset # Prepare data and splits\n",
      "```\n",
      "\n",
      "```\n",
      "tomatoes = load_dataset(\"rotten_tomatoes\") train_data, test_data = tomatoes[\"train\"], tomatoes[\"test\"]\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "The first step in our classification task is to select the underlying model we want to use. We use \"bert-base-cased\" ,  which was pretrained on the English Wikipedia as well as a large dataset consisting of unpublished books. 1\n",
      "\n",
      "We define the number of labels that we want to predict beforehand. This is necessary to  create  the  feedforward  neural  network  that  is  applied  on  top  of  our  pretrained model:\n",
      "\n",
      "```\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "```\n",
      "\n",
      "```\n",
      "# Load model and tokenizer model_id = \"bert-base-cased\" model = AutoModelForSequenceClassification.from_pretrained( model_id, num_labels=2 ) tokenizer = AutoTokenizer.from_pretrained(model_id) Next, we will tokenize our data: from transformers import DataCollatorWithPadding # Pad to the longest sequence in the batch data_collator = DataCollatorWithPadding(tokenizer=tokenizer) def preprocess_function(examples): \"\"\"Tokenize input data\"\"\" return tokenizer(examples[\"text\"], truncation= True ) # Tokenize train/test data tokenized_train = train_data.map(preprocess_function, batched= True ) tokenized_test = test_data.map(preprocess_function, batched= True )\n",
      "```\n",
      "\n",
      "Before  creating  the Trainer ,  we  will  want  to  prepare  a  special DataCollator .  A DataCollator is a class that helps us build batches of data but also allows us to apply data augmentation.\n",
      "\n",
      "During this process of tokenization, and as shown in Chapter 9, we will add padding to  the  input  text  to  create  equally  sized  representations.  We  use DataCollatorWith Padding for that.\n",
      "\n",
      "Of course, an example would not be complete without defining some metrics:\n",
      "\n",
      "```\n",
      "import numpy as np from datasets import load_metric def compute_metrics(eval_pred): \"\"\"Calculate F1 score\"\"\" logits, labels = eval_pred\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "predictions = np.argmax(logits, axis=-1) load_f1 = load_metric(\"f1\") f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"] return {\"f1\": f1}\n",
      "```\n",
      "\n",
      "With compute_metrics we can define any number of metrics that we are interested in  and  that  can  be  printed  out  or  logged  during  training.  This  is  especially  helpful during training as it allows for detecting overfitting behavior.\n",
      "\n",
      "Next, we instantiate our Trainer :\n",
      "\n",
      "```\n",
      "from transformers import TrainingArguments, Trainer # Training arguments for parameter tuning training_args = TrainingArguments( \"model\", learning_rate=2e-5, per_device_train_batch_size=16, per_device_eval_batch_size=16, num_train_epochs=1, weight_decay=0.01, save_strategy=\"epoch\", report_to=\"none\" ) # Trainer which executes the training process trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_train, eval_dataset=tokenized_test, tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics, )\n",
      "```\n",
      "\n",
      "The TrainingArguments class defines hyperparameters we want to tune, such as the learning rate and how many epochs (rounds) we want to train. The Trainer is used to execute the training process.\n",
      "\n",
      "Finally, we can train our model and evaluate it:\n",
      "\n",
      "```\n",
      "trainer.evaluate()\n",
      "```\n",
      "\n",
      "```\n",
      "{'eval_loss': 0.3663691282272339, 'eval_f1': 0.8492366412213741, 'eval_runtime': 4.5792, 'eval_samples_per_second': 232.791, 'eval_steps_per_second': 14.631, 'epoch': 1.0}\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "We get an F1 score of 0.85, which is quite a bit higher than the task-specific model we used in Chapter 4, which resulted in an F1 score of 0.80. It shows that fine-tuning a  model yourself can be more advantageous than using a pretrained model. It only costs us a couple of minutes to train.\n",
      "\n",
      "Freezing Layers\n",
      "\n",
      "To further showcase the importance of training the entire network, the next example will demonstrate how you can use Hugging Face Transformers to freeze certain layers of your network.\n",
      "\n",
      "We will  freeze  the  main  BERT  model  and  allow  only  updates  to  pass  through  the classification  head.  This  will  be  a  great  comparison  as  we  will  keep  everything  the same, except for freezing specific layers.\n",
      "\n",
      "To start, let's reinitialize our model so we can start from scratch:\n",
      "\n",
      "```\n",
      "# Load model and tokenizer model = AutoModelForSequenceClassification.from_pretrained( model_id, num_labels=2 ) tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
      "```\n",
      "\n",
      "Our pretrained BERT model contains a lot of layers that we can potentially freeze. Inspecting these layers gives insight into the structure of the network and what we might want to freeze:\n",
      "\n",
      "```\n",
      "# Print layer names for name, param in model.named_parameters(): print(name) bert.embeddings.word_embeddings.weight bert.embeddings.position_embeddings.weight bert.embeddings.token_type_embeddings.weight bert.embeddings.LayerNorm.weight bert.embeddings.LayerNorm.bias bert.encoder.layer.0.attention.self.query.weight bert.encoder.layer.0.attention.self.query.bias ... bert.encoder.layer.11.output.LayerNorm.weight bert.encoder.layer.11.output.LayerNorm.bias bert.pooler.dense.weight bert.pooler.dense.bias classifier.weight classifier.bias\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "There are  12  (0-11)  encoder  blocks  consisting  of  attention  heads,  dense  networks, and  layer  normalization.  We  further  illustrate  this  architecture  in  Figure  11-4  to demonstrate everything that could be potentially frozen. On top of that, we have our classification head.\n",
      "\n",
      "\n",
      "\n",
      "We could choose to only freeze certain layers to speed up computing but still allow the main model to learn from the classification task. Generally, we want frozen layers to be followed by trainable layers.\n",
      "\n",
      "We  are  going  to  freeze  everything  except  for  the  classification  head  as  we  did  in Chapter 2:\n",
      "\n",
      "```\n",
      "for name, param in model.named_parameters(): # Trainable classification head if name.startswith(\"classifier\"): param.requires_grad = True # Freeze everything else else : param.requires_grad = False\n",
      "```\n",
      "\n",
      "As  shown  in  Figure  11-5,  we  have  frozen  everything  except  for  the  feedforward neural network, which is our classification head.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Now that we have successfully frozen everything but the classification head, we can move on to train our model:\n",
      "\n",
      "from transformers import TrainingArguments, Trainer\n",
      "\n",
      "```\n",
      "# Trainer which executes the training process trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_train, eval_dataset=tokenized_test, tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics, ) trainer.train()\n",
      "```\n",
      "\n",
      "You  might  notice  that  training  has  become  much  faster.  That  is  because  we  are only  training  the  classification  head,  which  provides  us  with  a  significant  speedup compared to fine-tuning the entire model:\n",
      "\n",
      "```\n",
      "trainer.evaluate()\n",
      "```\n",
      "\n",
      "```\n",
      "{'eval_loss': 0.6821751594543457, 'eval_f1': 0.6331058020477816, 'eval_runtime': 4.0175, 'eval_samples_per_second': 265.337, 'eval_steps_per_second': 16.677, 'epoch': 1.0}\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "When we evaluate the model, we only get an F1 score of 0.63, which is quite a bit lower compared to our original 0.85 score. Instead of freezing nearly all layers, let's freeze  everything  up  until  encoder  block  10  as  illustrated  in  Figure  11-6,  and  see how it affects performance. A major benefit is that this reduces computation but still allows updates to flow through part of the pretrained model:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "# Load model model_id = \"bert-base-cased\" model = AutoModelForSequenceClassification.from_pretrained( model_id, num_labels=2 ) tokenizer = AutoTokenizer.from_pretrained(model_id) # Encoder block 11 starts at index 165 and # we freeze everything before that block for index, (name, param) in enumerate(model.named_parameters()): if index < 165: param.requires_grad = False # Trainer which executes the training process trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_train, eval_dataset=tokenized_test,\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics, ) trainer.train()\n",
      "```\n",
      "\n",
      "After training, we evaluate the results:\n",
      "\n",
      "```\n",
      "trainer.evaluate() {'eval_loss': 0.40812647342681885, 'eval_f1': 0.8, 'eval_runtime': 3.7125, 'eval_samples_per_second': 287.137, 'eval_steps_per_second': 18.047, 'epoch': 1.0}\n",
      "```\n",
      "\n",
      "We got an F1 score of 0.8, which is much higher than our previous score of 0.63 when freezing all layers. It demonstrates that although we generally want to train as many layers as possible, you can get away with training less if you do not have the necessary computing power.\n",
      "\n",
      "To  further  illustrate  this  effect,  we  tested  the  effect  of  iteratively  freezing  encoder blocks and fine-tuning them as we did thus far. As shown in Figure 11-7, training only  the  first  five  encoder  blocks  (red  vertical  line)  is  enough  to  almost  reach  the performance of training all encoder blocks.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "When  you  are  training  for  multiple  epochs,  the  difference  (in training  time  and  resources)  between  freezing  and  not  freezing often becomes larger. It is therefore advised to play around with a balance that works for you.\n",
      "\n",
      "Few-Shot Classification\n",
      "\n",
      "Few-shot classification is a technique within supervised classification where you have a classifier learn target labels based on only a few labeled examples. This technique is great when you have a classification task but do not have many labeled data points readily available. In other words, this method allows you to label a few high-quality data points per class on which to train the model. This idea of using a few labeled data points for training your model is shown in Figure 11-8.\n",
      "\n",
      "\n",
      "\n",
      "SetFit: Efficient Fine-Tuning with Few Training Examples\n",
      "\n",
      "To perform few-shot text classification, we use an efficient framework called SetFit.  It 2 is built on top of the architecture of sentence-transformers to generate high-quality textual representations that are updated during training. Only a few labeled examples are needed for this framework to be competitive with fine-tuning a BERT-like model on a large, labeled dataset as we explored in the previous example.\n",
      "\n",
      "The underlying algorithm of SetFit consists of three steps:\n",
      "\n",
      "1. Sampling training data\n",
      "\n",
      "Based  on  in-class  and  out-class  selection  of  labeled  data  it  generates  positive (similar) and negative (dissimilar) pairs of sentences\n",
      "\n",
      "|\n",
      "\n",
      "2. Fine-tuning embeddings\n",
      "\n",
      "Fine-tuning  a  pretrained  embedding  model  based  on  the  previously  generated training data\n",
      "\n",
      "3. Training a classifier\n",
      "\n",
      "Create a classification head on top of the embedding model and train it using the previously generated training data\n",
      "\n",
      "Before  fine-tuning  an  embedding  model,  we  need  to  generate  training  data.  The model  assumes  the  training  data  to  be  samples  of  positive  (similar)  and  negative (dissimilar)  pairs  of  sentences.  However,  when  we  are  dealing  with  a  classification task, our input data is generally not labeled as such.\n",
      "\n",
      "Say, for example, we have the training dataset in Figure 11-9 that classifies text into two categories: text about programming languages, and text about pets.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In  step  1,  SetFit  handles  this  problem  by  generating  the  necessary  data  based  on in-class and out-class selection as we illustrate in Figure 11-10. For example, when we have 16 sentences about sports, we can create 16 * (16 - 1) / 2 = 120 pairs  that  we label as positive pairs. We can use this process to generate negative pairs by collecting pairs from different classes.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "In step 2, we can use the generated sentence pairs to fine-tune the embedding model. This leverages a method called contrastive learning to fine-tune a pretrained BERT model. As we reviewed in Chapter 10, contrastive learning allows accurate sentence embeddings to be learned from pairs of similar (positive) and dissimilar (negative) sentences.\n",
      "\n",
      "Since  we  generated  these  pairs  in  the  previous  step,  we  can  use  them  to  fine-tune a SentenceTransformers model.  Although  we  have  discussed  contrastive  learning before, we again illustrate the method in Figure 11-11 as a refresher.\n",
      "\n",
      "\n",
      "\n",
      "The  goal  of  fine-tuning  this  embedding  model  is  that  it  can  create  embeddings that are tuned to the classification task. The relevance of the classes, and their relative meaning, are distilled into the embeddings through fine-tuning the embedding model.\n",
      "\n",
      "|\n",
      "\n",
      "In step 3, we generate embeddings for all sentences and use those as the input of a classifier.  We  can  use  the  fine-tuned SentenceTransformers model  to  convert  our sentences  into  embeddings  that  we  can  use  as  features.  The  classifier  learns  from our fine-tuned embeddings to accurately predict unseen sentences. This last step is illustrated in Figure 11-12.\n",
      "\n",
      "\n",
      "\n",
      "When  we  put  all  the  steps  together,  we  get  an  efficient  and  elegant  pipeline  for performing classification when you only have a few labels per class. It cleverly makes use of the idea that we have labeled data, although not in the way that we would like it. The three steps together are illustrated in Figure 11-13 to give a single overview of the entire procedure.\n",
      "\n",
      "First, sentence pairs are generated based on in-class and out-class selection. Second, the  sentence  pairs  are  used  to  fine-tune  a  pretrained SentenceTransformer model. Third, the sentences are embedded with the fine-tuned model on which a classifier is trained to predict the classes.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Fine-Tuning for Few-Shot Classification\n",
      "\n",
      "We previously trained on a dataset containing roughly 8,500 movie reviews. However, since this is a few-shot setting, we will only sample 16 examples per class. With two classes,  we  will  only  have  32  documents  to  train  on  compared  to  the  8,500  movie reviews we used before!\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "# We simulate a few-shot setting by sampling 16 examples per class sampled_train_data = sample_dataset(tomatoes[\"train\"], num_samples=16)\n",
      "```\n",
      "\n",
      "After sampling the data, we choose a pretrained SentenceTransformer model to finetune. The official documentation contains an overview of pretrained SentenceTrans former models  from  which  we  are  going  to  be  using \"sentence-transformers/ all-mpnet-base-v2\" .  It  is  one of the best-performing models on the MTEB leaderboard, which shows the performance of embedding models across a variety of tasks:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "# Load a pretrained SentenceTransformer model model = SetFitModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
      "```\n",
      "\n",
      "After loading in the pretrained SentenceTransformer model, we can start defining our SetFitTrainer . By default, a logistic regression model is chosen as the classifier to train.\n",
      "\n",
      "Similar to what we did with Hugging Face Transformers, we can use the trainer to define and play around with relevant parameters. For example, we set the num_epochs to 3 so that contrastive learning will be performed for three epochs:\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "from setfit import TrainingArguments as SetFitTrainingArguments from setfit import Trainer as SetFitTrainer # Define training arguments args = SetFitTrainingArguments( num_epochs=3, # The number of epochs to use for contrastive learning num_iterations=20 # The number of text pairs to generate ) args.eval_strategy = args.evaluation_strategy # Create trainer trainer = SetFitTrainer( model=model, args=args, train_dataset=sampled_train_data, eval_dataset=test_data, metric=\"f1\" )\n",
      "```\n",
      "\n",
      "We only need to call train to start the training loop. When we do, we should get the following output:\n",
      "\n",
      "```\n",
      "# Training loop trainer.train()\n",
      "```\n",
      "\n",
      "```\n",
      "***** Running training ***** Num unique pairs = 1280 Batch size = 16 Num epochs = 3 Total optimization steps = 240\n",
      "```\n",
      "\n",
      "Notice that  the  output  mentions  that  1,280  sentence  pairs  were  generated  for  finetuning the SentenceTransformer model. As a default, 20 sentence pair combinations are generated for each sample in our data, which would be 20 * 32 = 680 samples. We will have to multiply this value by 2 for each positive and negative pair generated, 680 * 2 = 1,280 sentence pairs. Generating 1,280 sentence pairs is quite impressive considering we only had 32 labeled sentences to start with!\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "When we do not specifically define a classification head, by default a  logistic  regression  is  used.  If  we  would  like  to  specify  a  classification  head  ourselves,  we  can  do  so  by  specifying  the  following model in SetFitTrainer :\n",
      "\n",
      "```\n",
      "\"sentence-transformers/all-mpnet-base-v2\", head_params={\"out_features\": num_classes},\n",
      "```\n",
      "\n",
      "```\n",
      "# Load a SetFit model from Hub model = SetFitModel.from_pretrained( use_differentiable_head= True , ) # Create trainer trainer = SetFitTrainer( model=model, ... )\n",
      "```\n",
      "\n",
      "Here, num_classes refers to the number of classes that we want to predict.\n",
      "\n",
      "Next, let's evaluate the model to get a feeling of its performance:\n",
      "\n",
      "```\n",
      "# Evaluate the model on our test data trainer.evaluate() {'f1': 0.8363988383349468}\n",
      "```\n",
      "\n",
      "With only 32 labeled documents, we get an F1 score of 0.85. Considering that the model  was  trained  on  a  tiny  subset  of  the  original  data,  this  is  very  impressive! Moreover, in Chapter 2, we got the same performance but instead trained a logistic regression model on the embeddings of the full data. Thus, this pipeline demonstrates the potential of taking the time to label just a few instances.\n",
      "\n",
      "\n",
      "\n",
      "Not only can SetFit perform few-shot classification tasks, but it also has support for when you have no labels at all, also called zero-shot classification.  SetFit  generates  synthetic  examples  from  the  label names to resemble the classification task and then trains a SetFit model on them. For example, if the target labels are 'happy' and 'sad, '  then  synthetic  data  could  be  'The  example  is  happy'  and 'This example is sad. '\n",
      "\n",
      "|\n",
      "\n",
      "Continued Pretraining with Masked Language Modeling\n",
      "\n",
      "In  the  examples  thus  far,  we  leveraged  a  pretrained  model  and  fine-tuned  it  to perform classification. This process describes a two-step process: first pretraining a model (which was already done for us) and then fine-tuning it for a particular task. We illustrate this process in Figure 11-14.\n",
      "\n",
      "\n",
      "\n",
      "This  two-step  approach  is  typically  used  throughout  many  applications.  It  has  its limitations  when  faced  with  domain-specific  data.  The  pretrained  model  is  often trained on very general data, like Wikipedia pages, and might not be tuned to your domain-specific words.\n",
      "\n",
      "Instead  of  adopting  this  two-step  approach,  we  can  squeeze  another  step  between them,  namely  continue  pretraining  an  already  pretrained  BERT  model.  In  other words,  we  can  simply  continue  training  the  BERT  model  using  masked  language modeling  (MLM)  but  instead  use  data  from  our  domain.  It  is  like  going  from  a general BERT model to a BioBERT model specialized for the medical domain, to a fine-tuned BioBERT model to classify medication.\n",
      "\n",
      "|\n",
      "\n",
      "This  will  update  the  subword  representations  to  be  more  tuned  toward  words  it would  not  have  seen  before.  This  process  is  illustrated  in  Figure  11-15  and  demonstrates how this additional step updates a masked language modeling task. Continuing  pretraining  on  a  pretrained  BERT  model  has  been  shown  to  improve  the performance  of  models  in  classification  tasks  and  is  a  worthwhile  addition  to  the fine-tuning pipeline. 3\n",
      "\n",
      "\n",
      "\n",
      "Instead of having to pretrain an entire model from scratch, we can simply continue pretraining before fine-tuning it for classification. This also helps the model to adapt to  a  certain  domain  or  even  the  lingo  of  a  specific  organization.  The  genealogy  of models a company might want to adopt is further illustrated in Figure 11-16.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "In this example, we will demonstrate how to apply step 2 and continue pretraining an already pretrained BERT model. We use the same data that we started with, namely the Rotten Tomatoes reviews.\n",
      "\n",
      "We start by loading the \"bert-base-cased\" model we have used thus far and prepare it for MLM:\n",
      "\n",
      "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
      "\n",
      "```\n",
      "# Load model for masked language modeling (MLM) model = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\") tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      "```\n",
      "\n",
      "We need to tokenize the raw sentences. We will also remove the labels since this is not a supervised task:\n",
      "\n",
      "```\n",
      "def preprocess_function(examples): return tokenizer(examples[\"text\"], truncation= True ) # Tokenize data tokenized_train = train_data.map(preprocess_function, batched= True ) tokenized_train = tokenized_train.remove_columns(\"label\") tokenized_test = test_data.map(preprocess_function, batched= True ) tokenized_test = tokenized_test.remove_columns(\"label\")\n",
      "```\n",
      "\n",
      "Previously, we used DataCollatorWithPadding , which dynamically pads the input it receives.\n",
      "\n",
      "Instead,  we  will  have  a DataCollator that  will  perform  the  masking  of  tokens  for us.  There  are  two  methods  that  are  generally  used  for  this:  token  and  whole-word masking. With token masking, we randomly mask 15% of the tokens in a sentence. It might happen that part of a word will be masked. To enable masking of the entire word, we could apply whole-word masking, as illustrated in Figure 11-17.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Generally, predicting whole words tends to be more complicated than tokens, which makes the model perform better as it needs to learn more accurate and precise representations  during  training.  However,  it  tends  to  take  a  bit  more  time  to  converge. We  will  be  going  with  token  masking  in  this  example  using DataCollatorForLan guageModeling for  faster  convergence.  However,  we  can  use  whole-word  masking by  replacing DataCollatorForLanguageModeling with DataCollatorForWholeWord Mask . Lastly, we set the probability that a token is masked in a given sentence to 15% ( mlm_probability ):\n",
      "\n",
      "from transformers import DataCollatorForLanguageModeling\n",
      "\n",
      "```\n",
      "# Masking Tokens data_collator = DataCollatorForLanguageModeling( tokenizer=tokenizer, mlm= True , mlm_probability=0.15 )\n",
      "```\n",
      "\n",
      "Next,  we  will  create  the Trainer for  running  the  MLM  task  and  specify  certain parameters:\n",
      "\n",
      "```\n",
      "# Training arguments for parameter tuning training_args = TrainingArguments( \"model\", learning_rate=2e-5, per_device_train_batch_size=16, per_device_eval_batch_size=16, num_train_epochs=10, weight_decay=0.01, save_strategy=\"epoch\", report_to=\"none\" )\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "# Initialize Trainer train_dataset=tokenized_train,\n",
      "```\n",
      "\n",
      "```\n",
      "trainer = Trainer( model=model, args=training_args, eval_dataset=tokenized_test, tokenizer=tokenizer, data_collator=data_collator )\n",
      "```\n",
      "\n",
      "Several parameters are worth noting. We train for 20 epochs and keep the task short. You can experiment with the learning rate and weight decay to ascertain whether they assist in fine-tuning the model.\n",
      "\n",
      "Before  we  start  our  training  loop  we  will  first  save  our  pretrained  tokenizer.  The tokenizer is not updated during training so there is no need to save it after training. We will, however, save our model after we continue pretraining:\n",
      "\n",
      "```\n",
      "# Save pre-trained tokenizer tokenizer.save_pretrained(\"mlm\") # Train model trainer.train() # Save updated model model.save_pretrained(\"mlm\")\n",
      "```\n",
      "\n",
      "This gives us an updated model in the mlm folder.  To  evaluate  its  performance  we would normally fine-tune the model on a variety of tasks. For our purposes, however, we can run some masking tasks to see if it has learned from its continued training.\n",
      "\n",
      "We will do so by loading in our pretrained model before we continue pretraining. Using the sentence \"What a horrible [MASK]!\" the model will predict which word would be in place of \"[MASK]\" :\n",
      "\n",
      "```\n",
      "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
      "```\n",
      "\n",
      "```\n",
      "from transformers import pipeline # Load and create predictions preds = mask_filler(\"What a horrible [MASK]!\") # Print results for pred in preds: print(f\">>> { pred[\"sequence\"] } \") >>> What a horrible idea! >>> What a horrible dream! >>> What a horrible thing! >>> What a horrible day! >>> What a horrible thought!\n",
      "```\n",
      "\n",
      "The  output  demonstrates  concepts  like  'idea, '  'dream, '  and  'day, '  which  definitely make sense. Next, let's see what our updated model predicts:\n",
      "\n",
      "```\n",
      "# Load and create predictions mask_filler = pipeline(\"fill-mask\", model=\"mlm\") preds = mask_filler(\"What a horrible [MASK]!\") # Print results for pred in preds: print(f\">>> { pred[\"sequence\"] } \") >>> What a horrible movie! >>> What a horrible film! >>> What a horrible mess! >>> What a horrible comedy! >>> What a horrible story!\n",
      "```\n",
      "\n",
      "A  horrible  movie,  film,  mess,  etc.  clearly  shows  us  that  the  model  is  more  biased toward the data that we fed it compared to the pretrained model.\n",
      "\n",
      "The next step would be to fine-tune this model on the classification task that we did at the beginning of this chapter. Simply load the model as follows and you are good to go:\n",
      "\n",
      "from transformers import AutoModelForSequenceClassification\n",
      "\n",
      "```\n",
      "# Fine-tune for classification model = AutoModelForSequenceClassification.from_pretrained(\"mlm\", num_labels=2) tokenizer = AutoTokenizer.from_pretrained(\"mlm\")\n",
      "```\n",
      "\n",
      "Named-Entity Recognition\n",
      "\n",
      "In  this  section,  we  will  delve  into  the  process  of  fine-tuning  a  pretrained  BERT model specifically for NER (named-entity recognition). Instead of classifying entire documents, this  procedure  allows  for  the  classification  of  individual  tokens  and/or words, including people and locations. This is especially helpful for de-identification and anonymization tasks when there is sensitive data.\n",
      "\n",
      "NER shares similarities with the classification example we explored at the beginning of this chapter. Nevertheless, a key distinction lies in the preprocessing and classification  of  data.  Given  that  we  are  focusing  on  classifying  individual  words  instead  of entire  documents, we must preprocess the data to consider this granular structure. Figure 11-18 provides a visual representation of this word-level approach.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Fine-tuning the pretrained BERT model follows a similar architecture akin to what we  observed  with  document  classification.  However,  there  is  a  fundamental  shift in  the  classification  approach.  Rather  than  relying  on  the  aggregation  or  pooling of  token  embeddings, the model now makes predictions for individual tokens in a sequence. It is  crucial  to  emphasize  that  our  word-level  classification  task  does  not entail classifying entire words, but rather the tokens that collectively constitute those words. Figure 11-19 provides a visual representation of this token-level classification.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Preparing Data for Named-Entity Recognition\n",
      "\n",
      "In  this  example,  we  will  use  the  English  version  of  the CoNLL-2003 dataset,  which contains  several  different  types  of  named  entities  (person,  organization,  location, miscellaneous, and no entity) and has roughly 14,000 training samples. 4\n",
      "\n",
      "```\n",
      "# The CoNLL-2003 dataset for NER dataset = load_dataset(\"conll2003\", trust_remote_code= True )\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "While  researching  datasets  to  use  for  this  example,  there  were  a few more that we wanted to share. wnut_17 is  a  task  that  focuses on  emerging  and  rare  entities,  those  that  are  more  difficult  to spot. Furthermore, the tner/mit_movie_trivia and tner/mit_res taurant datasets  are  quite  fun  to  use. tner/mit_movie_trivia is for detecting entities like actor, plot, and soundtrack whereas tner/ mit_restaurant aims to detect entities such as amenity, dish, and cuisine. 5\n",
      "\n",
      "Let's inspect the structure of the data with an example:\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "{'id': '848', 'tokens': ['Dean', 'Palmer', 'hit', 'his', '30th', 'homer', 'for', 'the', 'Rangers', '.'], 'pos_tags': [22, 22, 38, 29, 16, 21, 15, 12, 23, 7], 'chunk_tags': [11, 12, 21, 11, 12, 12, 13, 11, 12, 0], 'ner_tags': [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]}\n",
      "```\n",
      "\n",
      "This dataset provides us with labels for each word given in a sentence. These labels can be found in the ner_tags key, which refers to the following possible entities:\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "label2id = { \"O\": 0, \"B-PER\": 1, \"I-PER\": 2, \"B-ORG\": 3, \"I-ORG\": 4, \"B-LOC\": 5, \"I-LOC\": 6, \"B-MISC\": 7, \"I-MISC\": 8 } id2label = {index: label for label, index in label2id.items()} label2id\n",
      "```\n",
      "\n",
      "```\n",
      "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
      "```\n",
      "\n",
      "These entities correspond to specific categories: a person (PER), organization (ORG), location  (LOC),  miscellaneous  entities  (MISC),  and  no  entity  (O).  Note  that  these entities are prefixed with either a B (beginning) or an I (inside). If two tokens that follow each other are part of the same phrase, then the start of that phrase is indicated with B, which is followed by an I to show that they belong to each other and are not independent entities.\n",
      "\n",
      "This process is further illustrated in Figure 11-20. In the figure, since 'Dean' is the start of the phrase and 'Palmer' is the end, we know that 'Dean Palmer' is a person and that 'Dean' and 'Palmer' are not individual people.\n",
      "\n",
      "\n",
      "\n",
      "Our data is preprocessed and split up into words but not yet tokens. To do so, we will tokenize it  further  with  the  tokenizer  of  the  pretrained  model  we  used  throughout this chapter, namely bert-base-cased :\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "from transformers import AutoModelForTokenClassification # Load tokenizer # Load model model = AutoModelForTokenClassification.from_pretrained( \"bert-base-cased\", num_labels=len(id2label), id2label=id2label, label2id=label2id )\n",
      "```\n",
      "\n",
      "```\n",
      "token_ids = tokenizer(example[\"tokens\"], is_split_into_words= True sub_tokens = tokenizer.convert_ids_to_tokens(token_ids) sub_tokens\n",
      "```\n",
      "\n",
      "```\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") Let's explore how the tokenizer would process our example: # Split individual tokens into sub-tokens )[\"input_ids\"]\n",
      "```\n",
      "\n",
      "```\n",
      "['[CLS]', 'Dean', 'Palmer', 'hit', 'his', '30th', 'home', '##r', 'for', 'the', 'Rangers', '.', '[SEP]']\n",
      "```\n",
      "\n",
      "The tokenizer added the [CLS] and [SEP] tokens as we learned in Chapters 2 and 3. Note that the word 'homer' was further split up into the tokens 'home' and '##r' . This creates a bit of a problem for us since we have labeled data at the word level but not at the token level. This can be resolved by aligning the labels with their subtoken counterparts during tokenization.\n",
      "\n",
      "Let's  consider the word 'Maarten' ,  which has the label B-PER to signal that this is a person. If we pass that word through the tokenizer, it splits the word up into the tokens 'Ma' , '##arte' ,  and '##n' .  We  cannot  use  the  B-PER  entity  for  all  tokens as  that  would  signal  that  the  three  tokens  are  all  independent  people.  Whenever an entity is split into tokens, the first token should have B (for beginning) and the following should be I (for inner).\n",
      "\n",
      "Therefore, 'Ma' will get the B-PER to signal the start of a phrase, and '##arte' , and '##n' will get the I-PER to signal they belong to a phrase. This alignment process is illustrated in Figure 11-21.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "We  create  a  function, align_labels ,  that  will  tokenize  the  input  and  align  these tokens with their updated labels during tokenization:\n",
      "\n",
      "```\n",
      "def align_labels(examples): token_ids = tokenizer( examples[\"tokens\"], truncation= True , is_split_into_words= True ) labels = examples[\"ner_tags\"] updated_labels = [] for index, label in enumerate(labels): # Map tokens to their respective word word_ids = token_ids.word_ids(batch_index=index) previous_word_idx = None label_ids = [] for word_idx in word_ids: # The start of a new word if word_idx != previous_word_idx: previous_word_idx = word_idx updated_label = -100 if word_idx is None else label[word_idx] label_ids.append(updated_label) # Special token is -100 elif word_idx is None : label_ids.append(-100) # If the label is B-XXX we change it to I-XXX else : updated_label = label[word_idx] if updated_label % 2 == 1:\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "updated_label += 1 label_ids.append(updated_label) updated_labels.append(label_ids) token_ids[\"labels\"] = updated_labels return token_ids tokenized = dataset.map(align_labels, batched= True ) Looking at our example, note that additional labels ( -100 ) were added for the [CLS] and [SEP] tokens: # Difference between original and updated labels print(f\"Original: { example[\"ner_tags\"] } \") print(f\"Updated: { tokenized[\"train\"][\"labels\"] } \") Original: [1, 2, 0, 0, 0, 0, 0, 0, 3, 0] Updated: [-100, 1, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, -100] Now  that  we  have  tokenized  and  aligned  the  labels,  we  can  start  thinking  about defining our evaluation metrics. This is also different from what we have seen before. Instead of a single prediction per document, we now have multiple predictions per document, namely per token. We will make use of the evaluate package by Hugging Face to create a compute_met rics function that allows us to evaluate performance on a token level: import evaluate # Load sequential evaluation zip(prediction, label):\n",
      "\n",
      "```\n",
      "seqeval = evaluate.load(\"seqeval\") def compute_metrics(eval_pred): # Create predictions logits, labels = eval_pred predictions = np.argmax(logits, axis=2) true_predictions = [] true_labels = [] # Document-level iteration for prediction, label in zip(predictions, labels): # Token-level iteration for token_prediction, token_label in # We ignore special tokens if token_label != -100: true_predictions.append([id2label[token_prediction]]) true_labels.append([id2label[token_label]])\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "results = seqeval.compute( predictions=true_predictions, references=true_labels ) return {\"f1\": results[\"overall_f1\"]}\n",
      "```\n",
      "\n",
      "Fine-Tuning for Named-Entity Recognition\n",
      "\n",
      "We are nearly there. Instead of DataCollatorWithPadding ,  we  need  a  collator  that works with classification on a token level, namely DataCollatorForTokenClassifica tion :\n",
      "\n",
      "```\n",
      "from transformers import DataCollatorForTokenClassification # Token-classification DataCollator data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
      "```\n",
      "\n",
      "Now that  we  have  loaded  our  model,  the  rest  of  the  steps  are  similar  to  previous training procedures in this chapter. We define a trainer with specific arguments that we can tune and create a Trainer :\n",
      "\n",
      "```\n",
      "# Training arguments for parameter tuning training_args = TrainingArguments( \"model\", learning_rate=2e-5, per_device_train_batch_size=16, per_device_eval_batch_size=16, num_train_epochs=1, weight_decay=0.01, save_strategy=\"epoch\", report_to=\"none\" ) # Initialize Trainer trainer = Trainer( model=model, args=training_args, train_dataset=tokenized[\"train\"], eval_dataset=tokenized[\"test\"], tokenizer=tokenizer, data_collator=data_collator, compute_metrics=compute_metrics, ) trainer.train() We then evaluate the model that we created: # Evaluate the model on our test data trainer.evaluate()\n",
      "```\n",
      "\n",
      "Lastly,  let's  save  the  model  and  use  it  in  a  pipeline  for  inference.  This  allows  us  to check certain data so we can manually inspect what happens during inference and if we are satisfied with the output:\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "from transformers import pipeline\n",
      "```\n",
      "\n",
      "```\n",
      "# Save our fine-tuned model trainer.save_model(\"ner_model\") # Run inference on the fine-tuned model token_classifier = pipeline( \"token-classification\", model=\"ner_model\", ) token_classifier(\"My name is Maarten.\")\n",
      "```\n",
      "\n",
      "```\n",
      "[{'entity': 'B-PER', 'score': 0.99534035, 'index': 4, 'word': 'Ma', 'start': 11, 'end': 13}, {'entity': 'I-PER', 'score': 0.9928328, 'index': 5, 'word': '##arte', 'start': 13, 'end': 17}, {'entity': 'I-PER', 'score': 0.9954301, 'index': 6, 'word': '##n', 'start': 17, 'end': 18}]\n",
      "```\n",
      "\n",
      "In the sentence \"My name is Maarten\" , the word \"Maarten\" and its subtokens were correctly identified as a person!\n",
      "\n",
      "Summary\n",
      "\n",
      "In  this  chapter,  we  explored  several  tasks  for  fine-tuning  pretrained  representation models on specific classification tasks. We started by demonstrating how to fine-tune a pretrained BERT model and extended the examples by freezing certain layers of its architectures.\n",
      "\n",
      "We  experimented  with  a  few-shot  classification  technique  called  SetFit,  which involves  fine-tuning  a  pretrained  embedding  model  together  with  a  classification head  using  limited  labeled  data.  Using  only  a  few  labeled  data  points,  this  model generated similar performance to the models we explored in earlier chapters.\n",
      "\n",
      "Next,  we  delved  into  the  concept  of  continued  pretraining,  where  we  used  a  pretrained BERT model as a starting point and continued training it using different data. The underlying process, masked language modeling, is not only used for creating a representation model but can also be used to continue pretraining models.\n",
      "\n",
      "|\n",
      "\n",
      "Finally, we looked at named-entity recognition, a task that involves identifying specific  entities  such  as  people  and  places  in  unstructured  text.  Compared to previous examples,  this  classification  was  done  on  a  word  level  rather  than  on  a  document level.\n",
      "\n",
      "In  the  next  chapter,  we  continue  with  the  field  of  fine-tuning  language  models  but focus on generative models instead. Using a two-step process, we will explore how to fine-tune a generative model to properly follow instructions and then fine-tune it for human preference.\n",
      "\n",
      "|\n",
      "\n",
      "Fine-Tuning Generation Models\n",
      "\n",
      "In this chapter, we will take a pretrained text generation model and go over the process of fine-tuning it.  This  fine-tuning step is key in producing high-quality models and an important tool in our toolbox to adapt a model to a specific desired behavior. Fine-tuning allows us to adapt a model to a specific dataset or domain.\n",
      "\n",
      "Throughout this chapter, we will guide you among the two most common methods for fine-tuning text generation models, supervised fine-tuning and preference tuning . We will explore the transformative potential of fine-tuning pretrained text generation models to make them more effective tools for your application.\n",
      "\n",
      "The Three LLM Training Steps: Pretraining, Supervised Fine-Tuning, and Preference Tuning\n",
      "\n",
      "There are three common steps that lead to creating a high-quality LLM:\n",
      "\n",
      "1. Language modeling\n",
      "\n",
      "The first  step  in  creating  a  high-quality  LLM  is  to  pretrain  it  on  one  or  more massive  text  datasets  (Figure  12-1).  During  training,  it  attempts  to  predict  the next token to accurately learn linguistic and semantic representations found in the text. As we saw before in Chapters 3 and 11, this is called language modeling and is a self-supervised method.\n",
      "\n",
      "This produces a base model, also commonly referred to as a pretrained or foundation model. Base models are a key artifact of the training process but are harder for the end user to deal with. This is why the next step is important.\n",
      "\n",
      "Large language models (LLMs) are models that can generate human-like text by predicting the probability of a word given\n",
      "\n",
      "the previous words used in a sentence.\n",
      "\n",
      "Figure 12-1. During language modeling, the LLM aims to predict the next token based on an input. This is a process without labels.\n",
      "\n",
      "2. Fine-tuning 1 (supervised fine-tuning)\n",
      "\n",
      "LLMs  are  more  useful  if  they  respond  well  to  instructions  and  try  to  follow them. When humans ask the model to write an article, they expect the model to generate the article and not list other instructions for example (which is what a base model might do).\n",
      "\n",
      "With  supervised  fine-tuning  (SFT),  we  can  adapt  the  base  model  to  follow instructions. During this fine-tuning process, the parameters of the base model are updated to be more in line with our target task, like following instructions. Like a pretrained model, it is trained using next-token prediction but instead of only predicting the next token, it does so based on a user input (Figure 12-2).\n",
      "\n",
      "\n",
      "\n",
      "SFT can also be used for other tasks, like classification, but is often used to go from a base generative model to an instruction (or chat) generative model.\n",
      "\n",
      "3. Fine-tuning 2 (preference tuning)\n",
      "\n",
      "|\n",
      "\n",
      "The  final  step  further  improves  the  quality  of  the  model  and  makes  it  more aligned  with  the  expected  behavior  of  AI  safety  or  human  preferences.  This is  called  preference  tuning.  Preference  tuning  is  a  form  of  fine-tuning  and,  as the name implies, aligns the output of the model to our preferences, which are defined by the data that we give it. Like SFT, it can improve upon the original model but has the added benefit of distilling preference of output in its training process.  These  three  steps  are  illustrated  in  Figure  12-3  and  demonstrate  the process of starting from an untrained architecture and ending with a preferencetuned LLM.\n",
      "\n",
      "\n",
      "\n",
      "In this chapter, we use a base model that was already trained on massive datasets and explore how we can fine-tune it using both fine-tuning strategies. For each method, we start with the theoretical underpinnings before using them in practice.\n",
      "\n",
      "Supervised Fine-Tuning (SFT)\n",
      "\n",
      "The purpose of pretraining a model on large datasets is that it is able to reproduce language and its meaning. During this process, the model learns to complete input phrases as shown in Figure 12-4.\n",
      "\n",
      "\n",
      "\n",
      "This example also illustrates that the model was not trained to follow instructions and instead will attempt to complete a question rather than answer it (Figure 12-5).\n",
      "\n",
      "\n",
      "\n",
      "We  can  use  this  base  model  and  adapt  it  to  certain  use  cases,  such  as  following instructions, by fine-tuning it.\n",
      "\n",
      "Full Fine-Tuning\n",
      "\n",
      "The most common fine-tuning process is full fine-tuning. Like pretraining an LLM, this process involves updating all parameters of a model to be in line with your target\n",
      "\n",
      "|\n",
      "\n",
      "task. The main difference is that we now use a smaller but labeled dataset whereas the pretraining process was done on a large dataset without any labels (Figure 12-6).\n",
      "\n",
      "\n",
      "\n",
      "You can use any labeled data for full fine-tuning, making it also a great technique for learning domain-specific representations. To make our LLM follow instructions, we will need question-response data. This data, as shown in Figure 12-7, is queries by the user with corresponding answers.\n",
      "\n",
      "\n",
      "\n",
      "During full  fine-tuning,  the  model  takes  the  input  (instructions)  and  applies  nexttoken prediction on the output (response). In turn, instead of generating new questions, it will follow instructions.\n",
      "\n",
      "|\n",
      "\n",
      "Parameter-Efficient Fine-Tuning (PEFT)\n",
      "\n",
      "Updating all parameters of a model has a large potential of increasing its performance but  comes  with  several  disadvantages.  It  is  costly  to  train,  has  slow  training  times, and  requires  significant  storage.  To  resolve  these  issues,  attention  has  been  given to parameter-efficient fine-tuning (PEFT) alternatives that focus on fine-tuning pretrained models at higher computational efficiency.\n",
      "\n",
      "Adapters\n",
      "\n",
      "Adapters are a core component of many PEFT-based techniques. The method proposes  a  set  of  additional  modular  components  inside  the  Transformer  that  can  be fine-tuned to improve the model's performance on a specific task without having to fine-tune all the model weights. This saves a lot of time and compute.\n",
      "\n",
      "Adapters are described in the paper 'Parameter-efficient transfer learning for NLP', which  showed  that  fine-tuning  3.6%  of  the  parameters  of  BERT  for  a  task  can yield comparable performance to fine-tuning all the model's weights.  On the GLUE 1 benchmark,  the  authors  show  they  reach  within  0.4%  of  the  performance  of  full fine-tuning. In a single Transformer block, the paper's proposed architecture places adapters after the attention layer and the feedforward neural network as illustrated in Figure 12-8.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "It' s  not  enough  to  only  alter  one  Transformer block, however, so these components are part of every block in the model, as Figure 12-9 shows.\n",
      "\n",
      "\n",
      "\n",
      "Seeing all the adapter's components across the model like this enables us to see individual adapters as shown in Figure 12-10, which is a collection of these components spanning all the blocks of the model. Adapter 1 can be a specialist in, say, medical text classification, while Adapter 2 can specialize in named-entity recognition (NER). You can download specialized adapters from https://oreil.ly/XraXg .\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "The  paper  ' AdapterHub:  A  framework  for  adapting  transformers'  introduced  the Adapter Hub as a central repository for sharing adapters.  A lot of these earlier adapt2 ers were more focused on BERT architectures. More recently, the concept has been applied  to  text  generation  Transformers  in  papers  like  'LLaMA-Adapter:  Efficient fine-tuning of language models with zero-init attention'. 3\n",
      "\n",
      "Low-Rank Adaptation (LoRA)\n",
      "\n",
      "As an alternative to adapters, low-rank adaptation (LoRA) was introduced and is at the  time  of  writing  is  a  widely  used  and  effective  technique  for  PEFT.  LoRA  is  a technique that  (like  adapters)  only  requires  updating  a  small  set  of  parameters.  As\n",
      "\n",
      "|\n",
      "\n",
      "illustrated  in  Figure  12-11,  it  creates  a  small  subset  of  the  base  model  to  fine-tune instead of adding layers to the model. 4\n",
      "\n",
      "\n",
      "\n",
      "Like  adapters,  this  subset  allows  for  much  quicker  fine-tuning  since  we  only  need to  update  a  small  part  of  the  base  model.  We  create  this  subset  of  parameters  by approximating large matrices that accompany the original LLM with smaller matrices.  We  can  then  use  those  smaller  matrices  as  a  replacement  and  fine-tune  them instead of the original large matrices. Take for example the 10 × 10 matrix we see in Figure 12-12.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "We can come up with two smaller matrices, which when multiplied, reconstruct a 10 × 10 matrix. This is a major efficiency win because instead of using 100 weights (10 times 10) we now only have 20 weights (10 plus 10), as we can see in Figure 12-13.\n",
      "\n",
      "\n",
      "\n",
      "During  training,  we  only  need  to  update  these  smaller  matrices  instead  of  the  full weight changes. The updated change matrices (smaller matrices) are then combined with the full (frozen) weights as illustrated in Figure 12-14.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "But you might suspect that performance would drop. And you would be right. But where does this trade-off make sense?\n",
      "\n",
      "Papers  like  'Intrinsic  dimensionality  explains  the  effectiveness  of  language  model fine-tuning'  demonstrate  that  language  models  'have  a  very  low  intrinsic  dimension. '   This  means  that  we  can  find  small  ranks  that  approximate even the massive 5 matrices  of  an  LLM.  A  175B  model  like  GPT-3,  for  example,  would  have  a  weight matrix of 12,288 × 12,288 inside each of its 96 Transformer blocks. That's 150 million parameters.  If  we  can  successfully  adapt  that  matrix  into  rank  8,  that  would  only require two 12,288 × 2 matrices resulting in 197K parameters per block. These are major savings in speed, storage, and compute as explained further in the previously referenced LoRA paper.\n",
      "\n",
      "This smaller representation is quite flexible in that you can select which parts of the base  model  to  fine-tune.  For  instance,  we  can  only  fine-tune  the  Query  and  Value weight matrices in each Transformer layer.\n",
      "\n",
      "Compressing the model for (more) efficient training\n",
      "\n",
      "We can make LoRA even more efficient by reducing the memory requirements of the model's original weights before projecting them into smaller matrices. The weights of an LLM are numeric values with a given precision, which can be expressed by the number of bits like float64 or float32. As illustrated in Figure 12-15, if we lower the amount of bits to represent a value, we get a less accurate result. However, if we lower the number of bits we also lower the memory requirements of that model.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "With quantization, we aim to lower the number of bits while still accurately representing the original weight values. However, as shown in Figure 12-16, when directly mapping higher precision values to lower precision values, multiple higher precision values might end up being represented by the same lower precision values.\n",
      "\n",
      "\n",
      "\n",
      "Instead, the authors of QLoRA, a quantized version of LoRA, found a way to go from a  higher  number of bits to a lower value and vice versa without differentiating too much from the original weights. 6\n",
      "\n",
      "They used blockwise quantization to map certain blocks of higher precision values to lower precision values. Instead of directly mapping higher precision to lower precision values, additional blocks are created that allow for quantizing similar weights. As shown in Figure 12-17, this results in values that can be accurately represented with lower precision.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "A nice property of neural networks is that  their  values  are  generally  normally  distributed  between  -1  and  1.  This  property  allows  us  to  bin  the  original  weights  to lower bits based on their relative density, as illustrated in Figure 12-18. The mapping between weights is  more  efficient  as  it  takes  into  account  the  relative  frequency  of weights. This also reduces issues with outliers.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Combined with the blockwise quantization, this normalization procedure allows for accurate  representation  of  high  precision  values  by  low  precision  values  with  only a  small  decrease  in  the  performance  of  the  LLM.  As  a  result,  we  can  go  from  a 16-bit float representation to a measly 4-bit normalized float representation. A 4-bit representation  significantly  reduces  the  memory  requirements  of  the  LLM  during training. Note that the quantization of LLMs in general is also helpful for inference as quantized LLMs are smaller in size and therefore require less VRAM.\n",
      "\n",
      "There are more elegant methods to further optimize this like double quantization and paged  optimizers,  which  you  can  read  about  more  in  the  QLoRA  paper  discussed earlier. For a complete and highly visual guide to quantization, see this blog post.\n",
      "\n",
      "Instruction Tuning with QLoRA\n",
      "\n",
      "Now  that  we  have  explored  how  QLoRA  works,  let  us  put  that  knowledge  into practice!  In  this  section,  we  will  fine-tune  a  completely  open  source  and  smaller version  of  Llama,  TinyLlama,  to  follow  instructions  using  the  QLoRA  procedure. Consider this model a base or pretrained model, one that was trained with language modeling but cannot yet follow instructions.\n",
      "\n",
      "Templating Instruction Data\n",
      "\n",
      "To have the LLM follow instructions, we will need to prepare instruction data that follows a chat template. This chat template, as illustrated in Figure 12-19, differentiates between what the LLM has generated and what the user has generated.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "We chose this chat template to use throughout the examples since the chat version of TinyLlama uses the same format. The data that we are using is a small subset of the UltraChat dataset.  This dataset is a filtered version of the original UltraChat dataset 7 that contains almost 200k conversations between a user and an LLM.\n",
      "\n",
      "We create a function, format_prompt , to make sure that the conversations follow this template:\n",
      "\n",
      "```\n",
      "from transformers import AutoTokenizer from datasets import load_dataset # Load a tokenizer to use its chat template template_tokenizer = AutoTokenizer.from_pretrained( \"TinyLlama/TinyLlama-1.1BChat-v1.0\" ) def format_prompt(example): \"\"\"Format the prompt to using the <|user|> template TinyLLama is using\"\"\" # Format answers chat = example[\"messages\"] prompt = template_tokenizer.apply_chat_template(chat, tokenize= False ) return {\"text\": prompt} # Load and format the data using the template TinyLLama is using dataset = ( load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"test_sft\") .shuffle(seed=42) .select(range(3_000)) ) dataset = dataset.map(format_prompt)\n",
      "```\n",
      "\n",
      "We  select  a  subset  of  3,000  documents  to  reduce  the  training  time,  but  you  can increase this value to get more accurate results.\n",
      "\n",
      "Using the \"text\" column, we can explore these formatted prompts:\n",
      "\n",
      "```\n",
      "# Example of formatted prompt print(dataset[\"text\"])\n",
      "```\n",
      "\n",
      "```\n",
      "<|user|> Given the text: Knock, knock. Who's there? Hike. Can you continue the joke based on the given text material \"Knock, knock. Who's there? Hike\"?</s> <|assistant|> Sure! Knock, knock. Who's there? Hike. Hike who? Hike up your pants, it's cold outside!</s> <|user|>\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "Can you tell me another knock-knock joke based on the same text material \"Knock, knock. Who's there? Hike\"?</s> <|assistant|> Of course! Knock, knock. Who's there? Hike. Hike who? Hike your way over here and let's go for a walk!</s>\n",
      "```\n",
      "\n",
      "Model Quantization\n",
      "\n",
      "Now that  we  have  our  data,  we  can  start  loading  in  our  model.  This  is  where  we apply the Q in QLoRA, namely quantization. We use the bitsandbytes package to compress the pretrained model to a 4-bit representation.\n",
      "\n",
      "In BitsAndBytesConfig , you can define the quantization scheme. We follow the steps used in the original QLoRA paper and load the model in 4-bit ( load_in_4bit ) with a  normalized  float  representation  ( bnb_4bit_quant_type )  and  double  quantization ( bnb_4bit_use_double_quant ):\n",
      "\n",
      "```\n",
      "import torch from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" # 4-bit quantization configuration - Q in QLoRA bnb_config = BitsAndBytesConfig( load_in_4bit= True , # Use 4-bit precision model loading bnb_4bit_quant_type=\"nf4\", # Quantization type bnb_4bit_compute_dtype=\"float16\", # Compute dtype bnb_4bit_use_double_quant= True , # Apply nested quantization ) # Load the model to train on the GPU model = AutoModelForCausalLM.from_pretrained( model_name, device_map=\"auto\", # Leave this out for regular SFT quantization_config=bnb_config, ) model.config.use_cache = False model.config.pretraining_tp = 1 # Load LLaMA tokenizer tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code= True ) tokenizer.pad_token = \"<PAD>\" tokenizer.padding_side = \"left\"\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "This quantization procedure allows us to decrease the size of the original model while retaining most of the original weights' precision. Loading the model now only uses ~1 GB VRAM compared to the ~4 GB of VRAM it would need without quantization. Note that during fine-tuning, more VRAM will be necessary so it does not cap out on the ~1 GB VRAM needed to load the model.\n",
      "\n",
      "LoRA Configuration\n",
      "\n",
      "Next, we will need to define our LoRA configuration using the peft library,  which represents hyperparameters of the fine-tuning process:\n",
      "\n",
      "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
      "\n",
      "```\n",
      "# Prepare LoRA Configuration peft_config = LoraConfig( lora_alpha=32, # LoRA Scaling lora_dropout=0.1, # Dropout for LoRA Layers r=64, # Rank bias=\"none\", task_type=\"CAUSAL_LM\", target_modules= # Layers to target [\"k_proj\", \"gate_proj\", \"v_proj\", \"up_proj\", \"q_proj\", \"o_proj\", \"down_proj\"] ) # Prepare model for training model = prepare_model_for_kbit_training(model) model = get_peft_model(model, peft_config)\n",
      "```\n",
      "\n",
      "There are several parameters worth mentioning: r This  is  the  rank  of  the  compressed  matrices  (recall  this  from  Figure  12-13) Increasing this value will also increase the sizes of compressed matrices leading to less compression and thereby improved representative power. Values typically range between 4 and 64. lora_alpha Controls the amount of change that is added to the original weights. In essence, it balances the knowledge of the original model with that of the new task. A rule of thumb is to choose a value twice the size of r . target_modules Controls  which  layers  to  target.  The  LoRA  procedure  can  choose  to  ignore specific  layers,  like  specific  projection  layers.  This  can  speed  up  training  but reduce performance and vice versa.\n",
      "\n",
      "Playing around with the parameters is a worthwhile experiment to get an intuitive understanding of values that work and those that do not. You can find an amazing\n",
      "\n",
      "|\n",
      "\n",
      "resource  of  additional  tips  on  LoRA  fine-tuning  in  the  Ahead  of  AI  newsletter  by Sebastian Raschka.\n",
      "\n",
      "\n",
      "\n",
      "This  example  demonstrates  an  efficient  form  of  fine-tuning  your model.  If  you  want  to  perform  full  fine-tuning  instead,  you  can remove  the quantization_config parameter  when  loading  the model and skip the creation of peft_config .  By  removing  those, we  would  go  from  'Instruction  tuning  with  QLoRA '  to  'full instruction tuning. '\n",
      "\n",
      "Training Configuration\n",
      "\n",
      "Lastly, we need to configure our training parameters as we did in Chapter 11:\n",
      "\n",
      "```\n",
      "from transformers import TrainingArguments output_dir = \"./results\" # Training arguments training_arguments = TrainingArguments( output_dir=output_dir, per_device_train_batch_size=2, gradient_accumulation_steps=4, optim=\"paged_adamw_32bit\", learning_rate=2e-4, lr_scheduler_type=\"cosine\", num_train_epochs=1, logging_steps=10, fp16= True , gradient_checkpointing= True )\n",
      "```\n",
      "\n",
      "There are several parameters worth mentioning:\n",
      "\n",
      "num_train_epochs\n",
      "\n",
      "The total number of training rounds. Higher values tend to degrade performance so we generally like to keep this low.\n",
      "\n",
      "learning_rate\n",
      "\n",
      "Determines  the  step  size  at  each  iteration  of  weight  updates.  The  authors  of QLoRA found  that  higher  learning  rates  work  better  for  larger  models  (&gt;33B parameters).\n",
      "\n",
      "lr_scheduler_type\n",
      "\n",
      "A cosine-based scheduler to adjust the learning rate dynamically. It will linearly increase the learning rate, starting from zero, until it reaches the set value. After that, the learning rate is decayed following the values of a cosine function.\n",
      "\n",
      "|\n",
      "\n",
      "optim\n",
      "\n",
      "The paged optimizers used in the original QLoRA paper.\n",
      "\n",
      "Optimizing  these  parameters  is  a  difficult  task  and  there  are  no  set  guidelines  for doing  so.  It  requires  experimentation  to  figure  out  what  works  best  for  specific datasets, model sizes, and target tasks.\n",
      "\n",
      "\n",
      "\n",
      "Although this section  describes  instruction  tuning,  we  could  also use  QLoRA  to  fine-tune  an  instruction  model.  For  instance,  we could  fine-tune  a  chat  model  to  generate  specific  SQL  code  or to  create  JSON  output  that  adheres  to  a  specific  format.  As  long as  you  have  the  data  available  (with  appropriate  query-response items),  QLoRA  is  a  great  technique  for  nudging  an  existing  chat model to be more appropriate for your use case.\n",
      "\n",
      "Training\n",
      "\n",
      "Now that we have prepared all our models and parameters, we can start fine-tuning our model. We load in SFTTrainer and simply run trainer.train() :\n",
      "\n",
      "from trl import SFTTrainer\n",
      "\n",
      "```\n",
      "# Set supervised fine-tuning parameters trainer = SFTTrainer( model=model, train_dataset=dataset, dataset_text_field=\"text\", tokenizer=tokenizer, args=training_arguments, max_seq_length=512, # Leave this out for regular SFT peft_config=peft_config, ) # Train model trainer.train() # Save QLoRA weights trainer.model.save_pretrained(\"TinyLlama-1.1B-qlora\")\n",
      "```\n",
      "\n",
      "During  training  the  loss  will  be  printed  every  10  steps  according  to  the log ging_steps parameter.  If  you  are  using  the  free  GPU  provided  by  Google  Colab, which is the Tesla T4 at the time of writing, then training might take up to an hour. A good time to take a break!\n",
      "\n",
      "|\n",
      "\n",
      "Merge Weights\n",
      "\n",
      "After we have trained our QLoRA weights, we still need to combine them with the original weights to use them. We reload the model in 16 bits, instead of the quantized 4 bits, to merge the weights. Although the tokenizer was not updated during training, we save it to the same folder as the model for easier access:\n",
      "\n",
      "```\n",
      "from peft import AutoPeftModelForCausalLM model = AutoPeftModelForCausalLM.from_pretrained( \"TinyLlama-1.1B-qlora\", low_cpu_mem_usage= True , device_map=\"auto\", ) # Merge LoRA and base model merged_model = model.merge_and_unload()\n",
      "```\n",
      "\n",
      "After  merging  the  adapter  with  the  base  model,  we  can  use  it  with  the  prompt template that we defined earlier:\n",
      "\n",
      "```\n",
      "from transformers import pipeline # Use our predefined prompt template prompt = \"\"\"<|user|> Tell me something about Large Language Models.</s> <|assistant|> \"\"\" # Run our instruction-tuned model pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer) print(pipe(prompt)[\"generated_text\"]) Large Language Models (LLMs) are artificial intelligence (AI) models that learn language and understand what it means to say things in a particular language. They are trained on huge amounts of text…\n",
      "```\n",
      "\n",
      "The  aggregate  output  shows  that  the  model  now  closely  follows  our  instructions, which is not possible with the base model.\n",
      "\n",
      "Evaluating Generative Models\n",
      "\n",
      "Evaluating  generative  models  poses  a  significant  challenge.  Generative  models  are used across many diverse use cases, making it a challenge to rely on a singular metric for  judgment. Unlike more specialized models, a generative model's ability to solve mathematical questions does not guarantee success in solving coding questions.\n",
      "\n",
      "At the same time, evaluating these models is vital, particularly in production settings where consistency is important. Given their probabilistic nature, generative models do not necessarily generate consistent outputs; there is a need for robust evaluation.\n",
      "\n",
      "|\n",
      "\n",
      "In this section, we will explore a few common evaluation methods, but we want to emphasize the current lack of golden standards. No one metric is perfect for all use cases.\n",
      "\n",
      "Word-Level Metrics\n",
      "\n",
      "One common metrics category for comparing generative models is word-level evaluation. These classic techniques compare a reference dataset with the generated tokens on  a  token(set)  level.  Common  word-level  metrics  include  perplexity,   ROUGE, 8 9 BLEU, 10 and BERTScore. 11\n",
      "\n",
      "Of  note  is  perplexity,  which  measures  how  well  a  language  model  predicts  a  text. Given input text, the model predicts how likely the next token is. With perplexity, we assume a model performs better if it gives the next token a high probability. In other words,  the  models  should  not  be  'perplexed'  when  presented  with  a  well-written document.\n",
      "\n",
      "As  illustrated  in  Figure  12-20,  when  presented  with  the  input  'When  a  measure becomes a,' the model is asked how probable the word 'target' is as the next word.\n",
      "\n",
      "\n",
      "\n",
      "Although perplexity, and other word-level metrics, are useful metrics to understand the confidence of the model, they are not a perfect measure. They do not account for consistency, fluency, creativity, or even correctness of the generated text.\n",
      "\n",
      "Benchmarks\n",
      "\n",
      "A  common  method  for  evaluating  generative  models  on  language  generation  and understanding  tasks  is  on  well-known  and  public  benchmarks,  such  as  MMLU, 12 GLUE, 13 TruthfulQA, 14 GSM8k, 15 and HellaSwag. 16 These benchmarks  give us\n",
      "\n",
      "|\n",
      "\n",
      "information about basic language understanding but also complex analytical answering, like math problems.\n",
      "\n",
      "Aside  from  natural  language  tasks,  some  models  specialize  in  other  domains,  like programming.  These  models  tend  to  be  evaluated  on  different  benchmarks,  such as HumanEval, 17 which consists of challenging programming tasks for the model to solve.  Table  12-1  gives  an  overview  of  common  public  benchmarks  for  generative models.\n",
      "\n",
      "\n",
      "\n",
      "Benchmarks  are  a  great  way  to  get  a  basic  understanding  on  how  well  a  model performs on a wide variety of tasks. A downside to public benchmarks is that models can  be  overfitted  to  these  benchmarks  to  generate  the  best  responses.  Moreover, these are still broad benchmarks and might not cover very specific use cases. Lastly, another downside is that some benchmarks require strong GPUs with a long running time (over hours) to compute, which makes iteration difficult.\n",
      "\n",
      "|\n",
      "\n",
      "Leaderboards\n",
      "\n",
      "With so many different benchmarks, it is hard to choose which benchmark best suits your model. Whenever a model is released, you will often see it evaluated on several benchmarks to showcase how it performs across the board.\n",
      "\n",
      "As such, leaderboards were developed containing multiple benchmarks. A common leaderboard is the Open LLM Leaderboard, which, at the time of writing, includes six benchmarks, including HellaSwag, MMLU, TruthfulQA, and GSM8k. Models that top the leaderboard, assuming they were not overfitted on the data, are generally regarded  as  the  'best'  model.  However,  since  these  leaderboards  often  contain  publicly available benchmarks, there is a risk of overfitting on the leaderboard.\n",
      "\n",
      "Automated Evaluation\n",
      "\n",
      "Part of evaluating a generative output is the quality of its text. For instance, even if two models were to give the same correct answer to a question, the way they derived that answer might be different. It is often not just about the final answer but also the construction of it. Similarly, although two summaries might be similar, one could be significantly shorter than another, which is often important for a good summary.\n",
      "\n",
      "To evaluate the quality of the generated text above the correctness of the final answer, LLM-as-a-judge was introduced. 18 In  essence,  a  separate  LLM  is  asked  to  judge  the quality of the LLM to be evaluated. An interesting variant of this method is pairwise comparison. Two different LLMs will generate an answer to a question and a third LLM will be the judge to declare which is better.\n",
      "\n",
      "As a result, this methodology allows for automated evaluation of open-ended questions. A major advantage is that as LLMs improve, so do their capabilities to judge the quality of output. In other words, this evaluation methodology grows with the field.\n",
      "\n",
      "Human Evaluation\n",
      "\n",
      "Although  benchmarks  are  important,  the  gold  standard  of  evaluation  is  generally  considered  to  be  human  evaluation.  Even  if  an  LLM  scores  well  on  broad benchmarks, it still might not score well on domain-specific tasks. Moreover, benchmarks do not fully capture human preference and all methods discussed before are merely proxies for that.\n",
      "\n",
      "A  great  example  of  a  human-based  evaluation  technique  is  the  Chatbot  Arena. 19 When you go to this leaderboard you are shown two (anonymous) LLMs you can interact with. Any question or prompt you ask will be sent to both models and you\n",
      "\n",
      "|\n",
      "\n",
      "will receive their output. Then, you can decide which output you prefer. This process allows  for  the  community  to  vote  on  which  models  they  prefer  without  knowing which  ones  are  presented.  Only  after  you  vote  do  you  see  which  model  generated which text.\n",
      "\n",
      "At the time of writing, this method has generated over 800,000+ human votes that were used to compute a leaderboard. These votes are used to calculate the relative skill level of LLMs based on their win rates. For instance, if a low-ranked LLM beats a high-ranked LLM, its ranking changes significantly. In chess, this is referred to as the Elo rating system.\n",
      "\n",
      "This methodology therefore uses crowdsourced votes, which helps us understand the quality  of  the  LLM.  However,  it  is  still  the  aggregated  opinion  of  a  wide  variety  of users, which might not relate to your use case.\n",
      "\n",
      "As  a  result,  there  is  no  one  perfect  method  of  evaluating  LLMs.  All  mentioned methodologies and benchmarks provide an important, although limited evaluation perspective. Our advice is to evaluate your LLM based on the intended use case. For coding, HumanEval would be more logical than GSM8k.\n",
      "\n",
      "But most importantly, we believe that you are the best evaluator. Human evaluation remains the gold standard because it is up to you to decide whether the LLM works for  your  intended  use  case.  As  with  the  examples  in  this  chapter,  we  highly  advise that  you  also  try  these  models  and  perhaps  develop  some  questions  yourself.  For example,  the  authors  of  this  book  are  Arabic  (Jay  Alammar)  and  Dutch  (Maarten Grootendorst), and we often ask questions in our native language when approached with new models.\n",
      "\n",
      "One final note on this topic is a quote we hold dear:\n",
      "\n",
      "When a measure becomes a target, it ceases to be a good measure.\n",
      "\n",
      "-Goodhart's Law 20\n",
      "\n",
      "In the context of LLMs, when using a specific benchmark, we tend to optimize for that benchmark regardless of the consequences. For instance, if we focus purely on optimizing for generating grammatically correct sentences, the model could learn to only output one sentence: 'This is a sentence. ' It is grammatically correct but tells you nothing about its language understanding capabilities. Thus, the model may excel at a specific benchmark but potentially at the expense of other useful capabilities.\n",
      "\n",
      "|\n",
      "\n",
      "Preference-Tuning / Alignment / RLHF\n",
      "\n",
      "Although our model can now follow instructions, we can further improve its behavior by a final training phase that aligns it to how we expect it to behave in different scenarios. For instance, when asked 'What is an LLM?' we might prefer an elaborate answer that describes the internals of an LLM compared to the answer 'It is a large language model' without further explanations. How exactly do we align our (human) preference for one answer over the other with the output of an LLM?\n",
      "\n",
      "To start with, recall that an LLM takes a prompt and outputs a generation as illustrated in Figure 12-21.\n",
      "\n",
      "\n",
      "\n",
      "We  can  ask  a  person  (preference  evaluator)  to  evaluate  the  quality  of  that  model generation. Say they assign it a certain score, like 4 (see Figure 12-22).\n",
      "\n",
      "\n",
      "\n",
      "Figure 12-23 shows a preference tuning step updating the model based on that score:\n",
      "\n",
      " · If  the score is high, the model is updated to encourage it to generate more like this type of generation.\n",
      "\n",
      " · If the score is low, the model is updated to discourage such generations.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "As  always,  we  need  many  training  examples.  So  can  we  automate  the  preference evaluation? Yes, we can by training a different model called a reward model.\n",
      "\n",
      "Automating Preference Evaluation Using Reward Models\n",
      "\n",
      "To automate preference evaluation, we need a step before the preference-tuning step, namely to train a reward model, as shown in Figure 12-24.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "The Inputs and Outputs of a Reward Model\n",
      "\n",
      "The  way  we  expect  this  reward  model  to  work  is  that  we  give  it  a  prompt  and a  generation,  and  it  outputs  a  single  number  indicating  the  preference/quality  of that  generation  in  response  to  that  prompt.  Figure  12-26  shows  the  reward  model generating this single number.\n",
      "\n",
      "\n",
      "\n",
      "Training a Reward Model\n",
      "\n",
      "We cannot directly use the reward model. It needs to first be trained to properly score generations. So let's get a preference dataset that the model can learn from.\n",
      "\n",
      "Reward model training dataset\n",
      "\n",
      "One  common  shape  for  preference  datasets  is  for  a  training  example  to  have  a prompt, with one accepted generation and one rejected generation. (Nuance: it's not always a good versus bad generation; it can be that the two generations are both good, but  that  one  is  better  than  the  other).  Figure  12-27  shows  an  example  preference training set with two training examples.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "One way to generate  preference  data  is  to  present  a  prompt  to  the  LLM  and  have it  generate two different generations. As shown in Figure 12-28, we can ask human labelers which of the two they prefer.\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Reward model training step\n",
      "\n",
      "Now that we have the preference training dataset, we can proceed to train the reward model.\n",
      "\n",
      "A simple step is that we use the reward model to:\n",
      "\n",
      " 1. Score the accepted generation\n",
      "\n",
      " 2. Score the rejected generation\n",
      "\n",
      "\n",
      "\n",
      "When we combine everything together as shown in Figure 12-30, we get the three stages to preference tuning:\n",
      "\n",
      " 1. Collect preference data\n",
      "\n",
      " 2. Train a reward model\n",
      "\n",
      " 3. Use  the  reward  model  to  fine-tune  the  LLM  (operating  as  the  preference evaluator)\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Reward  models  are  an  excellent  idea  that  can  be  further  extended  and  developed. Llama  2,  for  example,  trains  two  reward  models:  one  that  scores  helpfulness  and another that scores safety (Figure 12-31).\n",
      "\n",
      "\n",
      "\n",
      "A common method to fine-tune the LLM with the trained reward model is Proximal Policy Optimization (PPO). PPO is a popular reinforcement technique that optimizes the instruction-tuned LLM by making sure that the LLM does not deviate too much from the expected rewards. 21 It was even used to train the original ChatGPT released in November 2022.\n",
      "\n",
      "|\n",
      "\n",
      "Training No Reward Model\n",
      "\n",
      "A  disadvantage  of  PPO  is  that  it  is  a  complex  method  that  needs  to  train  at  least two models, the reward model and the LLM, which can be more costly than perhaps necessary.\n",
      "\n",
      "Direct Preference Optimization (DPO) is an alternative to PPO and does away with the reinforcement-based learning procedure. 22 Instead of using the reward model to judge  the  quality  of  a  generation,  we  let  the  LLM  itself  do  that.  As  illustrated  in Figure  12-32,  we  use  a  copy  of  the  LLM  as  the  reference  model  to  judge  the  shift between the reference and trainable model in the quality of the accepted generation and rejected generation.\n",
      "\n",
      "\n",
      "\n",
      "By calculating this shift during training, we can optimize the likelihood of accepted generations  over  rejected  generations  by  tracking  the  difference  in  the  reference model and the trainable model.\n",
      "\n",
      "To  calculate  this  shift  and  its  related  scores,  the  log  probabilities  of  the  rejected generations and accepted generations are extracted from both models. As illustrated in Figure 12-33, this process is performed at a token level where the probabilities are combined to calculate the shift between the reference and trainable models.\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "Using these scores, we can optimize the parameters of the trainable model to be more confident of generating the accepted generations and less confident of generating the rejected generations. Compared to PPO, the authors found DPO to be more stable during  training  and  more  accurate.  Due  to  its  stability,  we  will  be  using  it  as  our primary model for preference tuning our previously instruction-tuned model.\n",
      "\n",
      "Preference Tuning with DPO\n",
      "\n",
      "When  we  use  the  Hugging  Face  stack,  preference  tuning  is  eerily  similar  to  the instruction  tuning  we  covered  before  with  some  slight  differences.  We  will  still  be using  TinyLlama  but  this  time  an  instruction-tuned  version  that  was  first  trained using full  fine-tuning  and  then  further  aligned  with  DPO.  Compared  to  our  initial instruction-tuned model, this LLM was trained on much larger datasets.\n",
      "\n",
      "In this section, we will demonstrate how you can further align this model using DPO with reward-based datasets.\n",
      "\n",
      "|\n",
      "\n",
      "Templating Alignment Data\n",
      "\n",
      "We will  use  a  dataset  that  for  each  prompt  contains  an  accepted  generation  and  a rejected generation. This dataset was in part generated by ChatGPT with scores on which output should be accepted and which rejected:\n",
      "\n",
      "```\n",
      "from datasets import load_dataset def format_prompt(example): \"\"\"Format the prompt to using the <|user|> template TinyLLama is using\"\"\" # Format answers system = \"<|system|> \\n \" + example[\"system\"] + \"</s> \\n \" prompt = \"<|user|> \\n \" + example[\"input\"] + \"</s> \\n <|assistant|> \\n \" chosen = example[\"chosen\"] + \"</s> \\n \" rejected = example[\"rejected\"] + \"</s> \\n \" return { \"prompt\": system + prompt, \"chosen\": chosen, \"rejected\": rejected, } # Apply formatting to the dataset and select relatively short answers dpo_dataset = load_dataset( \"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\" ) dpo_dataset = dpo_dataset.filter( lambda r: r[\"status\"] != \"tie\" and r[\"chosen_score\"] >= 8 and not r[\"in_gsm8k_train\"] ) dpo_dataset = dpo_dataset.map( format_prompt,  remove_columns=dpo_dataset.column_names ) dpo_dataset\n",
      "```\n",
      "\n",
      "Note  that  we  apply  additional  filtering  to  further  reduce  the  size  of  the  data  to roughly 6,000 examples from the original 13,000 examples.\n",
      "\n",
      "Model Quantization\n",
      "\n",
      "We load our base model and load it with the LoRA we created previously. As before, we quantize the model to reduce the necessary VRAM for training:\n",
      "\n",
      "```\n",
      "from peft import AutoPeftModelForCausalLM from transformers import BitsAndBytesConfig, AutoTokenizer # 4-bit quantization configuration - Q in QLoRA bnb_config = BitsAndBytesConfig( load_in_4bit= True , # Use 4-bit precision model loading\n",
      "```\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "bnb_4bit_quant_type=\"nf4\", # Quantization type bnb_4bit_compute_dtype=\"float16\", # Compute dtype bnb_4bit_use_double_quant= True , # Apply nested quantization ) # Merge LoRA and base model model = AutoPeftModelForCausalLM.from_pretrained( \"TinyLlama-1.1B-qlora\", low_cpu_mem_usage= True , device_map=\"auto\", quantization_config=bnb_config, ) merged_model = model.merge_and_unload() # Load LLaMA tokenizer model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\" tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code= True ) tokenizer.pad_token = \"<PAD>\" tokenizer.padding_side = \"left\" Next, we use the same LoRA configuration as before to perform the DPO training: from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model # Prepare LoRA configuration peft_config = LoraConfig( lora_alpha=32, # LoRA Scaling lora_dropout=0.1, # Dropout for LoRA Layers r=64, # Rank bias=\"none\", task_type=\"CAUSAL_LM\", target_modules= # Layers to target [\"k_proj\", \"gate_proj\", \"v_proj\", \"up_proj\", \"q_proj\", \"o_proj\", \"down_proj\"] ) # prepare model for training model = prepare_model_for_kbit_training(model) model = get_peft_model(model, peft_config)\n",
      "```\n",
      "\n",
      "Training Configuration\n",
      "\n",
      "For the sake of simplicity, we will use the same training arguments as we did before with  one  difference.  Instead  of  running  for  a  single  epoch  (which  can  take  up  to two  hours),  we  run  for  200  steps  instead  for  illustration  purposes.  Moreover,  we added the warmup_ratio parameter, which increases the learning rate from 0 to the learning_rate value we set for the first 10% of steps. By maintaining a small learning rate at the start (i.e., warmup period), we allow the model to adjust to the data before applying larger learning rates, therefore avoiding harmful divergence:\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "from trl import DPOConfig output_dir = \"./results\" # Training arguments training_arguments = DPOConfig( output_dir=output_dir, per_device_train_batch_size=2, gradient_accumulation_steps=4, optim=\"paged_adamw_32bit\", learning_rate=1e-5, lr_scheduler_type=\"cosine\", max_steps=200, logging_steps=10, fp16= True , gradient_checkpointing= True , warmup_ratio=0.1 )\n",
      "```\n",
      "\n",
      "Training\n",
      "\n",
      "Now that we have prepared all our models and parameters, we can start fine-tuning our model:\n",
      "\n",
      "from trl import DPOTrainer # Create DPO trainer dpo_trainer = DPOTrainer( model, args=training_arguments, train_dataset=dpo_dataset, tokenizer=tokenizer, peft_config=peft_config, beta=0.1, max_prompt_length=512, max_length=512, ) # Fine-tune model with DPO dpo_trainer.train() # Save adapter dpo_trainer.model.save_pretrained(\"TinyLlama-1.1B-dpo-qlora\") We have created a second adapter. To merge both adapters, we iteratively merge the adapters with the base model: from peft import PeftModel model = AutoPeftModelForCausalLM.from_pretrained(\n",
      "\n",
      "|\n",
      "\n",
      "```\n",
      "# Merge LoRA and base model \"TinyLlama-1.1B-qlora\",\n",
      "```\n",
      "\n",
      "```\n",
      "low_cpu_mem_usage= True , device_map=\"auto\", ) sft_model = model.merge_and_unload() # Merge DPO LoRA and SFT model dpo_model = PeftModel.from_pretrained( sft_model, \"TinyLlama-1.1B-dpo-qlora\", device_map=\"auto\", ) dpo_model = dpo_model.merge_and_unload()\n",
      "```\n",
      "\n",
      "This  combination  of  SFT+DPO  is  a  great  way  to  first  fine-tune  your  model  to perform basic chatting and then align its answers with human preference. However, it  does  come at a cost since we need to perform two training loops and potentially tweak the parameters in two processes.\n",
      "\n",
      "Since the release of DPO, new methods of aligning preferences have been developed. Of note is Odds Ratio Preference Optimization (ORPO), a process that combines SFT and DPO into a single training process. 23 It removes the need to perform two separate training loops, further simplifying the training process while allowing for the use of QLoRA.\n",
      "\n",
      "Summary\n",
      "\n",
      "In this chapter, we explored different steps of fine-tuning pretrained LLMs. We performed fine-tuning by making use of parameter-efficient fine-tuning (PEFT) through the low-rank adaptation (LoRA) technique. We explained how LoRA can be extended through quantization, a technique for reducing memory constraints when representing the parameters of the model and adapters.\n",
      "\n",
      "The fine-tuning process we explored has two steps. In the first step, we performed supervised  fine-tuning  using  instruction  data  on  a  pretrained  LLM,  often  called instruction  tuning.  This  resulted  in  a  model  that  has  chat-like  behavior  and  could closely follow instructions.\n",
      "\n",
      "In  the  second  step,  we  further  improved  the  model  by  fine-tuning  it  on  alignment data, data that represents what type of answers are preferred over others. This process,  referred  to  as  preference  tuning,  distills  human  preference  to  the  previously instruction-tuned model.\n",
      "\n",
      "Overall, this chapter has shown the two major steps of fine-tuning a pretrained LLM and how that could lead to more accurate and informative outputs.\n",
      "\n",
      "|\n",
      "\n",
      "Afterword\n",
      "\n",
      "Thank  you  to  all  who  joined  us  on  this  fascinating  journey  through  the  world  of large  language  models.  We  are  grateful  for  your  dedication  to  learning  about  these powerful models that have revolutionized language processing.\n",
      "\n",
      "Throughout  this  book,  we  have  seen  how  LLMs  work  and  how  they  can  be  used to  create  a  wide  range  of  applications,  from  simple  chatbots  to  more  complex  systems  like  search  engines.  We  have  also  explored  various  methods  for  fine-tuning pretrained LLMs on specific tasks, including classification, generation, and language representation.  By  mastering  these  techniques,  readers  will  be  able  to  unlock  the potential of LLMs and create innovative solutions that can benefit from their capabilities. This knowledge will enable readers to stay ahead of the curve and adapt to new developments in the field.\n",
      "\n",
      "As we come to the end of this book, we want to emphasize that our exploration of LLMs  is  only  just  the  beginning.  There  are  many  more  exciting  developments  on the horizon, and we encourage you to continue following the advancements in the field. To help with this process, keep an eye out on the repository of this book as we continue to add resources.\n",
      "\n",
      "We hope that by reading this book, you gained a deeper understanding of how LLMs can  be  used  in  various  applications  and  how  they  have  the  potential  to  transform industries.\n",
      "\n",
      "With this book as your guide, we believe that you will be well-equipped to navigate the  exciting  landscape  of  LLMs  and  make  meaningful  contributions  to  this  rapidly advancing field.\n",
      "\n",
      "A\n",
      "\n",
      "\n",
      "\n",
      "accuracy\n",
      "\n",
      "Index\n",
      "\n",
      "optimizing attention, 98\n",
      "\n",
      "\n",
      "\n",
      "B\n",
      "\n",
      "\n",
      "\n",
      "bag-of-words model, 6-7\n",
      "\n",
      "\n",
      "\n",
      "C\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "generating embeddings, 123\n",
      "\n",
      "\n",
      "\n",
      "D\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "E\n",
      "\n",
      "Elo rating system, 377\n",
      "\n",
      "easy negatives, 308\n",
      "\n",
      "embeddings, 8-10, 37, 57-70\n",
      "\n",
      "embedding models, defined, 114, 290\n",
      "\n",
      "dense retrieval, 226, 228-240\n",
      "\n",
      "multimodality, 263-272\n",
      "\n",
      "OpenCLIP, 268-272\n",
      "\n",
      "CLIP, 265-268\n",
      "\n",
      "overview of, 8-10, 289-291\n",
      "\n",
      "recommendation systems, 67-70\n",
      "\n",
      "positional embeddings, 102-104\n",
      "\n",
      "text classification tasks that leverage,\n",
      "\n",
      "supervised classification, 121-123\n",
      "\n",
      "120-126\n",
      "\n",
      "zero-shot classification, 123-126\n",
      "\n",
      "cluster model, 142-144\n",
      "\n",
      "text clustering pipeline, 139-146\n",
      "\n",
      "dimensionality reduction model,\n",
      "\n",
      "embedding model, 139\n",
      "\n",
      "140-142\n",
      "\n",
      "inspecting clusters, 144-146\n",
      "\n",
      "contrastive learning, 291-293\n",
      "\n",
      "text embedding models, 61-63, 289-321\n",
      "\n",
      "creating, 296-308\n",
      "\n",
      "SBERT, 293-296\n",
      "\n",
      "fine-tuning, 309-315\n",
      "\n",
      "unsupervised learning, 316\n",
      "\n",
      "creating contextualized word embed-\n",
      "\n",
      "token embeddings, 57-61\n",
      "\n",
      "dings, 58-61\n",
      "\n",
      "types of, 10\n",
      "\n",
      "tokenizer's vocabulary and, 57\n",
      "\n",
      "word embeddings, 63-67\n",
      "\n",
      "pretrained, 63\n",
      "\n",
      "\n",
      "\n",
      "F\n",
      "\n",
      "\n",
      "\n",
      "F1 score, confusion matrices, 120\n",
      "\n",
      "G\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "Galactica, 52\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "\n",
      "\n",
      "H\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "\n",
      "\n",
      "J\n",
      "\n",
      "\n",
      "\n",
      "Jupyter, 85\n",
      "\n",
      "K\n",
      "\n",
      "\n",
      "\n",
      "L\n",
      "\n",
      "\n",
      "\n",
      "few-shot classification, 333-339\n",
      "\n",
      "movie reviews, 112, 113\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "with representation models, 113-126\n",
      "\n",
      "\n",
      "\n",
      "M\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "mask_token [MASK], 48\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "N\n",
      "\n",
      "\n",
      "\n",
      "304-308\n",
      "\n",
      "named-entity recognition (see NER)\n",
      "\n",
      "O\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "P\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "pad_token [PAD], 47\n",
      "\n",
      "\n",
      "\n",
      "Q\n",
      "\n",
      "\n",
      "\n",
      "R\n",
      "\n",
      "\n",
      "\n",
      "r parameter, 370\n",
      "\n",
      "|\n",
      "\n",
      "ReAct\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "RWKV, 24\n",
      "\n",
      "S\n",
      "\n",
      "\n",
      "\n",
      "templating instruction data, 367\n",
      "\n",
      "\n",
      "\n",
      "T\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "model selection, 115-116\n",
      "\n",
      "128-131\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "thenlper/gte-small model, 140\n",
      "\n",
      "\n",
      "\n",
      "transfer learning, 19\n",
      "\n",
      "overview of, 316-320\n",
      "\n",
      "\n",
      "\n",
      "U\n",
      "\n",
      "\n",
      "\n",
      "V\n",
      "\n",
      "\n",
      "\n",
      "valid output, verifying, 192\n",
      "\n",
      "\n",
      "\n",
      "W\n",
      "\n",
      "\n",
      "\n",
      "warmup_ratio parameter, 387\n",
      "\n",
      "&lt;work&gt; token, 53\n",
      "\n",
      "Y\n",
      "\n",
      "\n",
      "\n",
      "Year of Generative AI, 23-24\n",
      "\n",
      "Z\n",
      "\n",
      "\n",
      "\n",
      "|\n",
      "\n",
      "About the Authors\n",
      "\n",
      "Jay Alammar is Director and Engineering Fellow at Cohere (pioneering provider of large  language  models as an API). In this role, he advises and educates enterprises and  the  developer  community  on  using  language  models  for  practical  use  cases. Through his popular AI/ML blog, Jay has helped millions of researchers and engineers visually understand machine learning tools and concepts from the basic (ending up in the documentation of packages like NumPy and pandas) to the cutting-edge (Transformers,  BERT,  GPT-3,  Stable  Diffusion).  Jay  is  also  a  co-creator  of  popular machine learning  and  natural  language  processing  courses  on  Deeplearning.ai  and Udacity.\n",
      "\n",
      "Maarten  Grootendorst is  a  Senior  Clinical  Data  Scientist  at  IKNL  (Netherlands Comprehensive Cancer Organization).  He  holds  master's  degrees  in  organizational psychology, clinical psychology, and data science, which he leverages to communicate complex  machine  learning  concepts  to  a  wide  audience.  With  his  popular  blogs, he has reached millions of readers by explaining the fundamentals of artificial intelligence-often from a psychological  point  of  view.  He  is  the  author  and  maintainer of several open source packages that rely on the strength of large language models, such as BERTopic, PolyFuzz, and KeyBERT. His packages are downloaded millions of times and used by data professionals and organizations worldwide.\n",
      "\n",
      "Colophon\n",
      "\n",
      "The  animal  on  the  cover  of Hands-On  Large  Language  Models is  a  red  kangaroo ( Osphranter rufus ). They are the largest of all kangaroos, with a body length that can get up to a little over 5 feet and a tail as long as 3 feet. They are very fast and can hop to speeds over 35 miles per hour. They can jump 6 feet high and leap a distance of 25 feet in a single bound. The position of their eyes allows them see up to 300 degrees.\n",
      "\n",
      "Red kangaroos are named after the color of their fur. While the name makes sense for the males-they have short, red-brown fur-females are typically more of a blue-grey color with a tinge of brown throughout. The red color in their fur comes from a red oil  excreted  from  the  glands  in  their  skin.  Because  of  their  color,  Australians  refer to male red kangaroos as 'big reds. ' However, because females are faster than males, they are often called 'blue fliers. '\n",
      "\n",
      "Preferring  open,  dry  areas  with  some  trees  for  shade,  red  kangaroos  can  be  found across  Australia's  mainland  except  in  the  upper  north,  lower  southwest,  and  east coast regions of the country. Surrounding environmental conditions can affect reproduction.  Because  of  this,  females  can  pause  or  postpone  pregnancy  or  birth  until conditions are better. They often use this ability to delay birth of a new baby (joey) until the previous one has left their pouch.\n",
      "\n",
      "The cover illustration is by Karen Montgomery, based on an antique line engraving from Cassell's  Popular Natural History .  The  series  design is by Edie Freedman, Ellie Volckhausen,  and  Karen  Montgomery.  The  cover  fonts  are  Gilroy  Semibold  and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag's Ubuntu Mono.\n",
      "\n",
      "OREILLY\n",
      "\n",
      "Learn from experts. Become one yourself.\n",
      "\n",
      "Books | Live online courses Instant answers | Virtual events Videos | Interactive learning\n",
      "\n",
      "Get started at oreilly.com.\n"
     ]
    }
   ],
   "source": [
    "source = \"../data/Hands-On Large Language Models Language.pdf\"\n",
    "print(convert_pdf_to_text(source))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bookshelf\n",
      "\n",
      "\n",
      "\n",
      "THE EASY WAY To DO MORE [N LESS TIME\n",
      "\n",
      "\n",
      "\n",
      "What Readers Are Saying About Pomodoro Technique Illustrated\n",
      "\n",
      "Staffan gives us the theory and practice of Francesco Cirillo's Pomodoro Technique in an enjoyable package with wonderful illustrations. This is a delightful and useful book!\n",
      "\n",
      "Ron Jeffries\n",
      "\n",
      "www.XProgramming.com\n",
      "\n",
      "The Pomodoro Technique is amazing in its simplicity and its power to make you more productive, and this book is the perfect introduction to the technique.\n",
      "\n",
      " Dave Klein\n",
      "\n",
      "Author, Grails: A Quick-Start Guide\n",
      "\n",
      "Thank you for such a beautiful and thoughtful book! It's on my Christmas list for my friends.\n",
      "\n",
      "Portia Tung\n",
      "\n",
      "Agile Consultant-Coach and Chief Strategy Officer, emergn\n",
      "\n",
      "The Pomodoro Technique is the one action-planning technique that fits exactly as conceived into Agile approaches to projects. If you want to learn the technique and become excellent at it, you need this book. Staffan brings humor, examples, and a step-by-step approach to making the Pomodoro Technique work for you. Your overall estimates will become better, and you'll get more work done.\n",
      "\n",
      "Johanna Rothman\n",
      "\n",
      "Author, Consultant\n",
      "\n",
      "This is an easy read with a life-changing message to all of us who have 'too much to do and not enough time.' Since Staffan introduced me to the Pomodoro Technique, I have become a better person, both professionally and in my private life. This is a book I wish I had read years ago!\n",
      "\n",
      "Thomas Nilsson\n",
      "\n",
      "CTO, Agile Mentor, Responsive Development Technologies AB\n",
      "\n",
      "Pomodoro Technique Illustrated\n",
      "\n",
      "The Easy Way to Do More in Less Time\n",
      "\n",
      "Staffan Nöteberg\n",
      "\n",
      "Dallas, Texas\n",
      "\n",
      "\n",
      "\n",
      "Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in this book, and The Pragmatic Programmers, LLC was aware of a trademark claim, the designations have been printed in initial capital letters or in all capitals. The Pragmatic Starter Kit, The Pragmatic Programmer, Pragmatic Programming, Pragmatic Bookshelf and the linking g device are trademarks of The Pragmatic Programmers, LLC.\n",
      "\n",
      "Every precaution was taken in the preparation of this book. However, the publisher assumes no responsibility for errors or omissions, or for damages that may result from the use of information (including program listings) contained herein.\n",
      "\n",
      "Our Pragmatic courses, workshops, and other products can help you and your team create better software and have more fun. For more information, as well as the latest Pragmatic titles, please visit us at\n",
      "\n",
      "http://www.pragprog.com\n",
      "\n",
      "Copyright © 2009 Pragmatic Programmers, LLC.\n",
      "\n",
      "All rights reserved.\n",
      "\n",
      "No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form, or by any means, electronic, mechanical, photocopying, recording, or otherwise, without the prior consent of the publisher.\n",
      "\n",
      "Printed in Canada.\n",
      "\n",
      "ISBN-10: 1-934356-50-6\n",
      "\n",
      "ISBN-13: 978-1-934356-50-0\n",
      "\n",
      "Printed on acid-free paper.\n",
      "\n",
      "P1.0 printing, November 2009\n",
      "\n",
      "Version: 2009-11-30\n",
      "\n",
      "'Investment in knowledge pays the best interest.'\n",
      "\n",
      "-Benjamin Franklin\n",
      "\n",
      "Contents\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Who doesn't want to have a stress-free life? To have brilliant ideas? To improve? To enjoy free time? But how can we achieve these goals? Often frequent interruptions, overlapping activities, and impending deadlines prevent us from the best intentions. In fact, stressful factors like these are harmful to us; they trigger more stress, compulsive behavior, and discontinuity, and they reduce our consciousness, concentration, and clear-minded thinking. The mind ends up wandering forward and backward in time, looking for someone or something to blame for our imagined inability.\n",
      "\n",
      "The Pomodoro Technique that I've developed aims to make us stop, observe, regain awareness, and, in doing so, improve ourselves. Time, rather than being a stress-inducing factor, becomes our ally in keeping the mind 100 percent focused on the present with no stress and no strain. With my Pomodoro Technique, you'll learn to reach your goals by being strong, instead of exerting strength, and doing so with a smile.\n",
      "\n",
      "Staffan is a serious practitioner of the Pomodoro Technique. I've been working for many years with people and groups that want to improve their working processes and optimize their time through the Pomodoro Technique, and I can say that Staffan's work is proof of his ability and imagination. With this book, I am impressed by how Staffan has succeeded in visually representing the concepts of my technique; readers will appreciate and benefit from his expertise. The content is informative, easy to use, stimulating, and effective. A real treasure! Enjoy!\n",
      "\n",
      "Ring!\n",
      "\n",
      " Francesco Cirillo, 2009\n",
      "\n",
      "CEO at XPLabs SRL in Rome\n",
      "\n",
      "Creator of the Pomodoro Technique\n",
      "\n",
      "http://www.pomodorotechnique.com\n",
      "\n",
      "Foreword by Henrik Kniberg\n",
      "\n",
      "[Winding up the clock. Tick-tick-tick...]\n",
      "\n",
      "A couple of days each week I schedule 'slack' days where I catch up on emails, prepare for upcoming engagements, and do other desk work. The first thing I do is decide whether I need to be effective that day. Today I wanted to be effective, so I selected eight Pomodori worth of items from my Activity Inventory sheet and added them to today's To Do Today sheet.\n",
      "\n",
      "I used to think I could do 12 Pomodori in a day (that's 'only' six hours, after all), but reality has shown me that eight is more realistic, since the Pomodoro Technique counts only focused time. Reality is a great but sometimes harsh teacher. I used to think course preparation took two Pomodori. It turned out it took four every time-at least twice as long as I originally thought. That triggered me to find ways to do it more effectively, so now I'm down to three. The Pomodoro Technique has helped me plan better, get more stuff done with less stress, and have more time for family and hobbies.\n",
      "\n",
      "[Opening my blog to see what I wrote earlier about the Pomodoro Technique. Oh, look-a comment has come in! Wait! Don't read the comment now. I'm in a Pomodoro, right? I'm supposed to focus on writing the foreword. I'll get back to that comment later. Mark the internal interruption.]\n",
      "\n",
      "If my wife, Sia, sees that my egg timer is ticking, she knows not to interrupt me unnecessarily. She knows that the clock will ring within 12 to 13 minutes (on average), at which time I'll take a short break and go give her a hug, rewarding myself for finishing a Pomodoro and her for letting me.\n",
      "\n",
      "Does that sound silly? Well, yes, it feels a bit silly having a dumb egg timer dictating what you do. That's why I have to decide every day whether I want to be effective that day. The Pomodoro Technique really helps, though, so a slight feeling of silliness is usually worth it. In fact,\n",
      "\n",
      "sometimes a bit of silliness is a good way to remember that life isn't primarily about getting through your to-do lists.\n",
      "\n",
      "If I choose not to be effective that day, I leave the egg timer in the drawer. I'll have a relaxing day of procrastination, with no self-imposed discipline. Then at least I'm not kidding myself and...\n",
      "\n",
      "[BRRRRIIIIING. Pomodoro is over...let me just complete that sentence.]\n",
      "\n",
      " ...building up false expectations about getting things done.\n",
      "\n",
      "[X. No hug-Sia is gone today. Go play bass for a few minutes. Back again. So, should I continue writing or check that blog comment? No, I'm in a flow. Keep writing; the blog can wait. Winding up the clock. Ticktick-tick...]\n",
      "\n",
      "The Pomodoro Technique is similar to Agile methods such as Scrum and XP but at a 'micro' level. It feels sort of like being a single-person team doing 25-minute iterations. The main difference is that, in Agile methods, velocity usually means how much stuff gets done per iteration. In the Pomodoro Technique, velocity means how many Pomodori get done per day. You get lots of stuff done, not by focusing on getting stuff done but by focusing on focusing!\n",
      "\n",
      "Thanks, Staffan, for introducing the Pomodoro Technique to me! I'm glad to see that it is getting popular, because it's another useful tool for the process tool kit.\n",
      "\n",
      "Oh, and about this book: if you are perfectly disciplined and efficient already, this book might not help you. At the other extreme, if you are completely undisciplined, you might not even get through the book, much less succeed in following the rules of the Pomodoro Technique. But most likely you are like the rest of us and somewhere in the middle. In that case, I congratulate you! Not only will Staffan's book help you improve, but the personal anecdotes and great illustrations will make it a pleasant journey! Enjoy. :o)\n",
      "\n",
      "[Go back to polishing the text a bit...]\n",
      "\n",
      "[BRRRIIIIIIING. X. OK, good enough to send as a first draft to Staffan.]\n",
      "\n",
      " Henrik Kniberg, 2009\n",
      "\n",
      "Agile &amp; Lean process coach at Crisp in Stockholm\n",
      "\n",
      "Agile Alliance board member\n",
      "\n",
      "Author of Scrum and XP from the Trenches\n",
      "\n",
      "Chapter 1\n",
      "\n",
      "One Activity at a Time\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A Cucumber and an Artichoke Meet at a Bar\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "How was work today?\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Terrible. I didn't get anything done.\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "Why? Don't you have anything to do?\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "I do! I need to complete and deliver a new printing feature.\n",
      "\n",
      "I planned to get it done today.\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "But you didn't do that?\n",
      "\n",
      "Artichoke: No, I was constantly interrupted with requests to do other things.\n",
      "\n",
      "Cucumber: And were these requests more important than completing your printing feature?\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Actually, I don't know. I didn't compare.\n",
      "\n",
      "Cucumber: And you completed nothing-neither the printing feature nor the important requests?\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "How could I complete so many things in just one day?\n",
      "\n",
      "Cucumber: I didn't ask whether you completed many things; I asked whether you completed anything.\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "No, nothing was completed.\n",
      "\n",
      "Cucumber: I think you should try the Pomodoro Technique. You focus on a single activity for 25 minutes and then, after a short break, compare any new requests with the one you were working on. You see which one has the highest priority and then follow your new priorities.\n",
      "\n",
      "Artichoke: I'm ready to give anything a try!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Getting Started\n",
      "\n",
      "Getting thoughts out of your head is mandatory if you want to be able to stay focused.\n",
      "\n",
      "Throughout this book, I describe my own experiences with the Pomodoro Technique. You will learn all about how to implement the technique and along the way hear how it improved my own productivity. In addition, I include research on the human mind that explains why this technique works, some of the consequences of adopting the technique, and ways in which you may want to adapt or extend the technique.\n",
      "\n",
      "This first chapter begins with a few stories and a chance for you to experience the technique. I want you to try a hands-on exercise early to give you context for understanding the rest of the book. I end this chapter with an inventory of symptoms in my life that helped me understand I needed to better focus my time and efforts. You may recognize yourself in this catalog. My story begins with a bus ride.\n",
      "\n",
      "\n",
      "\n",
      "Bus Time\n",
      "\n",
      "I live in a suburb of Stockholm. Since I'm a consultant, I mostly work at a client's big office somewhere downtown. There's a bus stop just 100 meters from my house. Every morning starts exactly the same: I go to the bus stop and wait for the bus to come. When it arrives, I step on and sit down in my usual seat. The trip downtown takes about 25 minutes, and I always use it for reading a nonfiction book related to my work.\n",
      "\n",
      "During these 25 minutes, it's impossible to get a cup of coffee, to turn on the TV, to check my favorite Internet community, or to do anything else that suddenly turns up to be so dramatically important when I try to read that same book at home. Furthermore, I don't know the people traveling on my bus. I might occasionally say 'Hello' to a familiar face, but that's all, so there are no other distractions.\n",
      "\n",
      "My reading time on the bus is a timeboxed , single-activity, single-goal experience with amazing results. I never learn as much as during my morning bus trip. I focus 100 percent on the book and trust that the bus driver will tell me when it's time to stop reading.\n",
      "\n",
      "I can't ride a bus back and forth all day long just to achieve this level of productivity, and neither can you. Fortunately, we don't have to. The Pomodoro Technique will help you split up your day into little bus rides like this. You just decide where you are traveling on your mini bus ride, set the timer, and focus on your work.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Tomato-Shaped Timer\n",
      "\n",
      "Francesco Cirillo defined the Pomodoro Technique in 1992, but it all started in the late 1980s, during his first years at college. The breakthrough came out of frustration over his low productivity and unstructured studying-thanks to a red, round object that made a ringing noise: 'I made a bet with myself, and it was as helpful as it was humiliating. I asked myself, 'Can I study-really study-for 10 minutes?' I needed objective validation-a time tutor-and I found one in a kitchen timer shaped like a pomodoro (the Italian word for 'tomato'). In other words, I found my Pomodoro.' 1\n",
      "\n",
      "With the Pomodoro Technique, you can make time your friend, not your enemy. Instead of feeling anxiety about deadlines for this hour, this day, this week, or this month, you set a timer for 25 minutes and completely focus on the task at hand. When the timer rings at the end of the 25 minutes and you're still working, it does not mean that you have failed to finish. On the contrary. It is a round of applause for your completed timebox.\n",
      "\n",
      "In a nutshell, that's what the Pomodoro Technique is-decide on the tasks you will do that day, set a timer for 25 minutes, and then start the first one. You'll also have daily retrospectives, create daily commitments, handle interruptions, and estimate the effort put forth. This book will walk you through how I do the technique, including how to record your activities and pick the tasks that are the most important.\n",
      "\n",
      "1.\n",
      "\n",
      "The Pomodoro Technique\n",
      "\n",
      "[Cir]\n",
      "\n",
      "\n",
      "\n",
      "Try It Now: Timeboxed Activity\n",
      "\n",
      "It sounds easy, but let's try a timeboxed activity to get you into the swing of things. Get a piece of paper and a pen. Make a list of several activities that you might have been doing right now if you weren't busy reading this book. It could be a form to fill out for your child's school, something you wanted to look up on the Web, an email that you must write, or some other administrative task.\n",
      "\n",
      "Now, which one of the tasks that you wrote down is most important? Which one would you prefer to have completed? Bookmark this page, and follow these steps to work on that task:\n",
      "\n",
      " 1. Wind up your kitchen timer for 10 minutes.\n",
      "\n",
      " 2. Concentrate only on that particular task.\n",
      "\n",
      " 3. Stop immediately when the timer rings, even if the task is not completed.\n",
      "\n",
      " 4. Take a three-minute break, and then continue reading this book.\n",
      "\n",
      "How did it go? Were you able to avoid thinking about other things during those 10 minutes? How often did you look at the timer?\n",
      "\n",
      "When using the Pomodoro Technique, you typically wind up the timer for 25-minute timeboxes. If you want, you can try to read the rest of the book in exactly that way: read the book in 25-minute iterationscontrolled by your kitchen timer, of course-and then spend three minutes of relaxation in between the reading periods.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Max, My Father's Grandfather\n",
      "\n",
      "Max moved to Berlin in the early 20th century to start a company that manufactured clothes. The company thrived, and his guiding principle contributed to his success. He lived by the following maxim: 'You can't dance at two weddings with one rear end.' In other words, you should focus on one activity at a time.\n",
      "\n",
      "Think of Max's maxim and the Pomodoro Technique as a total denial of simultaneous capacity-an axiomatic boundary. How can you then be sure that what you're doing is the most important activity? Well, for me, first I analyze what really matters, in other words, what result I want to achieve. Then I start to focus on the activity that will lead me to this result. But just before I start to focus, I wind up a timer. It will wake me up later. And I can then reanalyze: is the activity that I focused on still the one that matters most?\n",
      "\n",
      "Max was definitively a forerunner when it comes to modern attention management techniques!\n",
      "\n",
      "Why the Pomodoro Technique?\n",
      "\n",
      "Let me tell you a little bit about why I started using the Pomodoro Technique so that you can assess whether it might work for you. Before implementing the technique, time just seemed to disappear, poof! Much of what I intended to get done during the day remained undone. When I looked more carefully, I was able to identify many symptoms that needed treating. In fact, I bet you are familiar with one or two of them. The following are the problems that can cause people to not get work done. The rest of the book describes how the good practices of the Pomodoro Technique can help you solve them.\n",
      "\n",
      "Excitement decreases when complexity is high. When tasks are complex, you can't complete them in a single 25-minute period, so you tend to procrastinate. Procrastination provides easy relief when problems are hard-but the problems don't go away. No matter how complex the task, the important thing is to keep starting. Wind up the clock, and within a half hour you will have accomplished something and get rewarded with a break.\n",
      "\n",
      "\n",
      "\n",
      "Procrastination increases when tasks are boring. If you don't complete activities, they won't give you any value. Of course, doing the last bit of cleaning up always feels boring. But don't think about how much you have left to do of this activity. Imagine instead how quickly you'll finish off one Pomodoro. After that, you'll be rewarded.\n",
      "\n",
      "The hard work is done, but the activities that matter are still not completed. With the Pomodoro Technique, you do your planning in the\n",
      "\n",
      "\n",
      "\n",
      "morning and commit to doing a small number of activities that day. Then you reassess your priorities before every Pomodoro , one single activity that is the outstandingly most important activity to complete. You will always be doing the thing that matters most and nothing else.\n",
      "\n",
      "The pressure builds before a deadline. Long hours and working weekends are never productive in the long run. If you are forced to put in more hours than what you're comfortable with, you won't be able to produce quality work for long. You can create a sustainable pace with the Pomodoro Technique rhythm by using short iterations of 25 minutes, by not skipping breaks, and by focusing on one activity at a time.\n",
      "\n",
      "\n",
      "\n",
      "The mental transition between work and breaks is too slow. Sometimes you can lose time during the day just getting started or just coming back from lunch. The time it takes to get back on task can add up before you know it. The Pomodoro Technique is gesture-oriented. Do wind up the clock. Do have a personal ring signal. Do write the To Do Today sheet as an easy-to-grasp reference. Conditioned reflexes are great tools.\n",
      "\n",
      "Mistakes get repeated over and over. To avoid the same errors day after day, the last three stages of the Pomodoro Technique are done at the end of every day. Record, Process, and Visualize are together the daily retrospective and the key points for adapting your personal process, which means you learn and improve every day. It also means you can start from the textbook version of the Pomodoro Technique and then\n",
      "\n",
      "\n",
      "\n",
      "adapt it to suit your individual work context as you get more familiar with your working habits.\n",
      "\n",
      "The effort that one task takes is underestimated. Activities become much clearer when they are broken down into smaller items. If you estimate that an activity will need more than seven Pomodori, you can break it down. Using Pomodori estimating, you can get immediate feedback on the pace at which you are working when you compare your Pomodori estimate with the actual number of completed Pomodori per activity. This is called quantity estimating .\n",
      "\n",
      "The scope of a task is underestimated. Do secondary tasks spin off from your activities while you work on them? No problem, with the Pomodoro Technique, you just add them to the Unplanned &amp; Urgent list and then intensify your efforts to complete the main activity. This is called quality estimating .\n",
      "\n",
      "Your mind is invaded by competing thoughts. Sometimes it's hard to focus on one activity because you get so many other great ideas all the time. Just write them down under Unplanned &amp; Urgent, and then intensify your efforts in completing the activity that you're working on. Getting competing thoughts out of your head is mandatory if you want to be able to stay focused.\n",
      "\n",
      "\n",
      "\n",
      "A complex and demanding methodology of working consumes your time. The Pomodoro Technique is so easy to use that even my preschool daughters understand it. You don't need fancy tools. You don't spend half the day completing process artifacts. You don't need a process\n",
      "\n",
      "\n",
      "\n",
      "coach on-site to explain the comprehensive terminology. And what's more, it's adaptable. You should change it every day to avoid doing unnecessary routines.\n",
      "\n",
      "You forget about the wholeness while in the flow. The brain needs time to stabilize memories, see patterns, and make conclusions. While practicing the Pomodoro Technique, you take a break every half hour. This gives your brain a chance to absorb what you saw during the last Pomodoro. When you come back, you can see the overall picture and will probably have at least three new ideas.\n",
      "\n",
      "Estimates are seen as promises. It's impossible to guess exactly how much time an exploratory or development activity will take. You can only make a best guess. The habit of seeing your guess as a promiseeither by yourself or by co-workers-creates unnecessary anxiety. To avoid this trap, the Pomodoro Technique counts only the Pomodoro. After 25 minutes of effort, you can focus on your activities even when you're close to a deadline. Nonetheless, you should provide regular updates on your progress for all stakeholders to see.\n",
      "\n",
      "The process is not based on facts. You collect process metrics during the day; this is called the Tracking stage. These can be used in the daily retrospective to improve your process tomorrow. It's up to you what you decide to track. It depends on your current work situation, but start off by counting your interruptions and completed Pomodori.\n",
      "\n",
      "\n",
      "\n",
      "Someone pushes work on you. When people push work on you that you really don't want to do, it's unlikely you'll enjoy doing it. The fight\n",
      "\n",
      "\n",
      "\n",
      "between 'have-to' and 'want-to' is known as the terrible two . 2 Here's a third option. In the Pomodoro Technique, you select a number of activities in the morning that you believe you can complete during the day. By actively pulling activities into your To Do Today sheet instead of getting them pushed on you, you increase your personal commitment.\n",
      "\n",
      "Perfectionism prevents action. Waiting until you have devised the perfect solution to something is merely a form of procrastination. Procrastination is not an option in the Pomodoro Technique. You just go ahead and get started on a Pomodoro, and you don't have to worry about being 'totally perfect.' Winding up the clock and putting in 25 minutes of effort will reward you with being able to write an X and then take a break.\n",
      "\n",
      "Fear of failure or criticism is a mental impediment. The Pomodoro Technique is your commitment, your process metrics, and your process. You don't have to share your version of it with anybody. Find out what works for you. The number of completed Pomodori a day is your tool for working more effectively, completing more, and having more fun while working. It can't be used by your boss to review your performance.\n",
      "\n",
      "Did you recognize any habits from your office? Most people do. And if all these words like Pomodori , stages , and sheets are gibberish to you, then prepare yourself by reading this book.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Self-Reflection on One Activity at a Time\n",
      "\n",
      " · What makes you stop an activity before you are done?\n",
      "\n",
      " · Where is the best place to read to help you really focus?\n",
      "\n",
      " · What do you do to avoid starting on a boring task?\n",
      "\n",
      " · What type of activities usually take more time than you initially estimated?\n",
      "\n",
      " · Are you bound to do administrative activities that steal time from more important activities?\n",
      "\n",
      "Chapter 2\n",
      "\n",
      "Context\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A Cucumber and an Artichoke Meet at the Zoo\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "Did you try the Pomodoro Technique?\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "I did. It was great. I got so much done.\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "That's great. I knew you'd like it.\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "I didn't think it would work for me, but it did.\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "Of course it did; it's the way you are wired.\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "What do you mean?\n",
      "\n",
      "Cucumber: You just need to understand a bit more about how your brain works. There are forces in your environment that fight against your nature.\n",
      "\n",
      "Artichoke: So, the Pomodoro Technique helps me get control over all of the stimuli I'm getting from my environment?\n",
      "\n",
      "Cucumber: Exactly. And not only that, but you can benefit from what you've inherited from reptiles.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Foundation\n",
      "\n",
      "The prospects for getting into the flow are much better when you are rested, seated, and focused than when you are just hanging around and waiting for inspiration.\n",
      "\n",
      "In this chapter we'll look at what scientists know about you, your brain, and how it works. Read the sections of this chapter as short independent stories, and don't pay too much attention to how you will use this information. Later you'll come to see that the simplicity of the Pomodoro Technique allows you to work with your nature instead of fighting against it.\n",
      "\n",
      "\n",
      "\n",
      "The Brain Stack\n",
      "\n",
      "I like to think of the brain as a four-layered stack: the brain stem, the limbic system, the cortex, and the frontal lobe. All four of these layers have their own special strengths and weaknesses. As long as you can tailor a suitable role for each of them, they make an invincible team.\n",
      "\n",
      "We inherited our brain stems from our reptile ancestors living hundreds of millions of years ago. The brain stem gives us reliable reflexes without having to instantly use our conscious awareness.\n",
      "\n",
      "The limbic system is shared with our fellow mammals. It helps us with our long-term memory and also carries reward signals further up. The main themes here are feelings and social connections.\n",
      "\n",
      "The cortex is more developed in the more intelligent animals like us humans. It makes us aware and conscious. We can see the 'big picture' and consequently make conclusions.\n",
      "\n",
      "The human frontal lobe makes us different. We can imagine and create new things. We can argue and understand other people's arguments. We can plan, cooperate, and change direction. 1\n",
      "\n",
      "\n",
      "\n",
      "Brain Performance\n",
      "\n",
      "The human brain is refueled every hour with 2 grams of glucose (comparable to a sugar cube), 150 liters of blood transports, and 3 liters of oxygen. Even though our brains are only 2 percent of our body weight, it consumes 20 percent of the oxygen. 2\n",
      "\n",
      "Our brains have something like 100 billion nerve cells. They are called neurons , and they talk to each other through 100 trillion synaptic connections. The possible number of thought tracks is the digit 1 followed by 800 zeroes! 3\n",
      "\n",
      "Matching overall human behavior will take about 100 million instructions per second (MIPS) of computer power. Deep Blue, the chess machine that played against world chess champion Garry Kasparov in 1997, used specialized chips to process chess moves at a speed equivalent to a 3 million MIPS computer. Extrapolating the increase in processing power of top-of-the-line computers, it has been forecast that computers suitable for humanlike robots will appear in the 2020s. 4\n",
      "\n",
      "\n",
      "\n",
      "Rhythm\n",
      "\n",
      "Our bodies feature many rhythms. They help and control us, from the day we are born until the day when we die. The electrical activity produced by our brains, the electrical activity of our heart, and our breathing cycles are just a few examples of our rhythms. The need for food and rest varies predictably. Even hormone levels follow a cyclic schedule.\n",
      "\n",
      "Social rhythms are also important. To many people, it is a disaster to miss the annual Christmas celebration or a family member's birthdaythis despite the fact that there will probably be many more celebrations and birthdays.\n",
      "\n",
      "Many of our basic rhythms are controlled by the brain stem. We inherited them from the primates that lived a long, long time ago. A regular, rhythmic life is optimal for our brains. Displaced rhythms upset our brains-traveling through time zones gives us jet lag. People living without rhythms tend to be disorientated and anxious. 5\n",
      "\n",
      "That doesn't mean everyone has the same rhythm. A long-distance runner has a different rhythm from a sprinter. It's not only a difference in endurance and explosiveness; it's the amount of time until the next break-the next point of rest. This is the break rhythm . 6\n",
      "\n",
      "5.\n",
      "\n",
      "Blink\n",
      "\n",
      "[Gla06]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Superstition or Focus Enablers?\n",
      "\n",
      "Adrian Mutu is a great football player. He has played for Juventus, Inter, and Chelsea and has also scored many goals for the Romanian national team. His own explanation for this success is as simple as it is revealing: 'Curses cannot touch me because I wear my underwear inside out.'\n",
      "\n",
      "Turk Wendell was a Major League Baseball pitcher for more than 10 years. He played for the Cubs, the Mets, the Phillies, and the Rockies. He had a total of no less than 515 strikeouts. Why did he hide in the dugout during games? It was because he was brushing his teeth between every inning.\n",
      "\n",
      "John Terry is the long-term skipper of Chelsea as well as the England national football team. He achieved individual awards like a position on the FIFA World Team, PFA Player of the Year, and UEFA Best Club Football Defender. Why? Ask him, and he will tell you how he always listens to the same Usher CD before every game, how he parks in the same lot before every game, and how he ties tape around his socks exactly three times before every game.\n",
      "\n",
      "Try to switch desks with your neighbor at the office. It will probably interfere with your concentration for the rest of the day. By always preparing with the same gestures and routines, the brain will selfconfigure into the best mode to solve a particular kind of task.\n",
      "\n",
      "\n",
      "\n",
      "Conditioned and Unconditioned Reflexes\n",
      "\n",
      "The 1904 Nobel Prize winner Ivan Pavlov made a remarkable discovery by coincidence. He noticed that the dogs in his laboratory tended to salivate when they saw the staff members who usually fed them.\n",
      "\n",
      "So, Pavlov started a long series of experiments in which he gave dogs various signals just before the presentation of food. It included electric shocks, whistles, metronomes, tuning forks, and also visual stimuli. And, as he expected, after a while the dogs started to salivate every time they received these signals.\n",
      "\n",
      "A reflex is a special signal that creates a specific action. The salivation after hearing a metronome was conditioned by the individual's experience. The dogs learned this reflex from training.\n",
      "\n",
      "The evolution of species has given us unconditioned reflexes, such as when we blink our eyes. Like conditioned reflexes, the unconditioned are associative. But they are congenital as opposed to the conditioned.\n",
      "\n",
      "Here's an example in my personal routine: since I brush my teeth every night before I go to bed, I am now sleepy already when I put toothpaste on the toothbrush. Similarly, when using the Pomodoro Technique, I have trained my brain to start to focus as soon as I wind up the kitchen timer and to drop the focus when it rings. Even the ticking sound now reinforces my concentration. These are conditioned reflexes.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Left Brain and Right Brain\n",
      "\n",
      "The cerebral cortex covers the two cerebral hemispheres, popularly known as the left brain and the right brain . If we unfolded and then flattened out our 3mm-thick cortexes, they would be the same size as a tabloid-sized sheet of paper.\n",
      "\n",
      "A deep groove separates the left brain from the right brain. Nobel Prize winner Roger Sperry discovered in the 1960s that the two hemispheres divide the major intellectual functions between them. Each hemisphere is dominant in certain activities, but they are both basically skilled in all areas. 7\n",
      "\n",
      "In our left brain, we tend to match what we see now with what we have experienced before. If a pattern seems rational, then we add it to our repertoire. When we speak, our left brain helps us produce a linear sequence of words that follows syntactic and semantic rules. And analyzing the past is also a typical left-brain task.\n",
      "\n",
      "Our right brains, on the other hand, are more often intuitive. The right brain understands metaphors, creates dreams, creates new combinations of ideas, and can sometimes give us a eureka moment-from logical disorder.\n",
      "\n",
      "\n",
      "\n",
      "Savant\n",
      "\n",
      "John Langdon Down coined the phrase idiot savant , which is French for 'knowing idiot,' in the 19th century. He studied people with low IQs but exceptional abilities in remembering details. 8 Here are a few examples:\n",
      "\n",
      " · Orlando was an ordinary guy until the age of 10 when he was hit by a baseball on the left side of his head. Since then, he can remember the weather conditions for each and every day that has passed.\n",
      "\n",
      " · Kim couldn't walk until the age of four. But he can now recall the complete text of more than 9,000 books.\n",
      "\n",
      " · Daniel could recite π from memory to more than 22,000 digits. 9\n",
      "\n",
      " · George and Charles -a pair of identical twins-needed only one second to work out the weekday of an arbitrary day within the past 80,000 years.\n",
      "\n",
      "Most of the savants either have left-brain damage or lack nerves that connect the two hemispheres. One theory is that the right hemisphere is trying to compensate for this. So, if I see the world through an abstraction filter woven from my own experiences, does that hide an unexpected and astonishing skill in memorizing and calculating things?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hyperactivity\n",
      "\n",
      "If you find it very hard to focus all your attention into one single activity for a prolonged period of time, then you have one of the typical symptoms of a hyperactive person. Other indicators could be that you easily become angry or happy or that you have impulsive behavior. It doesn't really matter whether it's nature or nurture that made you this way; the key point is, what can you do about it?\n",
      "\n",
      "One of the responsibilities of dopamine , a neurotransmitter, is to keep people alert. One theory, which could explain why some people are so often hyperactive, is that reduced dopamine production is compensated in the brain by increased production of adrenaline.\n",
      "\n",
      "For example, when I get really tired, I show similar symptoms as a hyperactive person. My energy resource has run dry, and I'm responding with a short span of attention. Therefore, I need to rest to get new energy. To be optimally attentive, I need small breaks every half hour and not more than a 40-hour/week work schedule. My experience has taught me that working at a sustainable pace is a prerequisite if I want to produce great results.\n",
      "\n",
      "\n",
      "\n",
      "Working Memory\n",
      "\n",
      "Can you remember a sequence of more than seven digits? Alan Baddeley created a theoretical construct in 1974 that describes the structures and processes in the brain that are used for temporarily storing and manipulating information.\n",
      "\n",
      "The Central Executive controls our cognitive processes and channels information further. It's our CEO. The Phonological Loop stores what we see by rehearsing it. Its limited nature makes short words easier to remember. The Visio-spatial Sketchpad keeps information about what we see. It helps us judge distance or imagine images. The Episodic Buffer is our generalist. It links across domains, creating integrated visual, spatial, verbal, and chronological units. It's a pretty good friend with our long-term memory as well.\n",
      "\n",
      "The working memory is limited. For example, I can't focus on two problems simultaneously. If I'm working on a task and my attention is suddenly caught by something else, then when I return to what I was doing, I waste time getting back up to speed with it. That's why changing context too often reduces results. The Pomodoro Technique helps you avoid having your limited working memory become a bottleneck. 10\n",
      "\n",
      "\n",
      "\n",
      "Association Machine\n",
      "\n",
      "Alan Baddeley showed in 1966 in his study of memory encoding that information is normally stored as sound in the short-term memory. 11 This is opposed to long-term memory where the information is normally stored semantically. Long-term memory is like a gigantic association machine.\n",
      "\n",
      "For instance, immediately recalling a group of words like cat, hat, fat, rat is much harder than immediately recalling little, small, tiny, modest . The latter words are acoustically different and are easier to distinguish for our working memory's Phonological Loop.\n",
      "\n",
      "Delaying the recall a day turns this truth upside down. With delayed recall, it's easier to remember words with dissimilar meaning. We encode our memories as associations. Visual techniques such as mind maps are an excellent tool for pinpointing associations, which are suitable for long-term memory.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Succession and Duration\n",
      "\n",
      "Think about two different aspects of time:\n",
      "\n",
      " · Time as duration : 'I've been shopping for two hours, and I'm still not done' or 'Oh, I need to catch a bus in 10 minutes' or 'How long will it take you to complete this activity?' This is time as a distance between two points-the start and end points.\n",
      "\n",
      " · Time as succession : Hen 1 laid egg 2. Egg 2 was hatched, and hen 3 was born. Hen 3 laid egg 4, and so on. These are nonoverlapping events. We don't know when they happened, but we know the sequence.\n",
      "\n",
      "Thinking, planning, and calculating future time as a duration is very unpredictable-at least when it comes to doing things that we haven't done before in the same way. For me and for most people, this property of unpredictability produces anxiety. Anxiety will most certainly lead to lower productivity and may even spoil the result. If we can treat our work effort as a chain of events, it will increase our productivity.\n",
      "\n",
      "\n",
      "\n",
      "Dreaming\n",
      "\n",
      "A small part of our brains replay our daily impressions while we're in rapid eye movement (REM) sleep. That part is called the hippocampus , and it's shaped like a seahorse. Memories are stabilized, copied, and filed. The least relevant of the emotional parts are removed. The brain also discovers new relationships between collections of memory. Offline time-when we don't think about a problem-gives us new logical insights. Our brain runs this logical thread in the background, not only while we're asleep but also in offline time when we're awake. Small, regular breaks will improve our deciphering capacity.\n",
      "\n",
      "Eugène Aserinsky and Nathaniel Kleitman discovered REM sleep in 1953. 12 That was 53 years after Sigmund Freud published his seminal work Die Traumdeutung . 13 REM sleep-in other words, dream sleep -appears three to five times every night, in periods of five to fifteen minutes, which is a significant part of our lives. In total, we will spend about five years in REM state if we live for 70 years. Non-REM sleep is characterized by slow electrical waves, but during REM sleep, we have similar electrical waves as in the awake state: low amplitude at a rapid pace. 14\n",
      "\n",
      "\n",
      "\n",
      "Absorbing\n",
      "\n",
      "Relational memory , which is the ability to generalize across existing personal knowledge, is highly affected by offline time such as REM sleep or even small (awake) breaks from problem solving. One type of relational learning is transitive inference. For example, we initially learn individual premises; if we hear 'Yellow is preferred to green' and 'Green is preferred to violet,' then without ever explicitly being taught the relationship of yellow to violet, we infer that yellow is preferred to violet.\n",
      "\n",
      "In 2007 Jeffrey M. Ellenbogen et al. tested separate groups of participants, each having achieved the same level of premise pair training, with varying offline times of 20 minutes, 12 hours, and 24 hours. Participants initially learned five premise pairs, such as 'Green is preferred to violet' and 'Violet is preferred to orange.' They were not informed of the hierarchical structure from which inferences could be made, as in 'Green is preferred to orange.'\n",
      "\n",
      "Participants tested after 20 minutes could not answer the inference questions. But those tested after 12 or 24 hours of offline time showed a significant improvement in this relational knowledge, irrespective of whether they had slept in between the training and testing. 15\n",
      "\n",
      "This explains why I sometimes come up with a solution after lunch or after a night of sleep!\n",
      "\n",
      "\n",
      "\n",
      "Food-and-Sleep Clock\n",
      "\n",
      "Skalman is a turtle-shaped Swedish cartoon character. His name can be literally translated as 'shell man.' The most interesting thing about him is not that he wears a yellow hat. It's not that he's a genius with technology, logic, and chemistry, and it's not that he invented almost everything from a space rocket to a self-walking wheelbarrow.\n",
      "\n",
      "The most remarkable thing about Skalman is his favorite invention: the Food-and-Sleep Clock. Whenever it rings, he must do what it says. Well, he has chosen this lifestyle himself. Either the clock tells him to eat or it tells him to sleep. It doesn't matter whether the monster is just about to devour Skalman and his best friends, Bamse and Lille Skutt. If the clock says it's time to sleep, then Skalman immediately goes to sleep. (Skalman made an exception back in 1982, though; Bamse and his wife, Brummelisa, had triplets, and Skalman skipped a meal.)\n",
      "\n",
      "This is strictly timeboxed and iterative behavior. He relies on the clock and as a consequence, in contrast to Bamse and Lille Skutt, he's always able to act logically, even under pressure. Recurring gestures and routines helps the brain adapt to the context.\n",
      "\n",
      "\n",
      "\n",
      "Flow\n",
      "\n",
      "The mental state, characterized by the following properties, is known as flow : clear goals, concentration and focus, a loss of the feeling of selfconsciousness, a distorted sense of time, direct and immediate feedback, balance between ability level and challenge, a sense of personal control, intrinsically rewarding, the merging of action and awareness. 16\n",
      "\n",
      "Flow is a state of creativity. Wouldn't it be highly effective to always have flow? No, it wouldn't. Flow is not compatible with overview. Creativity is not compatible with real control. For example, now and then I want to see the big picture and make strategic decisions about the activity that I will dedicate myself to during my next flow.\n",
      "\n",
      "Before I enter a flow period, I want to wind up a timer that later wakes me up so that I can temporarily put on my strategy hat and see the big picture. Then I'll go back to flow again-it's rhythm. Also, bear in mind that the prospects for getting into the flow are much better when you are rested, seated, and focused than when you are just hanging around and waiting for inspiration.\n",
      "\n",
      "\n",
      "\n",
      "Arousal\n",
      "\n",
      "Say I just started a new major activity such as a new project at work. There's a lot of information to gather and analyze, so I have to talk to many people. Even discovering who to talk to is a project in itself. The situation might appear to be chaotic, but in reality it's more like it's complex. Every piece of information is unique, and there's no redundancy. The setup time is long, even before I get started.\n",
      "\n",
      "As time goes by, I start to see patterns in what used to be a mess. The more I learn, the more interesting the task becomes. At some point, I even become totally excited about it. I can't stop thinking about and working on this activity.\n",
      "\n",
      "Then, when I know everything about it, it becomes more and more boring. The most important things are done, but I still have to put the finishing touches on it. I have to tidy up and make the result of the activity presentable. Once again, I have a long setup time before I get started.\n",
      "\n",
      "Arousal is a state of heightened physiological activity. I don't need the peaks, and I definitively don't want the troughs. I want to have a sustainable pace.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Procrastination\n",
      "\n",
      "Procrastination is a mechanism for coping with the anxiety associated with starting or completing any task or decision. Why do you procrastinate? Because you're lazy? No, even the worst procrastinator has motivation and energy for something-a weekend hobby, a relationship, or perhaps a community.\n",
      "\n",
      "There are three outstanding sources of procrastination:\n",
      "\n",
      " · That other people force you to do something against your will\n",
      "\n",
      " · Your own pressure for making a perfect performance\n",
      "\n",
      " · Fear of making mistakes or receiving criticism\n",
      "\n",
      "Let's say you are chronically late, and deadlines surprise you. You have insufficient time for relationships and recreation. Why? It's because procrastination feels like a reward. It gives you temporary relief from stress. Remember that stress comes from the inside, and the cure is to find a starting point for your project. 17\n",
      "\n",
      "\n",
      "\n",
      "Heroism and Guilt\n",
      "\n",
      "Overtime comes from a combination of heroism and guilt. A promise to deliver can create guilt.\n",
      "\n",
      "Let me tell you a story about undeserved hero status. Once I spent a whole night at the office in order to fix a bug. Before I went home at 4 a.m., I sent an email to the project leader, explaining that the bug was fixed. The next day, the very same project leader fired off emails to the whole division, including all the vice presidents in the company. I was given a hero's reception-not for fixing the bug but for the devotion of being at the office in the middle of the night.\n",
      "\n",
      "Let's make this clear from the beginning: working overtime is like shopping with a credit card. With it, you buy things you can't afford right now. In the end, you have to pay everything with real money anyway. And-I can't stress this strongly enough-you will have to pay interest for the service of spending more than you can afford right now. It's a short-sighted way of managing time, since the interest charge will slow you down in the future. In the long run, we will produce more if we have a sustainable pace. Our brains need regular recreation timeboth short breaks during the workday and long breaks every day from the whole office environment.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Regulatory Process\n",
      "\n",
      "I used to travel smoothly in traffic until the day that I got glasses. Now cars emerge from everywhere!\n",
      "\n",
      "A regulator maintains a desired level of a designated characteristic or quality. For example, a thermostat controls the flow of heat energy into or out of a system. Actions taken by the thermostat are based on the feedback from a sensor. The system adapts its behavior, depending on sensor measurements and the desired level. An aircraft is another example that relies on regulatory processes. The autopilot takes the plane long distances even though the weather conditions fluctuate.\n",
      "\n",
      "So, why shouldn't a methodology be adapted at tight intervals?\n",
      "\n",
      "Relevant metrics should be tracked seamlessly during focused work. Recurring sessions are earmarked for analyzing the tracked data and also for looking forward: how can we adapt our process to our current office environment, our current type of activities, our current tools, our current teammates, our current deadlines, and all our other current circumstances?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Thin Slicing\n",
      "\n",
      "Sometimes I recognize patterns and make conclusions that are based on imaginary facts. My gut feeling tells me to choose one option, but my decision gets corrupted when I over-analyze. I'm interpreting items that I don't understand-with a little help from what I like and dislike, the stereotypes I know, and the rumors that I've heard.\n",
      "\n",
      "Good decisions balance between awareness and intuition. To make a good decision, you need to simplify the question to consist of only the significant facts and the areas that you're competent enough to analyze. Thin slicing is when we think without thinking. We extract what is really important from a slight period of experience. The quality of these judgments comes from our experience, training, and knowledge-nothing else. 18\n",
      "\n",
      "It is a true challenge to set up a process that is simple, transparent, supportive, self-improving, and adaptive. The adaptation is based on facts that we track, and these metrics should not only tailor our process but also improve us. We will then be able to make split-second decisions to do the right activity, at the right time, and in the right way.\n",
      "\n",
      "\n",
      "\n",
      "Embrace Change\n",
      "\n",
      "Suppose that I've been working hard on a high-priority activity for more than a month. Then my boss turns up and tells me that this project is now useless, since the customer who ordered this feature has gone bankrupt. All the work that I've done is now wasted. How could I possibly have known this when I started a month ago? What did I learn? To give and get feedback more often. Then me, my boss, and the customer would have been able to predict this failure earlier.\n",
      "\n",
      "Change is an ever-present part of business. We can't always control it, but sometimes we can control the effect change has on us. Learning to embrace change will open windows of opportunity and eliminate stress.\n",
      "\n",
      "It is easy to mistakenly believe that late changes have to be unprofitable; the cost of change always increases exponentially. With this reasoning, we have to plan meticulously in advance to avoid major changes later. But with the good practices, we can often make the cost of change curve flatten: we have control over the activities we must do, and we often reassess by asking 'What is most important to complete?' in the long and short term.\n",
      "\n",
      "If we can force the cost of change to rise only slowly as time passes, we will act completely differently from what we will do if costs rise exponentially. We will make major impact decisions as late in the process as possible to defer the cost of making the decisions and to have the greatest possible chance that they will be right. 19\n",
      "\n",
      "\n",
      "\n",
      "Paradox of Choice\n",
      "\n",
      "Constantly choosing all the time between all possible alternatives creates anxiety and disturbs our focus. We have too many choices, too many decisions, and too little time to do what is really important. Choosing creates anxiety, and evaluating alternatives steals time and focus. 20\n",
      "\n",
      "Still, we want to do the most important thing all the time. When our environment changes in a way that affects the value of what we're currently doing, then we should want to embrace this change and perhaps change our priorities.\n",
      "\n",
      "We must limit the occasions where we're forced to sort or allocate priorities, in other words, choosing our current activity. But, we must have these occasions often enough to be able to react to change, in other words, switching between activities. We need a rhythm such as choosing-working-break, choosing-working-break, and so on. And we need to timebox the work slots so that we won't forget to leave the zone when all our attention is on our current activity.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Self-Reflection on Context\n",
      "\n",
      " · For how long can you focus on one mentally challenging activity?\n",
      "\n",
      " · What are the rhythms in your private and your professional life?\n",
      "\n",
      " · Do you lose when you play games that reward good memory?\n",
      "\n",
      " · When did you last procrastinate? Why?\n",
      "\n",
      " · How do you balance between intuitive and conscious decisions?\n",
      "\n",
      "Chapter 3\n",
      "\n",
      "Mechanics\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A Cucumber and an Artichoke Meet at the Library\n",
      "\n",
      "Cucumber: How are you progressing with the Pomodoro Technique?\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Things are going well. I've implemented the entire process.\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "How do you figure out which activities you should do?\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Most of the time I just know. I have a good memory.\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "And how do you notice that 25 minutes have passed?\n",
      "\n",
      "Artichoke: I remember the time I started my iteration. But sometimes, of course, I forget to stop.\n",
      "\n",
      "Cucumber: I don't think you have even implemented half the process. There is a more formal way to do the Pomodoro Technique-with kitchen timer, artifacts, and retrospective. It helps you to not forget anything and adapts your process for you every day.\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Is it complicated?\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "Not at all.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Learning the Technique\n",
      "\n",
      "It's simple. Choose the highest-priority activity on the To Do Today sheet, wind up the clock to 25 minutes, and start focusing on that activity-and only that one.\n",
      "\n",
      "Finally, you have made it to the core practices! This chapter will walk you through the stages, the artifacts, and the how-tos of the Pomodoro Technique. And as you come to the end of this chapter, you will have practiced a complete day including creating your Activity Inventory sheet and your To Do Today sheet.\n",
      "\n",
      "\n",
      "\n",
      "Stages\n",
      "\n",
      "Here's a quick look at a day with the Pomodoro Technique. These are the five stages that you'll follow.\n",
      "\n",
      " · Planning : You start the day by extracting the most important activities from your backlog-called the Activity Inventory-and writing them in a list on your To Do Today sheet. This is your commitment for the day.\n",
      "\n",
      " · Tracking : Once you've decided on your activities for the day, you wind up the timer for 25 minutes and then start in on the first one. During every 25-minute timebox-called a Pomodoro -you collect a small amount of process metrics. You may, for example, count the number of times that you get interrupted.\n",
      "\n",
      " · Recording : At the end of the day, you file your daily observations on the Records sheet. If you tracked the number of interruptions, then this number is saved here.\n",
      "\n",
      " · Processing : After recording, you convert the raw data into information. For example, you might calculate how many interruptions you get in an average 25-minute timebox.\n",
      "\n",
      " · Visualizing : Finally, you organize the information in a way that helps you see how to improve your process. This is basically a daily retrospective and when you acclimatize your working habits to your reality.\n",
      "\n",
      "Every day starts with the Planning stage and ends with Recording, Processing, and Visualizing stages. In between, there is an iterative loop of 25-minute Tracking sessions.\n",
      "\n",
      "\n",
      "\n",
      "Deming-Shewhart Cycle\n",
      "\n",
      "The American statistician William Edwards Deming, along with Andrew Walter Shewhart, invented PDCA (Plan-Do-Check-Act). It's an iterative problem-solving process that can be used to improve other processes, and it is based on the scientific method Hypothesis-ExperimentEvaluation, which represents Plan-Do-Check-Act.\n",
      "\n",
      " · Plan : You define your goals and the processes you need to deliver the expected results.\n",
      "\n",
      " · Do : You implement the new process.\n",
      "\n",
      " · Check : You measure the new process and compare the results to the expected outcome in order to find discrepancies.\n",
      "\n",
      " · Act : You analyze the discrepancies and try to understand the root cause of them.\n",
      "\n",
      "This particular type of planning, monitoring, measuring, and improving is also the core of the Pomodoro Technique-and it makes it a typical PDCA process. In the morning, decide which tasks to do and what to track. You then track events throughout the day in the form of Xs, apostrophes, dashes, and other symbols of your choice. At the end of the day, compare your tracked data with tracked data from recent days and the expectations that you had beforehand. Did you manage to complete the activities that you committed to on your To Do Today this morning? Then think about how you can improve the process for tomorrow. Repeat this cycle every day to improve your process iteratively. 1\n",
      "\n",
      "\n",
      "\n",
      "Tools\n",
      "\n",
      "Tools that are easy to use make a method less complicated. You can then concentrate on the real tasks and ignore the process mechanics. In the Pomodoro Technique, you merely need a pen or pencil, a kitchen timer, and three plain sheets of paper.\n",
      "\n",
      "You can use whatever type of timer works for your office circumstances. It could be a mechanical kitchen timer, a digital kitchen timer, an hourglass, a mobile phone possibly set to vibrate, or some software on your computer. The act of winding up a mechanical clock will tutor your conditioned reflexes. After a while, this behavior will support your rhythm.\n",
      "\n",
      "The three pieces of paper that you'll need for the Pomodoro Technique are the following:\n",
      "\n",
      " · The To Do Today sheet is a paper with today's date, your name, and a list of your activities planned for today. You create a new one every morning.\n",
      "\n",
      " · The Activity Inventory sheet holds your name and an unsorted list of your upcoming activities in the near future. You use the same Activity Inventory sheet day after day-adding new activities and crossing out completed ones.\n",
      "\n",
      " · The Records sheet is where you keep your sampled process metrics to be used for your process improvement. The same Records sheet is used day after day-in order to compare today's tracking with prior tracking.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bookmark this page, and go get a pen, a piece of paper, and your kitchen timer. Wind up the clock for five minutes. Write the header 'Activity Inventory' at the top of the sheet and start to think about activities you need to do at home. Write down all of them, whether they are important or not. Do not worry about order or priority. Don't write what to do. Write how things will be when the activity is completed. For example:\n",
      "\n",
      " 1. Living room tidy\n",
      "\n",
      " 2. Cookbook ordered\n",
      "\n",
      " 3. Thea emailed\n",
      "\n",
      "Don't use abbreviations or long descriptions-just write the important words. You don't need to write so that other people can understand. What matters is that you understand your own activity titles, even if you write them today and read them in a month. It's easier if the activity's first word is a topic area-something that may reappear on several titles-and the second word is more specifically what you want to achieve.\n",
      "\n",
      "Write the activities in a vertical list; one on each line. You can also write down when they must be done by, if you know that. If you're having a dinner party at 7 p.m. that night, you might write 'Living room tidy, 7 p.m.' And do not stop this exercise until the clock rings. Then continue to read this book.\n",
      "\n",
      "\n",
      "\n",
      "Finding the Starting Point\n",
      "\n",
      "Without the Pomodoro Technique, figuring out how to start each workday can be hard. You might feel like you have a billion things and you can't possibly do everything simultaneously. So, you never really start, and suddenly it's lunch time.\n",
      "\n",
      "The concept of 'do it now' is a good principle for reducing the total amount of work you have to do. For example, as an independent software development consultant, I manage my own company. This entails all my regular work and also mandatory paperwork such as keeping the books, sending invoices, and filling in tax forms. Postponing this work makes it even harder and more complicated because I can forget important details as time passes. To wait for inspiration is a bad thing.\n",
      "\n",
      "For me, keeping a long to-do list is not enough because it's a question of selectivity. At the moment I decide to do one activity, I'm also implicitly deciding not to do the other hundreds of possible activities residing in my backlog. 2\n",
      "\n",
      "First prioritizing and then focusing on the most important activity will make you feel safe and sound. Otherwise, your focus will constantly be disturbed by questions like 'Am I really doing the most important thing now?' At the start of my day, for instance, I first look at the whole backlog and pick the most important activity. Then I stick to it for a short timebox, before I reevaluate whether it's still the most important one. In my mind I replace 'I must finish' with 'Where can I start?' and I replace 'This project is so big and important' with 'I can take one small step'. 3\n",
      "\n",
      "\n",
      "\n",
      "Morning\n",
      "\n",
      "With the Pomodoro Technique, it's not hard to know where to start. Let me show you how I do it. At the start of my Pomodoro Technique day, I select the most important activities from the Activity Inventory sheet and write them in a list on my To Do Today sheet. This is the Planning stage.\n",
      "\n",
      "Then I choose the highest-priority activity on the To Do Today sheet, wind up the clock to 25 minutes, and start focusing on that activityand only that one. In my head, I say that this activity is on the 'now list.' This virtual list is actually binary. It holds either one or no activity.\n",
      "\n",
      "Being able to do something because you have decided to do it is the foundation of good time management. You need to see the big picture when you decide what to do; otherwise, you will be bogged down with trivia. Every morning with the Pomodoro Technique, you can see all the activities available and choose a selection from them. 4\n",
      "\n",
      "The Pomodoro Technique is very goal-oriented. You select only the number of activities that you realistically stand a chance of completing today. This is your commitment. If you manage to complete them all, you get rewarded with a mental trophy.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Get a pen, a piece of paper, your kitchen timer, and the Activity Inventory sheet that you just created. Wind up the clock to five minutes. Write the header 'To Do Today' at the top of the sheet, and start to think about activities you believe you'll do during the rest of this day.\n",
      "\n",
      "Look first at your Activity Inventory sheet. Consider whether there are items that you have not entered there. For example:\n",
      "\n",
      " · Soccer drive Edda\n",
      "\n",
      " · Dinner cook\n",
      "\n",
      " · Call sister\n",
      "\n",
      "Do not write too many things without thinking about whether it is a reasonable commitment to complete all activities. Do not stop this exercise until the clock rings. Then continue to read from here.\n",
      "\n",
      "How would you feel if you got all these activities done today? Selecting activities for To Do Today sheet also means that you refuse all activities that you don't write down on this list. Is it easier to focus on the selected activities now?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Commitment\n",
      "\n",
      "A traditional to-do list is not a commitment. It may feel safe to add every potential activity there, including things that other people push on you. The list is long, and the continuous inward flow ensures that all activities can't possibly ever be done. Without commitment, there is less motivation. It's not really an achievement to complete and remove one line from this inventory that is still enormous.\n",
      "\n",
      "Your To Do Today sheet is your timeboxed commitment. You shouldn't put activities there that you do not think you will do today. This sheet allows you to see a goal that is reasonable to reach that very day. By contrast, the Activity Inventory sheet is a more traditional to-do list. It's allowed to grow, and you can put nice-to-haves there, even if you're not sure that they will ever be a high enough priority to be executed.\n",
      "\n",
      "Let me stress that an Activity Inventory sheet is a good thing. But it's not enough. To develop motivation, you need to extract an activity set from the inventory that is adequate for a timebox that you can foresee. If you extract this set yourself and you believe that it's an attainable goal within the timebox, then you will be committed. One day is a foreseeable timebox length, and it's also long enough to complete activities in every commitment.\n",
      "\n",
      "Distinguishing between the Activity Inventory (a traditional to-do list) and the To Do Today (an extracted commitment) is a compulsory strategy for doing the right thing, getting started now, and putting optimum effort into your work. It gives you clear goals and personal control.\n",
      "\n",
      "\n",
      "\n",
      "Direct and Immediate Feedback\n",
      "\n",
      "When people go bowling, they have a clear goal. They must roll the ball down the lane with the objective of knocking down as many pins as possible. Every time they do this, they know immediately whether they did it well or not. How many pins is not interesting in itself, but it's interesting in a context. What will be the consequences now that I've knocked down six pins? How will that affect my next step? Humans get satisfaction when our investment of mental energy-our attentionresults in success. 5\n",
      "\n",
      "For example, when I'm working on an activity for days and weeks, I never get this type of direct and immediate feedback. Successes and failures will not be apparent until the big task is done. If the activity I'm doing has a deadline some days from now, it will be even worse. The uncertainty about whether I will manage to meet the deadline will give me anxiety.\n",
      "\n",
      "With the Pomodoro Technique, rating 25 minutes of effort as a success gives us immediate feedback.\n",
      "\n",
      "\n",
      "\n",
      "Prospective Memory\n",
      "\n",
      "To avoid focusing on less important activities, you need to remember to recurrently reassess which activity is most important. You also have to remember to frequently take breaks. And you must remember to now and then do a personal introspection to improve your abilities and to be aware of your weaknesses. For me, I support my prospective memory with a written plan, a solid process, and a ring signal that I have programmed.\n",
      "\n",
      "People's retrospective memory is concerned about the past. The complement is our prospective memories, which is what helps us remember to do something in the future. For example, we use our prospective memories when we remember to attend a meeting, remember to call someone, or more generally remember to perform any kind of activity that we, in the past, intended to do in the future.\n",
      "\n",
      "Aging decreases the activity in our frontal lobes and our production of prospective memories. This is a fact of life, but we can compensate by external stimuli, such as creating reminders for ourselves. Prospective memories are closely connected to our wills. For example, if we want things to happen in the future, we create long-term plans. And when those plans are accomplished, we're rewarded. 6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Now List\n",
      "\n",
      "In 1933 Hedwig von Restorff performed a set of memory experiments. Her conclusion was that a visually isolated item, in a list of otherwise similar items, would be better remembered. For example, if you read a shopping list with one item highlighted in azure blue, it's more likely that you'll remember the highlighted item than any of the others. This is now identified as the Von Restorff effect.\n",
      "\n",
      "The 'now list' is not another tangible artifact in the Pomodoro Technique process. It's my name for a concept-the thing that I'm giving my attention to right now. The cardinality of my 'now list' is binary. I focus on either one activity or zero activities. It can never be two, three, four, or any other number of activities. Before I wind up the clock, I choose one single activity. My challenge during a 25-minute Pomodoro is to not give another activity any attention.\n",
      "\n",
      "The Von Restorff effect tells us that we can provoke our memories to store things that we highlight. So, to apply that effect to my 'now list,' I explicitly write the current activity that I'm doing on a slip of paper and put it in front of me. Or, to apply the technique even more literally, I may use a highlighter felt-tip pen to mark the current activity on the To Do Today sheet and then later cross the item off with a black pen when I complete the activity.\n",
      "\n",
      "\n",
      "\n",
      "Break\n",
      "\n",
      "When your kitchen timer rings after 25 minutes, it means you have completed one Pomodoro. You should immediately mark an X next to the activity on your To Do Today sheet and then take a break. Your break period can vary, but I totally detach myself for three to five minutes from the activity and everything else that is mentally challenging. I might drink water or think about what to eat for dinner that night.\n",
      "\n",
      "My minimum criterion for a break is to stand and move at least two steps from my office chair. That usually washes away my thoughts about the previous Pomodoro and also does some good to my back and shoulders. Office people like us sit for far too long in monotonous positions.\n",
      "\n",
      "After your break, you should decide whether to continue with the same activity or switch to another one. A switch could be initiated either by a change in priorities or otherwise by the simple fact you have completed the last activity.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Detach\n",
      "\n",
      "During your breaks, you're not allowed to think about the previous Pomodoro or about the next Pomodoro. Don't make important phone calls or start writing important emails. Your brain needs to absorb the last 25 minutes of challenging thinking.\n",
      "\n",
      "If your stress system is never neutralized by mental recreation, you'll notice a number of symptoms. The thinking system in the brain stem is affected, as well as the senses of the limbic system and in the end your biological rhythms. For example, your sleep might be affected.\n",
      "\n",
      "At chronic stress levels, the capacity of your working memory and your ability to concentrate will fall. The joy of working will be transformed into anxiety-inspiration is altered to irritation. 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Set Break\n",
      "\n",
      "In addition to the breaks you take after each timeboxed iteration, the Pomodoro Technique includes a set break , which is bit longer than a regular break. For me, four Pomodori makes a Pomodoro set, so after every four Pomodori, I take my set break. Typically, a set break is 15 to 30 minutes of recreation. You can use this time to clean your desk, take a walk to the coffee machine, or browse your favorite community on the Web.\n",
      "\n",
      "You can even glance at your email inbox. But don't start to write any important replies. That task should be scheduled like all the other activities.\n",
      "\n",
      "Since people are naturally inquisitive, you might want small activities. You can see the goal already from the beginning, and you're more likely to see small activities as a commitment. If you intersperse short periods of work with breaks and rewards, it will give you the motivation to keep a sustainable pace through the day, and day after day.\n",
      "\n",
      "These breaks are an example of guilt-free play. During your break, you will know that you already have achieved goals during the recent Pomodoro. And what's more, you'll get insights throughout the day, even during a break. 8\n",
      "\n",
      "How to Take a Five-Minute Nap\n",
      "\n",
      "by Renzo Borgatti\n",
      "\n",
      "The Pomodoro Technique advocates a 25-minute slot of quality, focused work followed by a five-minute break. There are so many ways to spend the break time, but the main goal is to recharge the brain and allow for background processing of previously assimilated information. The consequence is that you shouldn't be tempted to spend the break reading emails, reading the news, making phone calls, or doing anything that generates additional pressure to the following Pomodoro.\n",
      "\n",
      "\n",
      "\n",
      "So, for example, looking at the previous picture, I can say that looking out the window requires less effort than talking or reading emails. At the extremes are sleeping and working. Focused, quality work is the goal of the Pomodoro, and a focused, quality relaxation is the goal of the break. I made the mistake of thinking that just looking at email subjects or news titles was relaxing enough for a break, but after comparing it with five minutes of deep relaxation, I changed my mind.\n",
      "\n",
      "Ideally, the goal of the break should be light sleep for five minutes. I don't know if it's possible to sleep for just five minutes, but you can train yourself to really relax. After a couple of weeks of self-inspection of the break and implementing some relaxing techniques, I can say I'm very happy with the results. I discovered that a good-quality five-minute break can immediately give me the energy to start the next Pomodoro with unexpected ease.\n",
      "\n",
      "The problem is how to quickly fall into deep relaxation. I had success with the following:\n",
      "\n",
      " 1. Find a comfy chair or, if you happen to have one at work, a couch.\n",
      "\n",
      " 2. As soon as the break starts, close your eyes and find the best relaxing position. Your neck, arms, and legs should be perfectly relaxed.\n",
      "\n",
      "\n",
      "\n",
      " 3. Think about a light scanner-a horizontal line of light starting from your head and going down to your feet very slowly. While the line of light touches all your muscles, concentrate on that single area and further relax whatever is there. Especially important are the eyes: carefully remove tenseness.\n",
      "\n",
      " 4. Think about a white giant rectangle gently floating around. If that disappears, no big deal. It's just a starter thought to get you distracted from thinking about the goal of the previous Pomodoro.\n",
      "\n",
      " 5. When the break is done, gently open your eyes, start the Pomodoro, focus, and go.\n",
      "\n",
      "You know that you're doing it correctly if those five minutes feel like ten. Every time I failed to properly relax, I paid the consequences after five to six Pomodori. Something I also tried was to sleep during the 20-minute or longer breaks. That was real sleep, but it was never too deep to be counterproductive. Needless to say, the long break is as important as the shorter ones for a good day of Pomodori.\n",
      "\n",
      "The technique works well in environments with low noise that are familiar (where people don't pay too much attention to you). I borrowed the idea to train the body to deeply sleep in short period of times from the polyphasic sleep model, a fascinating (and scary) way to sleep less. 9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Itinerary\n",
      "\n",
      "Free time is fuel for the working brain. I never spend five minutes extra on an activity when the clock rings. If I did, it would slow me down for the next Pomodoro. When the clock rings, I finish the word I'm writing and maybe put a memo tag on a scrap paper-just to remember the direction that my mind was going in.\n",
      "\n",
      "An itinerary sets limits. In the Planning stage, you have essentially designed your itinerary for today's journey. It doesn't mean that you can't consider new facts and replan. Schedule updates have to be done in a mind state of overview and not in the state of flow. Also, you can make more aware decisions when you're rested after a short break than immediately after a long period of deep concentration.\n",
      "\n",
      "For me, concrete goals motivate me to complete things. Selecting a realistic number of activities in the morning is my commitment. Trying to meet the commitment in small iterations-my Pomodoro-is my itinerary. Paying no heed to breaks will spoil everything including the commitment and motivation. Respecting the itinerary gives me unwavering productivity all the time.\n",
      "\n",
      "\n",
      "\n",
      "Activity Completed\n",
      "\n",
      "So, you're now completing Pomodoro after Pomodoro with breaks in between. You do this until you're done with each activity. Every time you have completed an activity, you cross out its title on the To Do Today sheet.\n",
      "\n",
      "Never switch activities in the middle of a Pomodoro. If you finish an activity halfway through a Pomodoro, spend the rest of the time overlearning. For example, if I finish early, I review what I have done, I repeat what I have learned, I see whether I can enhance my work, or I note new conclusions on paper-until the kitchen timer rings.\n",
      "\n",
      "Over-learning is when we continue to study or practice even after attaining proficiency. Malcolm Gladwell argues that this is necessary if we want to be really good at something: 'Once a musician has enough ability to get into a top music school, the thing that distinguishes one performer from another is how hard he or she works. That's it. And what's more, the people at the very top do not work very much harder or even harder than everyone else. They work much, much harder'. 10\n",
      "\n",
      "So, you're not allowed to impulsively switch activities in the middle of a Pomodoro. In fact, just having the option to switch in the middle is a recurring disturbance. You can't just stop in the middle of a Pomodoro and take a break either. Then you will lose the rhythm. And since the stopped Pomodoro was shorter, it will not be compatible-in terms of tracking-with other Pomodori.\n",
      "\n",
      "\n",
      "\n",
      "Abstract Time Unit\n",
      "\n",
      "When you don't use the Pomodoro Technique, you have many tasks to do, and your teammates or your client wants to know when you're going to be done with them. Often that creates problems.\n",
      "\n",
      "Predicting how long something will take and then realizing that your guess is counted as a promise creates anxiety. The problem is that you don't know exactly how long the activity will take, and you will be punished-by, for example, missing a client's expectations-if you don't live up to your prediction. Anxiety reduces motivation and productivity.\n",
      "\n",
      "As you know, in the Pomodoro Technique, a single Pomodoro is 25 minutes of effort. It's an indivisible and abstract unit-a timebox that disregards scope. Basically, you promise to spend 25 minutes of effort in the best possible way that you can. Whether the activity is completed during this iteration is not an issue while you're in the Pomodoro. The only thing that counts at that moment is that you do your best.\n",
      "\n",
      "How does this help you tell your client or your teammates when you'll be done with an activity? Actually, it doesn't. But, when you're in a Pomodoro, you should only care about the 25 minutes-not about when the whole activity is completed. This will help you focus.\n",
      "\n",
      "The clock is visible, and it counts down from 25 to 0. After 25 minutes, you get the intrinsic reward. You write an X and take a break with guiltfree playing. 11 This gives you the feeling, throughout the Pomodoro, that you're getting closer and closer to a reward.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Recording and Processing\n",
      "\n",
      "Once your working day is about to end, you enter the Recording stage. To start, copy your raw tracking data to your Records sheet.\n",
      "\n",
      "What you track depends on what you want to see. When you start with the Pomodoro Technique, it's good to just track the number of Pomodori that you have completed during the day. So, you make one column with today's date and one with today's number of completed Pomodori. You keep the Records sheet day after day, and each line represents one day.\n",
      "\n",
      "Tracking data can answer questions about the way you work and help you improve your productivity. Why do I put so much effort into my work and get so few activities done? It could be because I achieve so few 25-minute timeboxes. Is this bad or is this good? Perhaps it's good, because I'm supposed to spend a major amount of my time helping other people. Anyway, now I have an explanation that is empirical and correct.\n",
      "\n",
      "After the Recording stage comes the Processing stage. This is where you turn the abstract data into informative bits. For example, you could calculate the average amount of Pomodori spent on one activity. If this is a massive number, you will learn to break down the activities into smaller and more manageable pieces. Or you can measure the average time an activity stays on the Activity Inventory sheet before you handle it. The calculated numbers are also on the Records sheet.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Try It Now: Recording\n",
      "\n",
      "Get a pen, a piece of paper, your kitchen timer, and your calendar. Bookmark this page, and wind up the clock for three minutes. Write the header 'Records' at the top of the sheet.\n",
      "\n",
      "Look at how many items you have in the calendar that were scheduled for yesterday. Did you have any meetings in the office, for example? Enter yesterday's date and to the right of this the number of items that were on it yesterday. On the next row, do the same thing for the day before yesterday. Continue backward in time until the clock rings. Then return to reading this book.\n",
      "\n",
      "Look at your Records sheet, and figure out how many items you have on average per day. Is the sum greater or less than you thought? Should you have more or fewer entries in the calendar?\n",
      "\n",
      "\n",
      "\n",
      "Kaizen\n",
      "\n",
      "This Japanese word Kaizen means 'to change over time for that which is better.' But it's more commonly translated as 'continuous improvement.' Kaizen is an approach to work that focuses on incremental changes aiming for improvement. For example, maintaining cleanliness should be part of your daily work. And you should be answering these questions every day: Do I need smaller activities? Am I bothered by a recurring distraction? Do I have unnecessary overhead in my standardized activities and personal process?\n",
      "\n",
      "Doing the improvement work in a scientific way helps me remember to do it. First I standardize a practice. Then I measure the standardized practice in some way. Then I compare the metrics to the requirements. Maybe I have to change how I work to meet the requirements. Then I measure again, and finally I standardize the new, improved way of working.\n",
      "\n",
      "An example: I decide to work in 40-minute iterations. Then I measure how many iterations I can complete in a day. Finally, I compare that number with my expectations. I realize that I completed too few iterations, so I decide to decrease the iteration size to 25 minutes.\n",
      "\n",
      "Recording tracking data and analyzing it in the end of every day is selfcentered observation . Improving your practices, based on this analysis, is incremental upgrading . 12\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Right Here, Right Now\n",
      "\n",
      "The Pomodoro Technique has a regulatory nature. Every failure can be transformed into an insight. The retrospective at the end of the day is a deliberate time and place to customize your personal process. For me, I want suitable habits for my current type of activities, office, teammates, and every other possible boundary around me.\n",
      "\n",
      "The small iterations and the daily commitment shrink what's important for me at this moment. While in a Pomodoro, I don't care about the future or about the past. There's time to reflect on the big picture when you are planning in the morning, when allocating priorities before every Pomodoro, and when recording, processing, and visualizing at the end of the day.\n",
      "\n",
      "It's one daily commitment-one Pomodoro, one activity, and one goal.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Self-Reflection on Mechanics\n",
      "\n",
      " · Have you ever experienced drawbacks with a long to-do list?\n",
      "\n",
      " · What makes you feel mentally rewarded?\n",
      "\n",
      " · What type of timer is right for you?\n",
      "\n",
      " · How often do you take a break at work?\n",
      "\n",
      " · When do you scrutinize your work habits?\n",
      "\n",
      "Chapter 4\n",
      "\n",
      "Interruptions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A Cucumber and an Artichoke Speak on the Phone\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "Hi Artie! See you at the bar tonight?\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Maybe. It depends a bit on...\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "Depends on what?\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Wait a minute, I just got an email...\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "We can go there another day.\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Today is a great day to go to the bar, provided that...\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "What?\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Got my copy?\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "What copy?\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Sorry, Artie. I was talking to a guy here in the office. You\n",
      "\n",
      "asked me something?\n",
      "\n",
      "Cucumber: Nah, I just wondered what's the weather like over there. I'll call again tomorrow.\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "OK! Always nice to talk to you.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Keeping on Course\n",
      "\n",
      "Email and phone are easy-you can turn them off.\n",
      "\n",
      "You have now learned that you should neither switch activities nor stop an activity in the middle of a Pomodoro. What about interruptions outside of your control? You get a phone call, you remember some urgent activity you need to do, or you desperately need to visit the restroom. The Pomodoro Technique can't protect us from all kinds of interruptions. But in this chapter, you'll learn how you can handle them in a rational and effective way.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Attention Deficit Trait\n",
      "\n",
      "American research has shown that office employees are interrupted approximately every third minute and that people who work in front of a computer screen have an average of eight windows open simultaneously. Psychiatrist Edward Hallowell coined the term attention deficit trait (ADT) to describe this severe way of modern life. The brain is inundated with a torrent of information that never runs dry.\n",
      "\n",
      "Imagine you're starting your day sorting new emails when the IT department calls and urges you to fill in some form at once? Then a colleague steps in to your room and asks a work-related question, and at the same time someone calls you and needs information on the spot to prepare for a meeting?\n",
      "\n",
      "New findings in psychology and brain research point to the same bottleneck for both simultaneous capacity and handling distractions: the limitation of the working memory. Every distraction makes us lose the original information that we had on our brain workbench. And when attention is lost, it's expensive to find it again. 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Avoid LIFO and BPUF\n",
      "\n",
      "Interruptions are constant. New requests come up all the time. If you always try to meet the last-known need, all your long-term activities end up taking a backseat. You will never finish anything, since the last thing that came in will be first to come out-Last-In-First-Out (LIFO). Add to this the inability to complete activities, since you have to put so much energy into the process of constantly receiving and evaluating new information. This will lead you to a state of chronic mental overstimulation. It will increase your stress level and affect your results.\n",
      "\n",
      "One alternative to LIFO is to make a Big Plan Up Front (BPUF). You sit down on New Year's Eve with a pen and a piece of paper. First you write down what to do January 1. Then you go ahead and plan January 2, 3, 4, and so on, until December 31. After that, you buy loads and loads of tinned food, lock yourself in the basement, and start to work for one year. Is this denial of the change around you the way to go? Of course it's not!\n",
      "\n",
      "There's a third option out there. It embraces change, it gives you time to focus on your commitments, and it's iterative! It's described on the next page.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sustainable Pace\n",
      "\n",
      "Overview and control are the opposite of flow and deep creative-thinking processes. You can't see the big picture and focus on details at the same time. Your focus will benefit from a process where you minimize the points where you have to sort and allocate priorities. But you need to do both. And you also need recreation time on a regular basis in order to absorb and recharge. So, you have three mental states to switch between. You alternate between them, but what triggers the mental state change?\n",
      "\n",
      "I use three hats: the recreation hat, which is a jester's hat; the working hat, which transforms me into the lion who is 100 percent focused on hunting the antelope; and the strategy hat, which makes me feel like a king when I'm sorting and deciding what to do during the next work iteration.\n",
      "\n",
      "I come to work wearing the recreation hat. I put on the strategy hat and choose what activity to focus on. Then I put on the work hat, wind up the clock, and begin to focus. The clock rings after 25 minutes, which reminds me to put on the recreation hat. After a short break, I put on the strategy hat, and so on.\n",
      "\n",
      "This timebox schedule that interleaves to focus, to prioritize, and to rest gives me a sustainable pace.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Interruption Strategy\n",
      "\n",
      "Even though a Pomodoro lasts for only 25 minutes, distractions will still occur. They interrupt us when we're focused and force us to use expensive context switching. Still, the Pomodoro Technique is not a method that suits only lone wolves who can't be team players. Rather, it develops your personal skills in a collaborative environment.\n",
      "\n",
      "If you have a strategy for handling interruptions, it can cut down on the number of them. Sometimes this strategy will help you carry on with your initial activity, and sometimes it won't.\n",
      "\n",
      "Interruptions during a Pomodoro come in two flavors.\n",
      "\n",
      " · First, internal interruptions come from the inside. To elaborate, your instincts send signals to your mind. They tell you to do other things than the activity that you're focused on.\n",
      "\n",
      " · Second, external interruptions are initiated by someone else. Someone requests something and is waiting for your response. The Pomodoro Technique has strategies for both these types of interruptions that are covered in the following sections.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Internal Interruptions\n",
      "\n",
      "Even though 25 minutes is a small amount of time, it's almost impossible for the Pomodoro Technique practitioner to not think of other important tasks to do, especially in the beginning. But, things that can seem extremely necessary at the moment might not be so significant when everything is summarized.\n",
      "\n",
      "Here's a typical Pomodoro for me: I wind up the clock and start to focus on one activity. Then I feel hungry. Then I realize I need to make an important call. And I also was just thinking that I want to check my favorite Internet community. And I must read my email and also reply immediately. And that's not to mention the most recurring of all my instincts-refilling my coffee cup whenever it is empty.\n",
      "\n",
      "All these are instincts that come from inside. I start them, and they are clear symptoms of procrastination. Perhaps I think that my current activity is too complex or redundant. Perhaps I fear upcoming blame for not getting enough quality in my result. Or perhaps I don't want to start before I know exactly how this whole activity will end. Anyway, the interruptions impede me from completing my Pomodoro and writing an X. The next section covers how to get these interruptions under control.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accept, Record, and Continue\n",
      "\n",
      "Say during a Pomodoro that I suddenly realize that I need to call the box office. I promised my wife I would reserve tickets for Tchaikovsky's Nutcracker Suite . Do I grab the phone and call immediately? No! I follow my process-the Pomodoro Technique. If I want to cut down on interruptions, I first need to have the facts-the real facts. How many interruptions do I get during a day, and what type are they? My interruptions must be visible.\n",
      "\n",
      "Instead of calling the box office, I write 'call box office' on the bottom of my To Do Today sheet. Actually, I have a header in the middle of this sheet, just below my planned commitment, which says Unplanned &amp; Urgent. Calling the box office was totally unplanned, and it seems pretty urgent.\n",
      "\n",
      "Then I put a small apostrophe in the right margin of the line where my current activity is written on the To Do Today sheet. Writing the apostrophe is essentially tracking. It represents one internal interruption. At the end of the day I can count the number of apostrophes and reflect. This number is a cold hard fact of how many interruptions I had, not just a hectic feeling of forgetting something. Finally, I intensify my determination to finish the current Pomodoro.\n",
      "\n",
      "So, to reiterate, the strategy when you get an internal interruption is to first accept it, then record it, and then immediately continue with what you were doing before you got interrupted. As for coffee, I allow myself to drink as much coffee as I'd like during a Pomodoro, but I only make or get more coffee during a break between Pomodori.\n",
      "\n",
      "\n",
      "\n",
      "Inverting Dependency\n",
      "\n",
      "The best strategy for dealing with internal interruptions is to observe, accept, and plan or remove. As mentioned, never switch activities in the middle of a Pomodoro. The rule says, 'Once a Pomodoro begins, it has to ring.' For instance, following my instincts can appear urgent, but with a little distance I realize that the box office will still be ready to answer my call if I choose to do it during my next Pomodoro, instead of interrupting my current one.\n",
      "\n",
      "Without this strategy, the result of my activities depends on me not responding urgently to any instincts. Now, when I think of something I need to do, I write it down and can then drop it from my mind. I can later schedule it for the next Pomodoro or another day. In other words, the call to the box office can now be put into my schedule and dropped from my mind. What I suddenly and instinctively wanted to do now depends on my schedule. I have 'inverted' the dependency.\n",
      "\n",
      "By the way, if I know from the beginning that I won't call the box office today, I can write it directly on my Activity Inventory sheet and also add a small u (unplanned) and a deadline.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "It's Atomic\n",
      "\n",
      "Some internal interruptions can't be fought. If you really have to visit the restroom, then you have to do that. If you glance at the clock before leaving and see that there is 10 minutes left of your Pomodoro, can you then complete the final minutes when you're back from the bathroom? No way.\n",
      "\n",
      "A Pomodoro is atomic. It's indivisible. It's the smallest monetary unit in this process. If you leave the activity-temporarily or not-then you have to void this Pomodoro. It will not count, and accordingly you're not allowed to write the X. Instead, you wind up the clock 25 minutes and start a new Pomodoro, conceivably preceded by a small break if you think you need that.\n",
      "\n",
      "Why can't you build a Pomodoro from the sum of small time slots? Because then you will lose the whole idea of rhythm, and it will be too easy to fall for the temptation of interruption. Is it a failure to void a Pomodoro? No, it isn't. The number of completed Pomodori is not an all-purpose measure of competence. It's a measure of the 25-minute atoms of effort you have completed, and it's a fact that can be used to improve your process for tomorrow's work.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Constant Internal Interruptions\n",
      "\n",
      "New practitioners of the Pomodoro Technique are surprised of how few Pomodoro they actually complete in one day. Starting to cheat and ignoring that you had internal interruptions is not a solution. The tracking is there for you to improve yourself and your process. It's not to be shown to your boss during your annual salary negotiation.\n",
      "\n",
      "First, shrink the Pomodoro. Try 15, 10, or even 5 minutes. When you find that you are writing a respectable amount of Xs every day, you can start to expand to 20 and finally 25 minutes at a time. Remember, though, that Pomodori of different length are not compatible. Your tracking will be useless if one X means 5 minutes and another one means 25 minutes. You should go for the smaller Pomodoro for at least two weeks.\n",
      "\n",
      "You can also track how long it takes until your first internal interruption in every Pomodoro. Try to stay focused just a little bit longer than the last Pomodoro. This tracking can also give you an idea of how long your Pomodori should be in order to avoid being interrupted.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "External Interruptions\n",
      "\n",
      "In addition to internal interruptions, there are also external interruptions in our lives. These are things like when a colleague drops in and asks you a work-related question. Or when someone drops by to ask you a more social question like, 'What did you think about last night's episode of Curb Your Enthusiasm ?' Or perhaps an old friend calls and wants to talk about old times. Or maybe your project leader needs some estimates from you for her upstream report. Or, the most common one-your email client constantly beeps.\n",
      "\n",
      "All these interruptions tend to happen while you're trying to focus on an activity in the middle of a Pomodoro! But, if you live in the river, you should make friends with the crocodile. Without understanding the finer nuances, external interruptions will just be irritating.\n",
      "\n",
      "External interruptions have an interactive nature. Someone is waiting for your response. They are trying to prevent you from writing your X. So, you need a strategy to cut down on interruptions. Still, I can't stress enough that the Pomodoro Technique is not about refusing to help your team buddies. It's not a method for the last man left on the moon.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Protect the Pomodoro\n",
      "\n",
      "External interruptions compete for your attention. You need to protect your Pomodoro to focus on completing a commitment. This is the commitment of focusing on one activity for 25 minutes. But when someone comes by your cubicle to see how you liked the latest episode of your favorite TV show, the optimum strategy might not always be to say, 'Sorry, I'm in the middle of a Pomodoro. Can you come back later?'\n",
      "\n",
      "So, you must 'invert' the dependency between the Pomodoro and the interruptions. Email and phone are easy-you can turn them off. No one takes for granted that they will get an email reply in less than 25 minutes. Throwing a glance at new email headers (not reading the whole email and definitively not replying) every half an hour just before putting your strategy hat on is more than enough. And voicemail works the same way-as long as you do call back. Sending an email response and calling back should of course be scheduled as an activity and not be handled during a break.\n",
      "\n",
      "When you get a face-to-face request, ask how long you can defer the activity without reducing the value of your result. It might not matter to the colleague if you offer your response today or on Friday. Then you can suggest the latest possible time to him. Rescheduling interruptions in a later Pomodoro instead of handling them straightaway is a vast benefit.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Visualize and Then Intensify\n",
      "\n",
      "External interruptions must be visible. If you are to cut down on interruptions, you first need to have facts about the type and quantity of your interruptions.\n",
      "\n",
      "Each time you get an external interruption, reach for your To Do Today sheet, and write down a title for the activity that is requested by the interruption. Writing it down removes it from your mind, and it also assures you that it will be thought of in future planning. If you're sure you won't handle the new activity today, then you can write it directly on the Activity Inventory sheet and add a deadline and a U for unplanned. Or you can write it on the bottom of your To Do Today sheet, under the header Unplanned &amp; Urgent.\n",
      "\n",
      "Then write a dash (minus) to the right of the activity that you're currently working on.\n",
      "\n",
      "Finally, intensify your determination to finish the Pomodoro that you started before the interruption.\n",
      "\n",
      "\n",
      "\n",
      "Void\n",
      "\n",
      "One time, no less than eight people suddenly showed up at my desk. Apparently every single member of my team needed me for something:\n",
      "\n",
      "Team: Did you recently check in some new code to the archive?\n",
      "\n",
      "Staffan:\n",
      "\n",
      "It depends on how you define 'recently.'\n",
      "\n",
      "Team:\n",
      "\n",
      "Less than five minutes ago.\n",
      "\n",
      "Staffan:\n",
      "\n",
      "Eh, yes, I guess so. Is something wrong?\n",
      "\n",
      "Team: Right now, none of us can do anything. The code you checked in doesn't compile!\n",
      "\n",
      "This is not the time and place to inform them that I'm in the middle of a Pomodoro and that they are welcome to come back another day. Clearly it's better to void my Pomodoro than to not respond to their urgent and reasonable request. After helping them, I take a short break and then wind up the clock for 25 minutes and start to focus again.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "External Interruptions Strategy\n",
      "\n",
      "To summarize, you need to be aware of the type and amount of external interruptions. You do that by tracking every interruption with a dash and writing down the title of the requested activity.\n",
      "\n",
      "The strategy for handling external interruptions is a four-stage one:\n",
      "\n",
      " 1. Inform : 'I'm in the middle of something.'\n",
      "\n",
      " 2. Negotiate : 'Is it OK if I come back to you on Friday?'\n",
      "\n",
      " 3. Schedule : Write down the title of the activity and later plan it for a future Pomodoro.\n",
      "\n",
      " 4. Call back : Call back as you have promised; otherwise, you will not be entrusted with this responsibility anymore.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Interruption Notation\n",
      "\n",
      "Here are a few examples of the different notations that I use when I'm interrupted. Say I'm starting this day by selecting three activities: 'Prepare invoice,' 'Pick up Darwin Award,' and 'Write tax letter.' It's always important to correspond with the tax authority, so I take that activity first. After the first Pomodoro, I mark the first box with an X. After two Pomodori, I have completed my tax letter and continue by preparing my invoice. I put an X in the second box because I have completed a Pomodoro, and I cross out the tax letter item on the To Do Today sheet because it is complete. So, what are the 1, 2, and 3 in the picture here?\n",
      "\n",
      " 1. While preparing the invoice, I got an internal interruption: out of nowhere, I remembered that I must call Edda. I wrote it down on the Activity Inventory sheet to do it another day. Then I put an apostrophe next to the title for preparing invoices. This is tracking that will be summarized at the end of the day.\n",
      "\n",
      " 2. Bina called me while I was writing the tax letter. I told her that I'm in the middle of something and asked if I could call her later. This was an external interruption, so I added a dash next to the tax letter title. Once again, this is tracking that will be summarized at the end of the day.\n",
      "\n",
      " 3. After writing the dash, I also wrote 'Call Bina' under Unplanned &amp; Urgent.\n",
      "\n",
      "\n",
      "\n",
      "Aware of the Extent\n",
      "\n",
      "What's the number of U marked activities on my Activity Inventory sheet, and how many Unplanned &amp; Urgent activities do I have on my To Do Today sheet? By looking at these metrics, I can assess my planning capability.\n",
      "\n",
      "If I have many unplanned activities, it implies that I miss a lot when I plan in the morning. It's a qualitative error. I didn't expect all these things to be included. The next morning I can check more carefully to see whether I have included everything in my commitment.\n",
      "\n",
      "You're always allowed to reassess priorities between each Pomodoro, and there's nothing wrong with doing that. But if you always select some activities in the morning and always realize at the end of the day that the majority of activities actually completed during this day were outside this initial selection, then you will not feel the sense of commitment when you plan. If you don't feel committed, then you won't feel rewarded when you summarize your day. Chiefly, there will be no difference between the Activity Inventory and To Do Today sheets. Both will include a bunch of activities that you 'might' do today.\n",
      "\n",
      "\n",
      "\n",
      "Self-Reflection on Interruptions\n",
      "\n",
      " · What type of interruptions do you get at work?\n",
      "\n",
      " · How do you act when someone bothers you with an irrelevant question?\n",
      "\n",
      " · How do you store the new good ideas that suddenly pop up?\n",
      "\n",
      " · When do you have difficulty concentrating on only one activity?\n",
      "\n",
      " · Do you always come back later when you have promised to do that?\n",
      "\n",
      "Chapter 5\n",
      "\n",
      "Estimate\n",
      "\n",
      "\n",
      "\n",
      "A Cucumber and an Artichoke Meet at the Races\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Number five will win.\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "How do you know?\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "He won when I was here last week.\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "Today's race was run last week?\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Of course not. Each race is unique.\n",
      "\n",
      "Cucumber: So, he might not win today. He may get injured in the middle of the race or just have a bad day. The horses he is running against today may be faster.\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Sure.\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "So, you can't guarantee that number five will win.\n",
      "\n",
      "Artichoke: No, it's a guess. But based on what I know right now, it's my best guess.\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "But it's not a sure thing?\n",
      "\n",
      "Artichoke: No, just a guess based on empirical knowledge.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Measurements and Guesses\n",
      "\n",
      "Estimating future achievements is basically guessing. So, why not use the history and assume it will repeat itself?\n",
      "\n",
      "Estimation and measurement are two essential parts of the Pomodoro Technique. Without them, it is very difficult to make a daily plan. As you work on the technique, you will only improve if you take the time to look back. Then you'll see how what you thought you could do maps to what you actually achieved. Tracking and recording are the Pomodoro Technique ways of measurement. In this chapter, you'll learn how to estimate.\n",
      "\n",
      "\n",
      "\n",
      "Estimates on Your Activity Inventory Sheet\n",
      "\n",
      "Commitment is a vital part of the Pomodoro Technique. The To Do Today sheet is your daily commitment. The collection of items on your sheet must be reasonable and realistic. The quantity of activities is about what you normally complete in one day.\n",
      "\n",
      "If there is a recurring discrepancy between your estimates and the actual effort you have spent on an activity, then there can be two types of error sources:\n",
      "\n",
      " · Your estimate process or your estimate ability is not good enough.\n",
      "\n",
      " · New information appeared after you started this activity, and it changed the circumstances.\n",
      "\n",
      "You can't predict what new information or changing circumstances will show up. But you can empirically improve your skills in estimating with training and monitoring. Every morning, before you plan the day, try looking at each new activity on your Activity Inventory sheet and estimating the number of Pomodori it will take to complete it. You can also quickly glance at your existing estimates and see whether any need to be revised.\n",
      "\n",
      "For example, I write my estimates to the right of the activity items on the Activity Inventory sheet. Of course, I use a pencil so that I can change the numbers if I later gain knowledge that makes me want to modify my estimate.\n",
      "\n",
      "\n",
      "\n",
      "Wisdom of Crowds\n",
      "\n",
      "James Surowiecki writes about a crowd at a county fair that almost perfectly guessed the weight of an ox-on average. No one was as close to the right answer as the average of all guesses was. Why? If the crowd consists of partly experts in this subject matter, then each one will add some knowledge, and their errors will sum up to zero. 1\n",
      "\n",
      "Psychologist Edward Vul showed in an experiment that averaging multiple guesses from one person about a fact provides a better estimate than any single guess. He asked people various trivia questions. Without telling them, he repeated some of the questions a moment later. He noticed then that the average of a person's answers was more correct than any of the person's single guesses. Why? Cognition is based on statistical inference. For example, trying to answer a trivia question creates a range of possible values in my head. Every time I answer, I unconsciously just pick one of these values. The average of my answers is close to perfect, but their standard deviation is far from zero. 2\n",
      "\n",
      "So, crowds know more than any single person making a single guess, and you can become your own crowd by reassessing the situation. From time to time you make a new guess even though no new information has emerged.\n",
      "\n",
      "\n",
      "\n",
      "Activity Size\n",
      "\n",
      "The currency that is used for estimation is the Pomodoro. You must decide how many Pomodori you need to spend on an activity. An estimate of four means that you predict that you will spend four Pomodori on this activity before it's done. Since a Pomodoro is atomic, you can't use numbers like 1/2 or 2.2 Pomodori.\n",
      "\n",
      "When you estimate that an activity will take more than seven Pomodori, then it's too complex. You'll need to break down the activity. Each subactivity will have a separate line on the Activity Inventory sheet and a separate estimate. There is no precision in gigantic estimates.\n",
      "\n",
      "When you estimate that an activity will take less than one Pomodoro, then you can indicate that by putting a zero next to this activity. This does not tell you that it will take zero Pomodoro to complete, just that it takes less than one. These activities should occupy one row each on the Activity Inventory sheet.\n",
      "\n",
      "You can then combine a few of these less-than-one-Pomodoro activities when you select the activities for my To Do Today sheet. Put them on the same line, demonstrating that they are one unit today.\n",
      "\n",
      "\n",
      "\n",
      "Choose\n",
      "\n",
      "Planning based on estimates makes your commitment for the day more realistic, and as a result, your motivation will improve. Recording the number of completed Pomodoro every day gives you a good understanding of your Pomodoro velocity.\n",
      "\n",
      "When you select the most important activities from the Activity Inventory sheet in the morning, their estimated sum should not be higher than your empirically established velocity. You then write the activities on your To Do Today sheet and add as many small boxes to each line as you've estimated this activity to take.\n",
      "\n",
      "Every time the clock rings and you have finished a Pomodoro, write an X next to the current activity. This means that you have done 25 minutes of effort on this activity.\n",
      "\n",
      "Then three things can happen. You can run out of boxes before you have completed the whole activity. In that case, you underestimated this activity. Or, you can complete the whole activity before you run out of boxes, or you can complete it exactly on schedule. In the latter two cases, you cross out the activity on your To Do Today sheet.\n",
      "\n",
      "\n",
      "\n",
      "Quantitative Estimate Error\n",
      "\n",
      "What if you run out of boxes before you have completed the whole activity? You have estimated this activity to last for a number of Pomodori, and now you have used them all and still the activity is not done.\n",
      "\n",
      "Then you reestimate. You guess how many more Pomodori you will need to complete this activity. Then you write as many circles next to the boxes as you estimate your remaining Pomodori to be. Now you can continue to write an X in the circles every time the clock rings and you have finished a 25-minute Pomodoro.\n",
      "\n",
      "If you run out of circles as well, then you can make a third and final reestimate. This time, write the same amount of triangles, next to the circles, as the new delta.\n",
      "\n",
      "To not complete the activity, even in the limit of the third reestimate, is a failure. You need to analyze why you under-estimate again and again. Maybe your activities should be smaller and less complex. Estimating complex activities has low precision by nature.\n",
      "\n",
      "\n",
      "\n",
      "Yesterday's Weather\n",
      "\n",
      "As you know by now, one Pomodoro is a 25-minute iteration. Multiple Pomodori are surrounded by a larger daily iteration. Each day starts with planning and ends with the retrospective. In a Pomodoro, you should allow yourself to focus on only one activity. But how many activities should you plan for one day?\n",
      "\n",
      "A national weather service spent a gigantic amount of money on a new forecast system. All the emerging technology was included, and it had an accuracy rate of almost 70 percent. Then a clever person challenged the super machine with a much simpler algorithm. It was called Yesterday's Weather, and it said, 'Tomorrow will be like today.' Guess what? It had the same accuracy as the super machine.\n",
      "\n",
      "Estimating future achievements is basically guessing. So, why not use the history and assume it will repeat itself? If you can quantify your achievements every day, then this is presumably your velocity tomorrow as well. By measuring the number of Pomodoro every day, you can even fine-tune your velocity continuously with an average. 3\n",
      "\n",
      "\n",
      "\n",
      "Estimates on Records Sheet\n",
      "\n",
      "At the end of the day, make sure to record your tracking data on the Records sheet. You can then add two columns for each estimate.\n",
      "\n",
      "Here's the process: first, I write down the number of remaining boxes for each activity that I have completed today. Suppose that I overestimated one activity by three Pomodori (+3), I over-estimated two activities by two Pomodori each (+2 and +2), and I under-estimated one activity by two Pomodori (-2). All in all I have added five boxes too many on the To Do Today sheet. My total estimate error is +5.\n",
      "\n",
      "Second, I use the rightmost column for the number of reestimates. How many rows have circles (one reestimate) or circles and triangles (two reestimates)?\n",
      "\n",
      "The goal is to get zeroes in both these columns. But since one source of estimate errors is the changing world and new problem knowledge, there will always be discrepancies between estimates and actually used Pomodori. Anyway, the sum of the first column should be zero. New problem knowledge both reduces and increases activity density. In the long run, these errors should cancel each other out.\n",
      "\n",
      "\n",
      "\n",
      "Drum-Buffer-Rope\n",
      "\n",
      "The average number of Pomodori that you complete per day is your drum rhythm . The sum of all the estimates that you have on your Activity Inventory sheet is your buffer . The rope is how the constraint signals to the upstream process when to slow down or speed up the pace.\n",
      "\n",
      "The rope starts in your hand and ends in the buffer. With it, you can pull new activities to your workbench. When the buffer is filled, the rope becomes slacker. When the buffer is empty, the rope is stretched to its maximum. You pull, but there are no new activities.\n",
      "\n",
      "You need to avoid both feast and famine syndromes. The feast syndrome occurs when you have too many activities on your Activity Inventory sheet. Some activities may stay there a long time with no progress. An Activity Inventory sheet like this is inflexible, discouraging, and even misleading. The famine syndrome is the opposite-a short Activity Inventory sheet might on a creative day turn up to be empty. The theory of constraints teaches you how to trim the size of your Activity Inventory so that you can avoid both the feast and famine syndromes.\n",
      "\n",
      "If I complete nine Pomodori per day and I want to have ten days of buffer, then the sum of all my estimates in my Activity Inventory sheet should be approximately 90. If 90 is significantly exceeded, I must prioritize. If it drops far below 90, I can add more to the inventory. If I have a boss or a client filling up much of my Activity Inventory sheet, then I may need to ask for their help in prioritizing or adding to my list. 4\n",
      "\n",
      "\n",
      "\n",
      "Self-Reflection on Estimate\n",
      "\n",
      " · Do you normally keep up with what you have planned for your workday?\n",
      "\n",
      " · Do you ever change an estimate afterward?\n",
      "\n",
      " · Do you know how long it would take to do all the things that you have promised?\n",
      "\n",
      " · How big can your activity estimate be-in days, hours, minutes?\n",
      "\n",
      " · How small can your activity estimate be-in days, hours, minutes?\n",
      "\n",
      "Chapter 6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A Cucumber and an Artichoke Meet at the Movie Theater\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Hey Cu! What's up, man?\n",
      "\n",
      "Cucumber: Now that I've been doing the Pomodoro thing for a week, I've invented a derivate that I call PomdoroButt. 1\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Pomo...what?\n",
      "\n",
      "Cucumber: Well, I'm doing The Pomodoro Technique right out of the box...\n",
      "\n",
      " · ...But, my iterations are two hours. I really want to progress.\n",
      "\n",
      " · ...But, I have no problem with activities lasting for weeks as long as I feel that they are important. Breaking them down just gets me more things to keep in mind.\n",
      "\n",
      " · ...But, I don't use a kitchen timer. If I use timeboxing, I always have to stop while in the flow.\n",
      "\n",
      " · ...But, I don't retrospect at the end of the day. I know I've found the optimum process now.\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "Have you heard of the Dreyfus model of skills acquisition?\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "Do you mean Richard Dreyfuss from the Jaws movie?\n",
      "\n",
      "Artichoke: Never mind. However, I think you should stick to the original Pomodoro Technique-exactly as it is designed-for at least two weeks. Then, when you have really experienced pros and cons, you can start to make small adjustments.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tweaking\n",
      "\n",
      "To avoid interrupting my other activities every time I receive an email, I can set aside two Pomodori a day for responding.\n",
      "\n",
      "When you know how to implement Pomodoro Technique the usual way, you may feel tempted to tweak it. That's good. That's why you collect and track data, and that's why you have your daily retrospectives. But don't tweak anything until you've tried the usual way for at least two weeks. You need experience in order to know what is good and bad for you. In this chapter, I share tweaks that I have done.\n",
      "\n",
      "\n",
      "\n",
      "Simple Tools\n",
      "\n",
      "All you need to successfully implement the Pomodoro Technique is a timer, a pen, and several pieces of paper. Keep your tools simple.\n",
      "\n",
      "You should keep your Activity Inventory sheet short and up-to-date to give it a feeling of relevance. Mine has space for 25 to 30 activities.\n",
      "\n",
      "But, after a few weeks, my Activity Inventory sheet is a muddle. When there's no room left, I review every item that has not been completed. If the activity is not completed but still pertinent, then I copy the item to a fresh Activity Inventory sheet. After that, I can discard the old sheet.\n",
      "\n",
      "You might be tempted to use software to keep track of your activities. You can, but the simple tools are the best. With simple tools, you change the rules for your notation without delay and without involving a software maintenance team. The usual advantages of software, such as long-term storage, bulk distribution, heavy processing, or allowing for many simultaneous users, are not relevant to the Activity Inventory sheet.\n",
      "\n",
      "\n",
      "\n",
      "Index Cards\n",
      "\n",
      "The Activity Inventory sheet and the To Do Today sheet do not have to be two sheets of paper. One alternative is to keep each single activity on its own index card.\n",
      "\n",
      "I take a fresh card every time I realize that I must add a new activity to the Activity Inventory sheet. Then I write the title of the activity at the top of the card with a black felt-tip pen. Below the title I can add diagrams, phone numbers, web addresses, or anything else relevant to complete this activity. This kind of nice-to-have scribble should not be as conspicuous as the title, so I use a pencil for extra information. Finally, I add the same number of boxes as I estimate that it will takein Pomodoro time-to finish the activity.\n",
      "\n",
      "Therefore, my Activity Inventory sheet is now a deck of index cards. Each card describes one activity. I select an amount of cards every morning during the Planning stage. This selection is my daily commitment: my To Do Today deck. It's ordered with the most important activity card on top. You can try this approach or find one that works for you.\n",
      "\n",
      "\n",
      "\n",
      "Administrative Pomodoro\n",
      "\n",
      "The best way to start with the Pomodoro Technique is with a pure, simple approach. After a while, you might want to optimize the process for your personal circumstances. For example, I might usually spend a significant part of the day writing emails. To avoid interrupting my other activities every time I receive an email, I can set aside two Pomodori a day for responding. Here are some examples:\n",
      "\n",
      " · Advanced adaption example #1 : I always use my first Pomodoro in the morning for sending responses to emails received since I went home yesterday. And if 25 minutes isn't enough, I don't continue writing until lunch, destroying my whole schedule. Instead, since I know that I will have another dedicated email-writing Pomodoro every day immediately after lunch, I stop writing emails after 25 minutes.\n",
      "\n",
      " · Advanced adaption example #2 : I usually drop in at the office about 45 minutes before the morning meeting. That's when I can customize a 40-minute Pomodoro for things I need to do every morning. Keeping a checklist for these morning activities guarantees that I won't just hang around without any goal. Unfortunately, this 40-minute Pomodoro is not comparable to Pomodori of other sizes. If all the other Pomodori last for 25 minutes, then I'm not counting the 40-minute administrative Pomodoro in the Record stage.\n",
      "\n",
      "\n",
      "\n",
      "Sound and Shape\n",
      "\n",
      "When I started to practice the Pomodoro Technique, I was convinced that the clock ticking would disturb me. So, I put the clock on a piece of cloth to reduce the resonance. After practicing the Pomodoro Technique for a few months, I couldn't concentrate without the ticking. I had trained a reflex that enabled me to focus.\n",
      "\n",
      "One clock must not control more than one person, pair, or chat group. Do not synchronize the Pomodori in a diversified group. What happens otherwise if only one person wants to void his Pomodoro?\n",
      "\n",
      "Various clock properties suit different office environments. I have found that I benefit from sounds, gestures, and visibility. For example, winding up the clock gives me a clear signal and a visible front for my rhythm and conditioned reflexes. It's also an advantage to me if my clock counts down, since the time left is much more interesting than the time that is gone. The environment decides whether a mechanical kitchen timer, a digital timer, an hourglass, a mobile phone, or computer software is most proper. But when you keep your clock visible on the desk, your teammates become aware of your technique and subconsciously more inclined to respect it. Try to find a personal design. Unfortunately, the shape of a tomato doesn't give any advantage, but red is a color that most people can't ignore!\n",
      "\n",
      "\n",
      "\n",
      "Length of Pomodori\n",
      "\n",
      "Is the number 25 magic? Is there any research that says that a 25minute focused session is the optimum? Not as far as I know. I'm even pretty sure that it's impossible to find a worldwide optimum session time. It depends on who you are. It depends on your type of activities. It depends on who you work with and what responsibilities you have to them. It depends on if you're tired, or if you're in good spirits on a particular day.\n",
      "\n",
      "Short iterations are more often completed. If you are constantly interrupted, it indicates that you should shorten your Pomodoro length. It's easier to stay focused all the way through when you have smaller iterations. If the iterations are longer, there are fewer breaks, but on the other hand, the breaks tend to be longer as well.\n",
      "\n",
      "Every Pomodoro Technique practitioner has the option to experiment and find her own optimum. But I urge you to start with 25 minutes and stick to the same length for at least two weeks. Changing the length all the time destroys your rhythm.\n",
      "\n",
      "\n",
      "\n",
      "Length of Breaks\n",
      "\n",
      "The default schedule in the Pomodoro Technique-three small breaks followed by one longer break-is not mandatory. It depends on what you do and who you are, including where you work and how you feel. Complex problems may demand longer breaks. If you're tired one day, that can also be a reason to lengthen the breaks.\n",
      "\n",
      "However, if breaks are too long, then you will lose the rhythm. Every Pomodoro start will feel like starting on a new activity. If you skip breaks, on the other hand, then quality will fall. You will get an attention deficit that you must pay for later the same day. Furthermore, if you randomly mix long and short breaks, you will not feel motivated to wind up the clock. The setup time will increase, since it's all about biological rhythms.\n",
      "\n",
      "Should the breaks be timeboxed? Will this stop drifting? No, because you need to be mentally ready when you start your next Pomodoro. There is no point in jumping into an activity if you're still daydreaming. A new Pomodoro Technique practitioner who has problems coming back after breaks might timebox them for a week. In the longer run, you should rely on rhythm.\n",
      "\n",
      "\n",
      "\n",
      "Progress Sheet\n",
      "\n",
      "When I see the same activity title recurrently show up on my To Do Today sheet without being done, then I do a variant of the Activity Inventory sheet, which I call the Progress sheet.\n",
      "\n",
      "I take an A4 sheet with 5x5 mm squares and type the name of the month and my name in the upper-left corner. Then I write, along the short side of the paper, this month's days from right to left (31, 30, 29, and so on, down to 1), with one number in each box. That gives me about a 5 cm margin on the left side. I write all the known activities in that margin, one activity per line. My Progress sheet then replaces the Activity Inventory sheet forever. You can see the progress sheet as an alternative implementation of the concept Activity Inventory. What's the Records sheet for then? The Records sheet is for tracking whatever I'm tracking, such as the number of phone calls.\n",
      "\n",
      "Every morning when I select activities for my To Do Today sheet, I'll also draw a circle in the square where the column with today's date meets the row with the activity that I have chosen. At the end of the day when I do my daily retrospective, I draw a star in every ring where the activity was completed. If I have many empty circles, it means that I often overestimate my daily commitment. And activities with many starless circles probably need to be broken down.\n",
      "\n",
      "\n",
      "\n",
      "Priority Tournament\n",
      "\n",
      "When I'm not sure what activities to choose for my To Do Today sheet, then I do a priority tournament. First, I write down the titles of all the activities that are candidates-one activity title per scrap of paper. I then put all the scraps in an unprioritized pile. After that, I take the top two and compare. Which one would I rather have done today if I can do only one? Whichever loses goes to the loser pile to the left of the unprioritized pile, and the winner stays in my hand.\n",
      "\n",
      "Then I take the next scrap from the unprioritized pile and compare it with the winner from the first game. I will continue like that, game after game. The winner remains in my hand, and the loser goes to the loser pile. I have a winner of the whole tournament when the unprioritized pile is empty. The highest prioritized activity is in my hand.\n",
      "\n",
      "But I can complete more than one activity per day. Therefore, I make a new tournament with the loser pile. Actually, I do as many tournaments as the number of activities that I estimate that I can complete today. I write all the winners on my To Do Today sheet until the amount is a realistic commitment for today.\n",
      "\n",
      "This process may sound trivial, but it is a lot easier to compare the priority of just two activities than to extract a prioritized group from a long list. I often get a different result than if I just look at the Activity Inventory sheet and choose activities freely. Again, this is because of our limited working memory. 2\n",
      "\n",
      "\n",
      "\n",
      "Inbox to Zero\n",
      "\n",
      "I never accidentally glance through my email inbox in the middle of a Pomodoro dedicated to some other activity. When, however, I have planned to read and act on my emails, then I apply the 'Inbox to Zero' principle. David Allen describes it as a three-step process: 3\n",
      "\n",
      " 1. Start from the top.\n",
      "\n",
      " 2. Take one thing at a time.\n",
      "\n",
      " 3. Never put something back into the inbox.\n",
      "\n",
      "I think about the next action for every new email. If it takes a minute or less to answer, then I'll do it immediately. If I can delegate it further, then I delegate it right now. When I know it's an activity that I will do but that takes a little longer, then I don't start it now. I write the item on my Activity Inventory sheet, move the email to an archive folder, and mentally drop the activity.\n",
      "\n",
      "Activities that I can't do because I need information from other people are similar to the delegated activities. I'll send the request to that person and then move the email to an archive folder.\n",
      "\n",
      "I also always think about whether I will benefit from this email in the future; otherwise, I delete it immediately. Loads of emails that will never lead to any action creates information overflow. It hides the emails that are new, that are important, and that really require action.\n",
      "\n",
      "\n",
      "\n",
      "Repeated Reviews\n",
      "\n",
      "Your memory capacity can be improved. According to Tony Buzan, 'A general guideline here is to review shortly after the learning period or daily event has occurred, to review one day later, to review a third time one week later, to review a fourth time one month later, and to review a fifth time three to six months later.'\n",
      "\n",
      "Every time you think of something, the resistance to connecting these memories in your brain decreases. It's like clearing a path through a forest. The more times you walk this path, the fewer impediments there will be.\n",
      "\n",
      "Thoughts become famous inside your head. And just like pop stars, publicity increases the chances of getting more publicity. By repeating an important conclusion, you can increase the probability that you will recall it at a proper time. 4\n",
      "\n",
      "\n",
      "\n",
      "Daily Mind Map\n",
      "\n",
      "While using the Pomodoro Technique, you need to reserve some time at the end of the day for review. You don't need to spend an entire Pomodoro on reviewing what you have done that day and on preparing for the next day. When you have one Pomodoro's worth or less, you can wind up your kitchen timer for whatever time is left, such as 10 minutes. Use the back side of the To Do Today sheet, and rotate it 90 degrees so it is in landscape orientation. Now you're ready to explore your mind in order to grasp at the most usable knowledge concluded today.\n",
      "\n",
      "This is how I do the next step: in the middle of the paper, I draw a picture of my daily subject. What's my daily subject? What have I been communicating most with colleagues today? What's the keyword of the activity with most Xs drawn today? What was the subject of the meetings I went to today? What tool did I use in a new way today? Those are a lot of questions with possibly many answers. I have to choose one single thing as my daily subject. I pick the first one that pops up in mind and that feels outstanding in some aspect.\n",
      "\n",
      "Then I draw an ordinary mind map with colored branches, small icons, and free associations. When the timer rings, I just put the To Do Today sheet with my daily mind map into my pile of past daily mind maps. Once a month I will end the work day with a review of the previous month's pile of mind maps to repeat what I've learned.\n",
      "\n",
      "\n",
      "\n",
      "Prescriptive and Adaptive\n",
      "\n",
      "There are many time management processes, and to some extent they recommend the same techniques, tools, and philosophies. 5 They help us optimize how we spend our time-or more correctly, how we spend our attention-by planning, goal setting, monitoring, recapping, and prioritizing. The Pomodoro Technique may not add entirely new tools or ideas. So, what distinguishes Pomodoro Technique then?\n",
      "\n",
      "The Pomodoro Technique is prescriptive. Having simple and concrete best practices makes it easy to get started. It could mean that it does not suit all people or contexts. But the Pomodoro Technique has adaptiveness built in.\n",
      "\n",
      "The Pomodoro Technique gets us to focus on execution. It could mean that we ignore Systems Thinking [Sen90] and holism. That's why it's often beneficial to let processes like Scrum [Sch04], XP [Bec00], or GTD enclose the Pomodoro Technique.\n",
      "\n",
      "The Pomodoro Technique has a few simple artifacts. Three blank sheets and a kitchen timer is all that you need to get started. The simplicity also gives you flexibility. Adjusting a computer-based tool can be unattainable.\n",
      "\n",
      "\n",
      "\n",
      "Ring Disquiet\n",
      "\n",
      "Is the Pomodoro Technique suitable for everybody? Maybe not. But I recommend you try it before you say 'no thanks.' There are some types of people who I think would be better off with Pomodoro Technique than they might first think.\n",
      "\n",
      "For instance, a person who wants to be inspired before he starts anything might refuse to use the Pomodoro Technique. Nonetheless, he's the person who needs a process like this most. In the morning selection, he gains a commitment instead of all these messy threads circling around in his head. Furthermore, when he allocates priority in his To Do Today sheet and chooses just one activity there-the most significant one-his arousal will be amplified even more. And if that is not enough, the clock-winding gesture will light his fire. These are the mechanics to get the inspiration that he was waiting for.\n",
      "\n",
      "A person who wants to turbo-drive when he's in the zone might also turn down the Pomodoro Technique. But he would be helped by this kind of process, too. Concentrating fully on an activity is not equivalent to being fully productive. He will easily be bogged down in trivia. He spends hour after hour with some problem that seems extremely important to him. Taking a step back every half hour will show him the big picture. And small regular breaks will give him the sustainable pace as a replacement for struggling with increasing amounts of internal distractions.\n",
      "\n",
      "\n",
      "\n",
      "Self-Reflection on Adapt\n",
      "\n",
      " · For what type of administrative activities do you use a pen and paper?\n",
      "\n",
      " · At what time of the workday do you make phone calls or send emails?\n",
      "\n",
      " · How long is your normal break?\n",
      "\n",
      " · When do you concentrate for too long without taking a break?\n",
      "\n",
      " · How do you use index cards in your work?\n",
      "\n",
      "Chapter 7\n",
      "\n",
      "Team\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A Cucumber and an Artichoke Meet in the Conference Room\n",
      "\n",
      "Artichoke:\n",
      "\n",
      "I tried the Pomodoro Technique yesterday at a meeting.\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "What happened?\n",
      "\n",
      "Artichoke: I wound up my kitchen timer to 25 minutes, put the clock in front of me, and then just closed my eyes.\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "What happened when someone asked you a question?\n",
      "\n",
      "Artichoke: I just said, 'I can't answer now; can't you see I'm in the middle of a Pomodoro?'\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "How did it go?\n",
      "\n",
      "Artichoke: I don't know. When the clock rang, I stepped up to the whiteboard, drew a mammoth X, and then left the room.\n",
      "\n",
      "Cucumber:\n",
      "\n",
      "You didn't really do that, did you?\n",
      "\n",
      "Artichoke: No. Honestly, I couldn't see how to apply the technique, so we just had an ordinary meeting.\n",
      "\n",
      "Cucumber: Meetings can be ideal for 25-minute, kitchen-timer-con- trolled, time-boxed iterations.\n",
      "\n",
      "Artichoke: But what about those situations where the Pomodoro Tech- nique doesn't apply?\n",
      "\n",
      "Cucumber: They are less frequent than you might expect. But when you can't apply the Pomodoro Technique, it's still possible to track those activities if you want.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Working with Others\n",
      "\n",
      "To promote communication within the team, you can decide to not count team-member-to-team-member questions as external interruptions.\n",
      "\n",
      "So far in this book we've looked at the Pomodoro Technique as a personal time management process. But since it's adaptive by nature, it's not limited to isolated individuals. In this chapter, you will see that Pomodoro Technique can also be used in a collaborative environmentfor example, in pair work, in meetings, and in teams.\n",
      "\n",
      "\n",
      "\n",
      "Timeboxed Meetings\n",
      "\n",
      "Meetings are one of the most perfect places for the Pomodoro Technique. In this section I'll explain how to apply the technique. On a whiteboard, list all activities or all the desired outcomes of the meeting. Then prioritize them with 1, 2, 3, and so on. Next, wind up the clock to 25 minutes and start focusing-as a group-on the first-priority activity.\n",
      "\n",
      "When the clock rings, stop immediately. As you know, you always have short breaks between Pomodori. Perhaps someone wants to visit the restroom. Then, after the break, you can decide which activity to go on with before you wind up the clock again.\n",
      "\n",
      "A meeting without breaks turns into a yawn-fest within one hour. On many occasions, it's a slumber long before that.\n",
      "\n",
      "If you finish the meeting halfway through a Pomodoro, then you should try to over-learn as a group. You can review what you have done, try to improve it, or make inferences-until the clock rings.\n",
      "\n",
      "Note that your meeting pals don't have to be firm believers in the Pomodoro Technique. They just need to obey the kitchen timer if they want a productive meeting.\n",
      "\n",
      "\n",
      "\n",
      "Tracking Meetings\n",
      "\n",
      "What if your suggestion for using the Pomodoro Technique for meetings in your company is falling on deaf ears? And if a meeting doesn't turn out as well as it could have, do you still write an X on your To Do Today sheet?\n",
      "\n",
      "The answer is that it depends on how you define your X. Writing an X every time the clock rings is one way of implementing the Tracking stage. It's good to track easily measured metrics that don't have complicated meanings. If your Xs mean '25-minute effort units spent working alone at your desk,' for example, then you don't write an X after a meeting.\n",
      "\n",
      "You choose your own metrics. It is not mandatory to track any particular metric, and no metric is universally prohibited. I keep the number of metrics down to avoid being overwhelmed by administrative work. Every type of metric I choose is there simply because I believe today that it may show me how to improve my process for tomorrow. If I suspect that meetings are not giving back the value they cost me, then I could write a Y for meetings and an X for ordinary Pomodori.\n",
      "\n",
      "\n",
      "\n",
      "Pair Work Rhythm\n",
      "\n",
      "The rhythm is essential in the Pomodoro Technique, and it's controlled by the length of the breaks. Participants need to be mentally ready at the start of a new Pomodoro. When doing the Pomodoro Technique individually, this is easy. I won't wind up the timer until I am in the right mood. In a pair Pomodoro, you need to be more cautious. Both of the people must be in a state of preparedness. And it's not a matter of sitting and waiting for that to happen. You can influence your readiness. The break starts when the clock rings, that is, as a signal of the previous Pomodoro ending. At that time, we tell each other the following:\n",
      "\n",
      " · What we will do during the break\n",
      "\n",
      " · Our estimate of how long it will take\n",
      "\n",
      "We don't need to be detailed about the first item, and no one will punish us if the estimate isn't exactly correct. My collaboration partner might say, 'I need to make a phone call; it'll take about five minutes.' This will make us aware of the size of the break already from the beginning.\n",
      "\n",
      "\n",
      "\n",
      "Authorizing Pomodoro Start\n",
      "\n",
      "When working in pairs, how can the person winding up the clock be sure that his partner has mentally finished what he did during the last break? The answer is quite easy. Both your pairing partner and you should authorize your commitment to start a new Pomodoro. If the authorization isn't complete, the transition of state-from break to work-is not legitimate. This is analogous to authorizing a bank customer before his money can be transmitted to another account.\n",
      "\n",
      "The authorization can be more or less formal. One way is to ask out loud, 'Are you ready?' Then wait for the mandatory positive response from the partner. Another way is to move a personal token. Each team member pets her own personal soft toy animal. I'm fond of soft toy bloodhounds. Before winding up the clock to start a Pomodoro, both my pairing partner and I need to put our pets on the desk in a free area. When the clock rings as a signal for break, the pets are removed from this free area. Does exposing your favorite soft toy animal make you feel silly? Well, use your mobile phone as a token. This is conditioned reflexes in practice!\n",
      "\n",
      "\n",
      "\n",
      "Pair Pomodoro Length\n",
      "\n",
      "Are the Pomodoro iterations lengthened or shortened when working in pairs? Neither. First, try 25 minutes for at least a week. If it feels too short or too long, then try another length for at least a week.\n",
      "\n",
      "Short Pomodori create more process ceremony compared to doing real work. Long Pomodori make it harder to focus all the time and harder to cut the day into small releases. Your interpair communication can be a heavy effort in a long Pomodoro. On the other hand, pair chatting takes some time, and it's still important that each Pomodoro increases your accumulated work result.\n",
      "\n",
      "Again, you don't want to change the length too often, since then you can't use the number of completed Pomodori as an effort metric. Suppose that you have 25-minute Pomodori on Monday, a 40-minute Pomodori on Tuesday, and a 20-minute Pomodori on Wednesday. How should you compare each day's efforts? The number 25 is not a holy grail, but try it at least for two weeks. If you're not satisfied, then try another length for at least another week.\n",
      "\n",
      "\n",
      "\n",
      "Split a Pair Temporarily\n",
      "\n",
      "Sometimes you come to a point where the emerging task is more of a search-and-investigate nature, not a normal constructive collaboration task. Then it could be easier to split up for a Pomodoro. 'I look for this, you look for that, and then we meet again after the next Pomodoro break.' Still, when you work independently, you need to be in sync if your ambition is to resume pairing later.\n",
      "\n",
      "Say my work partner needs to take a longer break from our shared work task. Maybe he's scheduled for a meeting for the next 35 minutes. When he tells me this, I ask him whether I can go on solo with our joint work task for one Pomodoro. If the nature of this task does not allow a single person to go on, then I have to come up with some other tasks for myself until my partner comes back.\n",
      "\n",
      "Sometimes it can be more fruitful to split apart for one Pomodoro just to get two independent views on a problem. We go apart and do two parallel and individual brainstorms. When the clock rings and after a small break, we join up and explain our new results to each other.\n",
      "\n",
      "\n",
      "\n",
      "Tracking Pair Work\n",
      "\n",
      "There are many seldom stated truths about process metrics:\n",
      "\n",
      " · You need a big sample in order to draw any conclusions from metrics.\n",
      "\n",
      " · Apples shouldn't be compared to pears.\n",
      "\n",
      " · Metrics hardly ever show up with a good-to-bad scale.\n",
      "\n",
      "If you change collaboration partners every morning, then it's hard to compare the five Pomodori when you worked with Lisa on Monday with the seven Pomodori when you worked with Fred on Tuesday. Lisa might have key knowledge for this project and needs to spend some time helping other teammates. The most valuable metrics are those collected while you work with a recurring partner.\n",
      "\n",
      "But you don't want to be caught up in details. Even though measuring with different partners is not perfect, it will give you feedback. Frequently changing partner is a fluctuating source of error for your tracking. In the long run, it should even itself out, however.\n",
      "\n",
      "\n",
      "\n",
      "Team Culture\n",
      "\n",
      "Every team has to find its own culture. By culture, I mean 'the way we do things here.' To promote communication within the team, you can decide to not count team-member-to-team-member questions as external interruptions. It's like tolerating two activities in every Pomodoro:\n",
      "\n",
      " · The chosen activity from the To Do Today sheet\n",
      "\n",
      " · Work-related, interteam communication\n",
      "\n",
      "This agreement entails an additional responsibility. Before you start to communicate with a teammate who's obeying his ticking clock, you need to consider whether you need the answer immediately. Otherwise, you might use an asynchronous communication channel, such as sending an email or passing a brief slip of paper.\n",
      "\n",
      "\n",
      "\n",
      "Self-Reflection on Team\n",
      "\n",
      " · How long are your meetings?\n",
      "\n",
      " · After how much time do you take a break when you work in pairs?\n",
      "\n",
      " · After how much time do you take a break when you work in groups?\n",
      "\n",
      " · What type of activities are not suitable for pair work?\n",
      "\n",
      " · Do you start your meetings even if everyone isn't ready?\n",
      "\n",
      "Appendix A\n",
      "\n",
      "Process Map\n",
      "\n",
      "\n",
      "\n",
      "Appendix B\n",
      "\n",
      "\n",
      "\n",
      "The official Pomodoro Techniques site : Francesco Cirillo invented the Pomodoro Technique, and he also maintains the official site. There you can find the official book, written by Francesco. He also shares templates for To Do Today sheet and Activity Inventory sheet. See http:// www.pomodorotechnique.com .\n",
      "\n",
      "Pomodoro timer : Personally, I prefer a robust mechanical kitchen timer. If you opt for software, you will find it on the Internet for most operating systems, phone models, and virtual machines. Search for pomodoro+timer+download .\n",
      "\n",
      "Discussion groups : There are two groups on Google Groups. One focuses on desktop timers, while the other one has general discussions about the Pomodoro Technique. You'll find them easily by searching for Pomodoro Technique at http://groups.google.com .\n",
      "\n",
      "My blog : http://blog.staffannoteberg.com . I write about the Pomodoro Technique on my blog, but I also write about many other fascinating things.\n",
      "\n",
      "Appendix C\n",
      "\n",
      "Afterword\n",
      "\n",
      "Myself\n",
      "\n",
      "\n",
      "\n",
      "My name is Staffan. I live in Stockholm and Istanbul with my wife Anni and our four daughters, Thea, Edda, Groa, and Bina. Two decades ago, I studied mathematics and computer science at the Royal Institute of Technology in Stockholm. But I started programming many years before that. My first computer was a Commodore 64, a machine that looked like a bread tin.\n",
      "\n",
      "Some years ago I read Francesco Cirillo's book about the Pomodoro Technique, 1 and I started practicing it. Then I began to teach this methodology, and now I'm writing a book about it.\n",
      "\n",
      "I'd love to hear your stories and experiences of using the Pomodoro Technique. Don't hesitate to send an email to staffan.noteberg@rekursiv.se or, even better, connect with me in your favorite social network service. You'll find me at AIM, Dopplr, Facebook, Flickr, FriendFeed, Friendster, GTalk, ICQ, LinkedIn, MSN/Live, MySpace, Plaxo, Slideshare, Twitter, Xing, Yahoo, and probably half a dozen more communities by now.\n",
      "\n",
      "Acknowledgments\n",
      "\n",
      "Kudos to Tindersticks for recording the Curtains album back in 1997 and Mercan Dede who recorded the Su album in 2003. They've been on the turntable all the time I was writing this book.\n",
      "\n",
      "Kudos to Francesco Cirillo for inventing the Pomodoro Technique. His website and original book on the Pomodoro Technique can be found at http://www.pomodorotechnique.com . The Pomodoro Technique copyright and trademark is owned by Francesco Cirillo and is used with his permission. Kudos to Henrik Kniberg and Francesco for contributing the forewords.\n",
      "\n",
      "Kudos to Vicente Ayestarán, Patrick Baumgartner, Simon Baker, Chris Beams, Isidor Behrens, Renzo Borgatti, Hans Brattberg, Daniel Brolund, Tommy Bryntse, Pascal Van Cauwenberghe, Mikael Dahlke, Brian Di Croce, Karl Dickson, Åsa Dickson, Ola Ellnestam, Dan Fernandez, Sebastian Ganslandt, Robert Ginsberg, Tormod Halvorsen, Chris Hedgate, Mats Henricson, Joakim Holm, Peter Hultgren, César Idrovo, Bent Jensen, Joakim Karlsson, Dave Klein, Pekka Klärck, Lasse Koskela, Jeff Kwak, Melanie Langenhan, Andreas Larsson, Eric Lefevre, Ralf Lippold, Brian McKeough, Karl Métivier, Gustaf Nilsson Kotte, Anders Nilsson, Michael Nilsson, Thomas Nilsson, Viktor Nordling, Lena Norman, Bernard Notarianni, Jack Nutting, Joakim Ohlrogge, Jelena Vencl Ohlrogge, Libin Pan, Tomas Rahkonen, Johanna Rothman, Magnus Rydin, Johan Rylander, Måns Sandström, Kevin E. Schlabach, Peter Sönnergren, Antonio Terreno, Thomas Thyberg, Portia Tung, Jacques Turbé, Kevin Walton, Tsutomu Yasui, and Jason Yip for reviewing the draft.\n",
      "\n",
      "Kudos to Daniel H Steinberg, Steve Peter, Kim Wimpsett, Janet Furlow, and the other people at Pragmatic Bookshelf for transforming the manuscript into a real book.\n",
      "\n",
      "Infinite kudos to AIK: alltid är vi med er, alltid ska vi se er .\n",
      "\n",
      "Colophon\n",
      "\n",
      "I made the drawings in an A6, top-spiral, 80-sheet pad from Esselte. It is Nordic Swan environmentally labeled and has 5x5 mm squares, no holes, and wood-free 60 gr/m2 paper.\n",
      "\n",
      "I did the pencil drawings with a BIC Matic mechanical pencil with 0.7 mmHBleads. Then I added watercolor from a Color &amp; Co paint set filled with six tempera blocks in size 2 (diameter 57 mm, height 19 mm) in the following colors: Gold Yellow, Carmine, Ultramarine, Brilliant Green, Black, and White. Finally, I scanned them with a HP Photosmart 1200 Photo Scanner in 300 dpi, 24-bit color.\n",
      "\n",
      "The spiral pad, the mechanical pencil, the watercolor paint set, and the photo scanner are all inexpensive, simple tools. I'm convinced that the content, the ideas, and the way something is explained is more important than the quality, the sophistication, and the price of the tools.\n",
      "\n",
      "The body type in this book is Bookman, designed in 1936 by Chauncey Griffith of American Type Founders, based on a design by Alexander Phemister. The current version is a digital revival by Ed Benguiat for the International Typeface Corporation in 1975.\n",
      "\n",
      "Headings are Avant Garde, designed by Herb Lubalin as the logo for Avant Garde magazine and developed into a typeface by Lubalin and Tom Carnase in the early 1970s.\n",
      "\n",
      "Appendix D\n",
      "\n",
      "Bibliography\n",
      "\n",
      "[AK53]\n",
      "\n",
      "Eugéne Aserinsky and Nathaniel Kleitman. Regularly occur- ring periods of eye motility, and concomitant phenomena, during sleep. Science , 104(18)(3062):273-274, 1953.\n",
      "\n",
      "[All02]\n",
      "\n",
      "David Allen. Getting Things Done: The Art of Stress-Free Pro- ductivity . Penguin, New York, 2002.\n",
      "\n",
      "[Bab08]\n",
      "\n",
      "Leo Babauta. Zen To Done: The Ultimate Simple Productivity System . Create Space, Scotts Valley, 2008.\n",
      "\n",
      "[Bad66]\n",
      "\n",
      "Alan Baddeley. Short-term memory for word sequences as a function of acoustic, semantic and formal similarity. Quar- terly Journal of Experimental Psychology , 18, 1966.\n",
      "\n",
      "[BB96]\n",
      "\n",
      "Tony Buzan and Barry Buzan. The Mind Map Book: How to Use Radiant Thinking to Maximize Your Brain's Untapped Potential . Plume, New York, 1996.\n",
      "\n",
      "[Bec00]\n",
      "\n",
      "Kent Beck. Extreme Programming Explained: Embrace Change . Addison-Wesley, Reading, MA, 2000.\n",
      "\n",
      "[Bra08]\n",
      "\n",
      "Nicole Branan. Go ahead, change your mind. Scientific American Mind , 19:5, 2008.\n",
      "\n",
      "[Buz03]\n",
      "\n",
      "Tony Buzan. Brain Child . Thorsons, London, 2003.\n",
      "\n",
      "[Cir]\n",
      "\n",
      "Francesco Cirillo. The Pomodoro Technique. http://www.pomodorotechnique.com .\n",
      "\n",
      "[Cov94]\n",
      "\n",
      "Stephen R. Covey. The 7 Habits of Highly Effective People . Free Press, New York, 1994.\n",
      "\n",
      "[Csi02]\n",
      "\n",
      "Mihaly Csikszentmihalyi. Flow . Rider, 2002.\n",
      "\n",
      "http://www.theautismnews.com/2009/07/01/what-autism-tells-us-about-development-of-sa\n",
      "\n",
      "[EHP + 07]\n",
      "\n",
      "Jeffrey M. Ellenbogen, Peter T. Hu, Jessica D. Payne, Debra Titone, and Matthew P. Walker. Human rela- tional memory requires time and sleep. Proceedings of the National Academy of Sciences of the United States of Amer- ica , 104(18), 2007.\n",
      "\n",
      "[Fio07]\n",
      "\n",
      "Neil A. Fiore. The Now Habit . Jeremy P Tarcher, New York, rev ed edition, 2007.\n",
      "\n",
      "[For00]\n",
      "\n",
      "Mark Forster. Get Everything Done . Hodder &amp; Stoughton, London, 2000.\n",
      "\n",
      "[Fre80]\n",
      "\n",
      "Sigmund Freud. The Interpretation of Dreams . Avon Books, Dresden, Tennessee, 1980.\n",
      "\n",
      "[Gla06]\n",
      "\n",
      "Malcolm Gladwell. Blink . Little, Brown and Company, Boston, 2006.\n",
      "\n",
      "[Gla08] Malcolm Gladwell. Outliers: The Story of Success . Little, Brown and Company, New York, 2008.\n",
      "\n",
      "[Gol04] North River Press, Great Bar-\n",
      "\n",
      "Eliyahu Goldratt. The Goal . rington, MA, third edition, 2004.\n",
      "\n",
      "[Ing01]\n",
      "\n",
      "David H. Ingvar.\n",
      "\n",
      "Hjärnans futurum\n",
      "\n",
      ".\n",
      "\n",
      "Robinson Publishing,\n",
      "\n",
      "Stockholm, 2001.\n",
      "\n",
      "[J ¨ 5] 0\n",
      "\n",
      "Bodil Jönsson. Ten Thoughts About Time . Robinson Pub- lishing, London, new edition edition, 2005.\n",
      "\n",
      "[Kli08]\n",
      "\n",
      "Torkel Klingberg. The Overflowing Brain . Oxford University Press, New York, 2008.\n",
      "\n",
      "[Lin08]\n",
      "\n",
      "Joakim Lindström. Hjärnkoll . Bonnier Carlsen, Stockholm, 2008.\n",
      "\n",
      "[Mor98]\n",
      "\n",
      "Hans Moravec. When will computer hardware match the human brain? Journal of Transhumanism , 1, 1998.\n",
      "\n",
      "[Rev]\n",
      "\n",
      "William\n",
      "\n",
      "Reville.\n",
      "\n",
      "What\n",
      "\n",
      "autism\n",
      "\n",
      "tells\n",
      "\n",
      "us\n",
      "\n",
      "about\n",
      "\n",
      "development\n",
      "\n",
      "of\n",
      "\n",
      "savant\n",
      "\n",
      "skills.\n",
      "\n",
      "[Rot09]\n",
      "\n",
      "Johanna Rothman. Manage Your Project Portfolio: Increase Your Capacity and Finish More Projects . The Pragmatic Pro- grammers, LLC, Raleigh, NC, and Dallas, TX, 2009.\n",
      "\n",
      "\n",
      "\n",
      "More Pragmatic Bookshelf titles for you...\n",
      "\n",
      "Pragmatic Thinking and Learning\n",
      "\n",
      "Software development happens in your head. Not in an editor, IDE, or design tool. In this book by Pragmatic Programmer Andy Hunt, you'll learn how our brains are wired, and how to take advantage of your brain's architecture. You'll master new tricks and tips to learn more, faster, and retain more of what you learn.\n",
      "\n",
      " · Use the Dreyfus Model of Skill Acquisition to become more expert · Leverage the architecture of the brain to strengthen different thinking modes\n",
      "\n",
      " · Avoid common 'known bugs' in your mind\n",
      "\n",
      " · Learn more deliberately and more effectively\n",
      "\n",
      " · Manage knowledge more efficiently\n",
      "\n",
      "Pragmatic Thinking and Learning: Refactor your Wetware\n",
      "\n",
      "Andy Hunt\n",
      "\n",
      "(288 pages) ISBN: 978-1-9343560-5-0. $34.95\n",
      "\n",
      "http://pragprog.com/titles/ahptl\n",
      "\n",
      "The Passionate Programmer\n",
      "\n",
      "This book is about creating a remarkable career in software development. Remarkable careers don't come by chance. They require thought, intention, action, and a willingness to change course when you've made mistakes. Most of us have been stumbling around letting our careers take us where they may. It's time to take control.\n",
      "\n",
      "This revised and updated second edition lays out a strategy for planning and creating a radically successful life in software development (the first edition was released as My Job Went to India: 52 Ways To Save Your Job) .\n",
      "\n",
      "The Passionate Programmer: Creating a Remarkable Career in Software Development\n",
      "\n",
      "Chad Fowler (200 pages) ISBN: 978-1934356-34-0. $23.95\n",
      "\n",
      "http://pragprog.com/titles/cfcar2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "...and for your project.\n",
      "\n",
      "Manage It!\n",
      "\n",
      "Manage It! is an award-winning, risk-based guide to making good decisions about how to plan and guide your projects. Author Johanna Rothman shows you how to beg, borrow, and steal from the best methodologies to fit your particular project. You'll find what works best for you .\n",
      "\n",
      " · Learn all about different project lifecycles · See how to organize a project · Compare sample project dashboards · See how to staff a project · Know when you're done-and what that means.\n",
      "\n",
      "Manage It! Your Guide to Modern, Pragmatic Project Management\n",
      "\n",
      "Johanna Rothman\n",
      "\n",
      "(360 pages) ISBN: 0-9787392-4-8. $34.95\n",
      "\n",
      "http://pragprog.com/titles/jrpm\n",
      "\n",
      "Agile Retrospectives\n",
      "\n",
      "Mine the experience of your software development team continually throughout the life of the project. Rather than waiting until the end of the project-as with a traditional retrospective, when it's too late to help-agile retrospectives help you adjust to change today .\n",
      "\n",
      "The tools and recipes in this book will help you uncover and solve hidden (and not-so-hidden) problems with your technology, your methodology, and those difficult 'people issues' on your team.\n",
      "\n",
      "Agile Retrospectives: Making Good Teams Great\n",
      "\n",
      "Esther Derby and Diana Larsen\n",
      "\n",
      "(170 pages) ISBN: 0-9776166-4-9. $29.95 http://pragprog.com/titles/dlret\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Pragmatic Bookshelf\n",
      "\n",
      "The Pragmatic Bookshelf features books written by developers for developers. The titles continue the well-known Pragmatic Programmer style and continue to garner awards and rave reviews. As development gets more and more difficult, the Pragmatic Programmers will be there with more titles and products to help you stay on top of your game.\n",
      "\n",
      "\n",
      "\n",
      "Pomodoro Technique Illustrated's Home Page\n",
      "\n",
      "http://pragprog.com/titles/snfocus\n",
      "\n",
      "Source code from this book, errata, and other resources. Come give us feedback, too!\n",
      "\n",
      "Register for Updates\n",
      "\n",
      "http://pragprog.com/updates\n",
      "\n",
      "Be notified when updates and new books become available.\n",
      "\n",
      "Join the Community\n",
      "\n",
      "http://pragprog.com/community\n",
      "\n",
      "Read our weblogs, join our online discussions, participate in our mailing list, interact with our wiki, and benefit from the experience of other Pragmatic Programmers.\n",
      "\n",
      "New and Noteworthy\n",
      "\n",
      "http://pragprog.com/news\n",
      "\n",
      "Check out the latest pragmatic developments, new titles and other offerings.\n",
      "\n",
      "\n",
      "\n",
      "If you liked this eBook, perhaps you'd like to have a paper copy of the book. It's available for purchase at our store: pragprog.com/titles/snfocus .\n",
      "\n",
      "\n",
      "\n",
      "Online Orders: Customer Service: Non-English Versions: Pragmatic Teaching: Author Proposals: Contact us:\n",
      "\n",
      "www.pragprog.com/catalog\n",
      "\n",
      "support@pragprog.com translations@pragprog.com academic@pragprog.com proposals@pragprog.com 1-800-699-PROG (+1 919 847 3884)\n"
     ]
    }
   ],
   "source": [
    "source = \"../data/Pomodoro Technique Illustrated Can You Focus - Really Focus - for 25 Minutes by Staffan Noteberg.pdf\"\n",
    "print(convert_pdf_to_text(source))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n",
      "Could not load the custom kernel for multi-scale deformable attention: /home/fkt/.cache/torch_extensions/py311_cu124/MultiScaleDeformableAttention/MultiScaleDeformableAttention.so: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ihe ultimate seeker's for those brave enough to face their true north and take their power back. JENNA BLACK, INTERNATIONAL COACH guide\n",
      "\n",
      "THE\n",
      "\n",
      "MO U NTA/ N\n",
      "\n",
      "1 S\n",
      "\n",
      "YO U\n",
      "\n",
      "TRANSFO R MING SELF -SAB OTAG E INTO SELF -MASTERY\n",
      "\n",
      "B RIANN A WI E S T\n",
      "\n",
      "T H E\n",
      "\n",
      "MOUNTAIN\n",
      "\n",
      "I S\n",
      "\n",
      "Y O U\n",
      "\n",
      "T R A N S F O R M I N G   S E L F - S A B O T A G E I N T O   S E L F - M A S T E R Y\n",
      "\n",
      "B R I A N N A   W I E S T\n",
      "\n",
      "\n",
      "\n",
      "THOUGHTCATALOG.COM\n",
      "\n",
      "NEW YORK   ·   LOS ANGELES\n",
      "\n",
      "\n",
      "\n",
      "Copyright © 2020 Brianna Wiest. All rights reserved.\n",
      "\n",
      "Published by Thought Catalog Books, an imprint of the digital magazine Thought Catalog, which is owned and operated by The Thought &amp; Expression Company LLC, an independent media organization based in Brooklyn, New York and Los Angeles, California.\n",
      "\n",
      "This book was produced by Chris Lavergne and Noelle Beams and designed by KJ Parish. Visit us on the web at thoughtcatalog.com and shopcatalog.com.\n",
      "\n",
      "Made in the United States of America.\n",
      "\n",
      "ISBN 978-1-949759-22-8\n",
      "\n",
      "10 9 8 7 6 5 4 3 2 1\n",
      "\n",
      "'Brianna's book is a beautiful expression of healing. Her insights  on  self-sabotage,  emotional  intelligence,  and deep  transformation  are  invaluable.  She  understands that change begins with self, and her book is a gift to the collective.'\n",
      "\n",
      "- DR. NICOLE LEPERA , 'The Holistic Psychologist'\n",
      "\n",
      "'I'm of the belief that in fulfilling our deepest potential, the greatest rewards come less from outcomes and more from who we must become in order to achieve what we know we are truly capable of. In this beautifully written and eye-opening book, Brianna Wiest inspires us to scale our own mountains with powerful insights to help prepare you for the climb ahead. A must-read for those ready to do the inner work required to live a life of fulfillment, wonder, and enjoyment!'\n",
      "\n",
      "- SIMON ALEXANDER ONG , International Life Coach &amp; Business Strategist\n",
      "\n",
      "' The Mountain Is You is a wake-up call that inspires hope in  adversity.  You're  invited  to  burn  the  rules  of  what you've been taught about yourself, as you awaken your inner hero and consciously choose a new narrative, and ultimately,  create  a  life  you  deeply  desire  and  deserve. Brianna  provides  an  alchemy  of  pragmatic  tools  and deep soul shifts to build the courage and clarity required to climb your own personal mountain-and essentially, remember who you came here to be. The ultimate seeker's guide for those brave enough to face their true north and take their power back.'\n",
      "\n",
      "- JENNA BLACK , International Coach\n",
      "\n",
      "'Brianna Wiest is one of my favorite writers. She combines life-changing wisdom with a unique eloquence that inspires readers to reclaim their power and change their lives for the better. The Mountain Is You is bound to help many people.'\n",
      "\n",
      " - YUNG PUEBLO , Best-Selling Author of 'Inward'\n",
      "\n",
      "'A revelation. The words wrote struck me so deep inside, there  were  several  moments  that  I  had  to  pause  from reading because my eyes filled with tears of realization and confirmation.'\n",
      "\n",
      " - DAWN ZULUETA , Film-Television Actress, Host &amp; Model\n",
      "\n",
      "'Brianna Wiest's masterpiece is the perfect roadmap for understanding why we self-sabotage, when we do it, and how to stop doing it-for good.'\n",
      "\n",
      " - DR. STEVEN EISENBERG , Wellbeing &amp; Connection Expert,\n",
      "\n",
      "Renowned Internist &amp; Oncologist\n",
      "\n",
      "I N T R O D U C T I O N\n",
      "\n",
      "MUCH LIKE NATURE , life is very often working in our favor, even when it seems like we are only being faced with adversity, discomfort, and change.\n",
      "\n",
      "As forest fires are essential to the ecology of the environment-opening new seeds that require heat to sprout and rebuild a population of trees-our minds also go through periodic episodes of positive disintegration, or a cleansing through which we release and renew our self-concept. We know that nature is most fertile and expansive at its perimeters, where climates meet, and we also transform when we reach our edge states, the points at which we are forced to step out of our comfort zones and regroup. 1 When we can no longer rely on our coping mechanisms to help distract us from the problems in our lives, it can feel as though we've hit rock bottom. The reality is that this sort of awakening is what happens when we finally come to terms with the problems that have existed for a long time. The breakdown is often just the tipping point that precedes the breakthrough, the moment a star implodes before it becomes a supernova.\n",
      "\n",
      "Just  as  a  mountain  is  formed  when  two  sections  of  the ground are forced against one another, your mountain will arise out of coexisting but conflicting needs. Your mountain requires you to reconcile two parts of you: the conscious and the unconscious, the part of you that is aware of what you want and the part of you that is not aware of why you are still holding yourself back.\n",
      "\n",
      "Historically, mountains have been used as metaphors for spiritual  awakenings,  journeys  of  personal  growth,  and of course, insurmountable challenges that seem impossible to overcome when we are standing at the bottom. Like so much of nature, mountains provide us with an inherent wisdom about what it will take to rise up to our highest potential.\n",
      "\n",
      "The objective of being human is to grow. We see this reflected back to us in every part of life. Species reproduce, DNA evolves  to  eliminate  certain  strands  and  develop new ones,  and  the  edges  of  the  universe  are  expanding forever  outward.  Likewise,  our  ability  to  feel  the  depth and beauty of life is capable of expanding forever inward if we are willing to take our problems and see them as catalysts. Forests need fire to do this, volcanoes need implosions, stars need collapse, and human beings often need to be faced with no other option but to change before they really do.\n",
      "\n",
      "To have a mountain in front of you does not mean you are  fundamentally  broken  in  some  way.  Everything  in\n",
      "\n",
      "nature is imperfect, and it is because of that imperfection  that  growth  is  possible.  If  everything  existed  in uniformity, the gravity that created the stars and planets and everything that we know would not exist. Without breaks, faults, and gaps, nothing could grow and nothing would become. 2 The fact that you are imperfect is not a sign that you have failed; it is a sign that you are human, and more importantly, it is a sign that you still have more potential within you.\n",
      "\n",
      "Maybe you know what your mountain is. Maybe it's addiction, weight, relationships, jobs, motivation, or money. Maybe you don't. Maybe it's a vague sense of anxiety, low self-esteem, fear, or a general discontentment that seems to bleed out onto everything else. The mountain is often less a challenge in front of us as it is a problem within us, an unstable foundation that might not seem evident on the surface but is nonetheless shifting almost every part of our lives.\n",
      "\n",
      "Usually when we have a problem that is circumstantial, we are facing the reality of life. When we have a problem that is chronic, we are facing the reality of ourselves. We often think that to face a mountain means to face life's hardships, but the truth is that it is almost always because of  the  years  we  have  spent  accumulating  tiny  traumas, adaptations,  and  coping  mechanisms,  all  of  which  have compounded over time.\n",
      "\n",
      "Your mountain is the block between you and the life you want to live. Facing it is also the only path to your freedom and becoming. You are here because a trigger showed you to  your  wound,  and  your  wound  will  show you to your path, and your path will show you to your destiny.\n",
      "\n",
      "When you arrive at this breaking point-the foot of the mountain, the heat of the fire, the night that finally wakes you-you are at the crux of the breakdown, and if you are willing to do the work, you will find that it is the entryway to the breakthrough you have spent your entire life waiting for.\n",
      "\n",
      "Your old self can no longer sustain the life you are trying to lead; it is time for reinvention and rebirth.\n",
      "\n",
      "You must release your old self into the fire of your vision and be willing to think in a way you have never even tried before. You must mourn the loss of your younger self, the person who has gotten you this far but who is no longer equipped  to  carry  you  onward.  You  must  envision  and become one with your future self,  the  hero  of  your  life that is going to lead you from here. The task in front of you is silent,  simple,  and  monumental. It is a feat most do not ever get to the point of attempting. You must now learn agility, resilience, and self-understanding. You must change completely, never to be the same again.\n",
      "\n",
      "The mountain that stands in front of you is the calling of your life, your purpose for being here, and your path finally made clear. One day, this mountain will be behind you, but who you become in the process of getting over it will stay with you always.\n",
      "\n",
      "In the end, it is not the mountain that you must master, but yourself.\n",
      "\n",
      "CHAPTER 1\n",
      "\n",
      "THE MOUNTAIN IS YOU\n",
      "\n",
      "THERE IS NOTHING HOLDING you back in life more than yourself.\n",
      "\n",
      "If  there  is  an  ongoing  gap  between  where  you  are  and where you want to be-and your efforts to close  it  are consistently met with your own resistance, pain, and discomfort-self-sabotage is almost always at work.\n",
      "\n",
      "On the surface,  self-sabotage  seems  masochistic.  It  appears  to  be  a  product  of  self-hatred,  low  confidence,  or a lack of willpower. In reality, self-sabotage is simply the presence of an unconscious need that is being fulfilled by the self-sabotaging behavior. To overcome this, we must go through a process of deep psychological excavation. We must  pinpoint  the  traumatic  event,  release  unprocessed emotions, find healthier ways to meet our needs, reinvent our self-image, and develop principles such as emotional intelligence and resilience.\n",
      "\n",
      "It is no small task, and yet it is the work that all of us must do at one point or another.\n",
      "\n",
      "SELF-SABOTAGE IS NOT ALWAYS OBVIOUS AT THE ONSET\n",
      "\n",
      "When Carl Jung was a child, he fell on the ground in school and hit his head. When he got hurt, he thought to himself: 'Yes, maybe I won't have to go back to school now.' 3\n",
      "\n",
      "Though he is known today for his insightful body of work, he actually didn't like school or fit in well with his peers. Shortly after his accident, Jung began experiencing sporadic and uncontrollable fainting spells. He unconsciously developed what he would call a 'neurosis' and ultimately came to realize that all neuroses are 'substitute[s] for legitimate suffering.'\n",
      "\n",
      "In  Jung's  case,  he  made  an  unconscious  association  between fainting and getting out of school. He came to believe that the fainting spells were a manifestation of his unconscious desire to get out of class, where he felt uncomfortable and unhappy. Likewise, for many people, their fears and attachments are very often just symptoms of deeper issues for which they do not have any better way to cope.\n",
      "\n",
      "SELF-SABOTAGE IS A COPING MECHANISM\n",
      "\n",
      "Self-sabotage  is  what  happens  when  we  refuse  to  consciously meet our innermost needs, often because we do not believe we are capable of handling them.\n",
      "\n",
      "Sometimes, we sabotage our relationships because what we really want is to find ourselves, though we are afraid to be alone. Sometimes, we sabotage our professional success because what we really want is to create art, even if it will make us seem less ambitious by society's measures. Sometimes, we sabotage our healing journey by psychoanalyzing our feelings, because doing so ensures we avoid actually experiencing them. Sometimes, we sabotage our self-talk because if we believed in ourselves, we'd feel free to get back out in the world and take risks, and that would leave us vulnerable.\n",
      "\n",
      "In the end, self-sabotage is very often just a maladaptive coping mechanism, a way we give ourselves what we need without having to actually address what that need is. But like any coping mechanism, it is just that - a way to cope. It's not an answer, it's not a solution, and it does not ever truly solve the problem. We are merely numbing our desires, and giving ourselves a little taste of temporary relief.\n",
      "\n",
      "SELF-SABOTAGE COMES FROM I R R AT I O N A L   F E A R\n",
      "\n",
      "Sometimes, our most sabotaging behaviors are really the result of long-held and unexamined fears we have about the world and ourselves.\n",
      "\n",
      "Perhaps  it  is  the  idea  that  you  are  unintelligent,  unattractive, or disliked. Perhaps it is the idea of losing a job,\n",
      "\n",
      "taking  an  elevator,  or  committing  to  a  relationship.  In other cases, it can be more abstract, such as the concept of someone 'coming to get' you, violating your boundaries, getting 'caught,' or being wrongly accused.\n",
      "\n",
      "These beliefs become attachments over time.\n",
      "\n",
      "For most people, the abstract fear is really a representation of a legitimate fear. Because it would be too scary to actually dwell on the real fear, we project those feelings onto issues or circumstances that are less likely to occur. If the situation has an extremely low likelihood of becoming reality, it therefore becomes a 'safe' thing to worry about, because subconsciously, we already know it isn't going to happen. Therefore, we have an avenue to express our feelings without actually endangering ourselves.\n",
      "\n",
      "For example, if you are someone who is deeply afraid of being a passenger in a car, maybe your real fear is the loss of control or the idea that someone or something else is controlling  your  life.  Perhaps  the  fear  is  of 'moving  forward,' and the moving car is simply a representation of that.\n",
      "\n",
      "If you were aware of the real issue, you could begin working to resolve it, perhaps by identifying the ways you are giving up your power or being too passive. However, if you aren't aware of the real problem, you'll continue to spend your time trying to convince yourself to not be triggered and anxious while riding in the car and find that it only gets worse.\n",
      "\n",
      "If you try to fix the problem on the surface, you will always come up against a wall. This is because you're trying to rip off a Band-Aid before you have a strategy to heal the wound.\n",
      "\n",
      "SELF-SABOTAGE COMES FROM UNCONSCIOUS, NEGATIVE ASSOCIATIONS\n",
      "\n",
      "Self-sabotage is also one of the first signs that your inner narrative is outdated, limiting, or simply incorrect.\n",
      "\n",
      "Your life is defined not only by what you think about it, but also what you think of yourself. Your self-concept is an idea that you have spent your whole life building. It was  created  by  piecing  together  inputs  and  influences from those around you: what your parents believed, what your  peers  thought,  what  became  self-evident  through personal experience, and so on. Your self-image is difficult to adjust, because your brain's confirmation bias works to affirm your preexisting beliefs about yourself.\n",
      "\n",
      "When we self-sabotage, it is often because we have a negative association between achieving the goal we aspire to and being the kind of person who has or does that thing.\n",
      "\n",
      "If your issue is that you want to be financially stable, and yet you keep ruining every effort you make to get there, you have to go back to your first concept of money. How\n",
      "\n",
      "did your parents manage their finances? More importantly,  what  did  they  tell  you  about  people  who  had  it  and people who didn't? Many people who struggle financially will  justify  their  place  in  life  by  disavowing  money  as  a whole. They will say that all rich people are terrible. If you grew up with people who told you your entire life that people who have money are this way, guess what you're going to resist having?\n",
      "\n",
      "Your anxiety around the issue that you're self-sabotaging is usually a reflection of your limiting belief.\n",
      "\n",
      "Maybe you associate being healthy with being vulnerable, because you had a parent who was perfectly healthy when they suddenly fell ill. Maybe you aren't writing your magnum opus because you don't really want to write; you just want to be seen as 'successful' because that will get you praise, which is typically what people revert to when they want acceptance but haven't gotten it.  Maybe  you  keep eating the wrong foods because they soothe you, but you haven't stopped to ask what they have to keep soothing you from .  Maybe  you  aren't  really  a  pessimist  but  don't know how to connect with the people in your life other than by complaining to them.\n",
      "\n",
      "In order to reconcile this, you have to begin to challenge these preexisting ideas and then adopt new ones.\n",
      "\n",
      "You  have  to  be  able  to  recognize  that  not  everybody with  money  is  corrupt,  not  by  a  long  shot.  Even  more\n",
      "\n",
      "importantly,  given  that  there are people  who  use  their money  in  selfish  ways,  it  is  even  more  important  that good people with great intentions are fearless in pursuit of acquiring this essential tool to create more time, opportunity, and wellness for themselves and others. You have to recognize that being healthy makes you less vulnerable, not more, and that criticism comes with creating anything for the public and isn't a reason to not do it. You have to show yourself that there are many different ways to selfsoothe that are more effective than unhealthy food choices and that there are far better ways to connect with others than through negativity.\n",
      "\n",
      "Once you begin to really question and observe these preexisting beliefs, you begin to see how warped and illogical they  were  all  along-not  to  mention  distinctly  holding you back from your ultimate potential.\n",
      "\n",
      "SELF-SABOTAGE COMES FROM WHAT'S UNFAMILIAR\n",
      "\n",
      "Human beings experience a natural resistance to the unknown, because it is essentially the ultimate loss of control. This is true even if what's 'unknown' is benevolent or even beneficial to us.\n",
      "\n",
      "Self-sabotage is very often the simple product of unfamiliarity,  and  it  is  because  anything  that  is  foreign,  no  matter  how  good,  will  also  be  uncomfortable  until  it  is  also\n",
      "\n",
      "familiar. This often leads people to confuse the discomfort of the unknown with being 'wrong' or 'bad' or 'ominous.' However, it is simply a matter of psychological adjustment.\n",
      "\n",
      "Gay Hendricks calls this your 'upper limit,' or your tolerance for happiness.  Everyone has a capacity for which 4 they allow themselves to feel good. This is similar to what other  psychologists  refer  to  as  a  person's  'baseline,'  or their set predisposition that they eventually revert back to, even if certain events or circumstances shift temporarily.\n",
      "\n",
      "Small  shifts,  compounded  over  time,  can  result  in  permanent baseline adjustments. However, they often don't stick because we come up on our upper limits. The reason we don't allow those shifts to become baselines is because as soon as our circumstances extend beyond the amount of happiness we're accustomed to, we find ways both conscious and unconscious to bring ourselves back to a feeling we're comfortable with.\n",
      "\n",
      "We  are  programmed  to  seek  what  we've  known.  Even though we think we're after happiness, we're actually trying to find whatever we're most used to.\n",
      "\n",
      "SELF-SABOTAGE COMES FROM BELIEF SYSTEMS\n",
      "\n",
      "What you believe about your life is what you will make true about your life.\n",
      "\n",
      "That's  why  it's  so  crucial  to  be  aware  of  these  outdated narratives and have the courage to change them.\n",
      "\n",
      "Maybe you have gone through the majority of your life believing  that  a  standard  $50K  per  year  salary  at  a  decent company is the most you'll ever be capable of. Maybe you've  spent  so  many  years  telling  yourself:  'I  am  an anxious person,' you started to actually identify with it, adopting anxiety and fear into your belief system about who you fundamentally are. Maybe you were raised in a closed-minded social circle or an echo chamber. Maybe you did not know that you could question or arrive at new conclusions about politics or religion. Maybe you never thought you were someone who could have great style, feel content, or travel the world.\n",
      "\n",
      "In  other  cases,  your  limiting  beliefs  might  come  from wanting to keep yourself safe.\n",
      "\n",
      "Maybe that's why you prefer the comfort of what you've known to the  vulnerability  of  what  you  don't,  why  you prefer  apathy  to  excitement,  think  that  suffering  makes you more worthy, or believe that for every good thing in life, there must also be an accompanying 'bad.'\n",
      "\n",
      "To truly heal, you are going to have to change the way you think. You are going to have to become very conscious of negative and false beliefs and start shifting to a mindset that actually serves you.\n",
      "\n",
      "HOW TO GET OUT OF DENIAL\n",
      "\n",
      "Maybe this preliminary information about self-sabotage resonates a bit, or maybe it resonates a lot.\n",
      "\n",
      "Either  way,  if  you  are  here  because  you  truly  want  to change your life, you are going to have to stop being in denial about your personal state of affairs. You are going to have to get real with yourself. You are going to have to decide that you love yourself too much to stop settling for less than what you really deserve.\n",
      "\n",
      "If  you  think  that  you  could  be  doing  better  in  life,  you might be right.\n",
      "\n",
      "If you think that there is more that you are here to accomplish, you might be right.\n",
      "\n",
      "If you think that you are not being your authentic self, you might be right.\n",
      "\n",
      "It does not serve us to use endless affirmations to placate our true feelings about where we are in our journey. When we do this, we start dissociating and get stuck.\n",
      "\n",
      "In an effort to 'love ourselves,' we try to validate everything about who we are. Yet those warm sentiments never quite seem to stick, only ever temporarily numbing  the  discomfort.  Why  don't  they  work?  Because deep down, we know we are not quite being who we\n",
      "\n",
      "want to be, and until we accept this, we are never going to find peace.\n",
      "\n",
      "When we are in denial, we tend to go into 'blame' mode. We look for anyone or anything to explain why we are the way we are. Then we start justifying. If you have to constantly-on a near daily basis-rationalize why you're unhappy about your life, you are not doing yourself any favors. You are not getting any closer to creating the lasting change that you so deeply desire.\n",
      "\n",
      "The first step in healing anything is taking full accountability.  It  is  no  longer  being  in  denial  about  the  honest truth of your life and yourself. It does not matter what your life looks like on the outside; it is how you feel about it  on the inside. It is not okay to be constantly stressed, panicked,  and  unhappy.  Something  is  wrong,  and  the longer you try to 'love yourself ' out of realizing this, the longer you are going to suffer.\n",
      "\n",
      "The greatest act of self-love is to no longer accept a life you are unhappy with. It is to be able to state the problem plainly and in a straightforward manner.\n",
      "\n",
      "That is precisely what you need to do to continue truly uprooting your life and transforming it. It is the first step towards real change.\n",
      "\n",
      "Take a piece of paper and a pen, and write down everything you aren't happy with. Write down, very specifically, every single problem you face. If you are struggling with finances, you need a very clear picture of what's wrong. Write down every debt, every bill, every asset, and every bit of income. If you are struggling with self-image, write down exactly what you dislike about yourself. If it is anxiety, write down everything that bothers or upsets you.\n",
      "\n",
      "You must first  and  foremost  get  out  of  denial  and  into clarity about what's really wrong. At this point, you have a choice: You can make peace, or you can commit to changing. The lingering is what is keeping you stuck.\n",
      "\n",
      "THE PATH BEGINS RIGHT WHERE YOU ARE NOW\n",
      "\n",
      "If you know that change needs to be made in your life, it is okay if you are far away from your goal or if you cannot yet conceive how you will arrive.\n",
      "\n",
      "It is okay if you are starting at the beginning.\n",
      "\n",
      "It is okay if you are at rock bottom and cannot yet see your way through.\n",
      "\n",
      "It is okay if you are at the foot of your mountain and have failed every time you've tried to overcome it.\n",
      "\n",
      "Rock bottom is very often where we begin on our healing journey. This is not because we suddenly see the light, not because our worst days are magically transmuted into some type of epiphany, and not because someone saves us from our own madness. Rock bottom becomes a turning point because it is only at that point that most people think: I never want to feel this way again.\n",
      "\n",
      "That thought is not just an idea. It is a declaration and a  resolution.  It  is  one  of  the  most  life-changing  things you can ever experience. It becomes the foundation upon which you build everything else.\n",
      "\n",
      "When you decide you truly do not ever want to feel a certain way again, you set out on a journey of self-awareness, learning, and growth that has you radically reinvent who you are.\n",
      "\n",
      "In that moment, fault becomes irrelevant. You're no longer mulling over who did what or how you've been wronged. In that moment, only one thing guides you, and it is this: No matter what it takes, I will never accept my life getting to this point again.\n",
      "\n",
      "Rock bottom isn't a bad day. It doesn't happen by chance. We only arrive at rock bottom when our habits begin to compound upon one another, when our coping mechanisms have spiraled so out of control that we can no longer resist the feelings we were attempting to hide. Rock bottom is when we are finally faced with ourselves, when\n",
      "\n",
      "everything has gone so wrong, we are left to realize that there is only one common denominator through it all.\n",
      "\n",
      "We must heal. We must change. We must choose to turn around so that we will never feel this way again.\n",
      "\n",
      "When we have a down day, we don't think: I never want to feel this way again. Why? Because it is not fun, but it's also not unbearable. Mostly, though, we are somewhat aware that small failures are a regular part of life; we are imperfect  but  trying  our  best,  and  that  vague  discomfort  will pass eventually.\n",
      "\n",
      "We  don't  reach  a  breaking  point  because  one  or  two things  go  wrong. We reach a breaking point when we finally accept that the problem isn't how the world is; it is how we are. This is a beautiful reckoning to have. Ayodeji Awosika describes his own like this: 'You must find the purest, purest, purest form of being fed up. Make it hurt. I literally screamed, 'I'm not going to fucking live like this anymore!''\n",
      "\n",
      "Human beings are guided by comfort. They stay close to what feels familiar and reject what doesn't, even if it's objectively better for them.\n",
      "\n",
      "Be this as it is, most people do not actually change their lives  until  not  changing  becomes  the  less  comfortable\n",
      "\n",
      "option. This means that they do not actually embrace the difficulty of altering their habits until they simply do not have another choice. Staying where they are is not viable. They can no longer even pretend that it is desirable in any way. They are, quite honestly, less at rock bottom and more stuck  between a rock that's impinging on them and an arduous climb out from beneath it.\n",
      "\n",
      "If you really want to change your life, let yourself be consumed with rage: not toward others, not with the world, but within yourself.\n",
      "\n",
      "Get angry, determined, and allow yourself to develop tunnel vision with one thing and one thing only at the end: that you will not go on as you are.\n",
      "\n",
      "PREPARING FOR RADICAL CHANGE\n",
      "\n",
      "One of the biggest reasons that people avoid doing important internal work is that they recognize if they heal themselves, their lives will change-sometimes drastically. If they come to terms with how unhappy they are, it means that they will have to temporarily be more uncomfortable, ashamed, or scared while they start all over.\n",
      "\n",
      "Let's  be  clear  about  something: To  put  an  end  to  your self-sabotaging behavior absolutely means that change is on the horizon.\n",
      "\n",
      "Your new life is going to cost you your old one.\n",
      "\n",
      "It's going to cost you your comfort zone and your sense of direction.\n",
      "\n",
      "It's going to cost you relationships and friends.\n",
      "\n",
      "It's going to cost you being liked and understood.\n",
      "\n",
      "It doesn't matter.\n",
      "\n",
      "The people who are meant for you are going to meet you on the other side. You're  going  to  build  a  new  comfort zone around the things that actually move you forward. Instead of being liked, you're going to be loved. Instead of being understood, you're going to be seen.\n",
      "\n",
      "All you're going to lose is what was built for a person you no longer are.\n",
      "\n",
      "Remaining attached to your old life is the first and final act of self-sabotage, and releasing it is what we must prepare for to truly be willing to see real change.\n",
      "\n",
      "CHAPTER 2\n",
      "\n",
      "THERE'S NO SUCH THING AS SELF-SABOTAGE\n",
      "\n",
      "WHEN YOU HABITUATE YOURSELF to do things that move your life forward, you call them skills. When they hold your life back, you call them self-sabotage. They are both essentially the same function.\n",
      "\n",
      "Sometimes, it  happens  by  accident.  Sometimes,  we  just get used to living a certain way and fail to have a vision for how life could be different. Sometimes, we make choices because we don't know how to make better ones or that anything else is  even  possible.  Sometimes,  we  settle  for what we're handed because we don't know we can ask for more. Sometimes, we run our lives on autopilot for long enough that we begin to think we no longer have a choice.\n",
      "\n",
      "However, most of the time, it's not accidental at all. The habits and behaviors you can't stop engaging in-no matter how destructive or limiting they may be-are intelligently designed by your subconscious to meet an unfulfilled need, displaced emotion, or neglected desire.\n",
      "\n",
      "Overcoming  self-sabotage is not about trying  to  figure  out  how  to  override  your impulses; it  is  first  determining  why  those impulses exist in the first place.\n",
      "\n",
      "Self-sabotage is often misunderstood to be a way in which we punish, deride, or intentionally hurt ourselves. On the surface, this seems true enough. Self-sabotage is committing to a healthier diet and finding yourself pulling up to the drive-thru a few hours later. It's identifying a market gap, conceiving an unprecedentedly brilliant business idea, then getting 'distracted' and forgetting to begin working on it. It's having strange and terrifying thoughts and allowing them to paralyze you in the face of important life changes or milestones. It is knowing you have so much to be grateful for and excited about and yet worrying anyway.\n",
      "\n",
      "We often misattribute these behaviors to a lack of intelligence, willpower, or capability. That is usually not the case. Self-sabotage is not a way we hurt ourselves; it's a way we try to protect ourselves.\n",
      "\n",
      "WHAT IS SELF-SABOTAGE?\n",
      "\n",
      "Self-sabotage  is  when  you  have  two  conflicting  desires. One is conscious, one is unconscious. You know how you want to move your life forward, and yet you are still, for some reason, stuck.\n",
      "\n",
      "When you have big, ongoing,  insurmountable  issues  in your life-especially when the solutions seem so simple, so  easy,  and  yet  so  impossible  to  stick  with-what  you have are not big problems but big attachments.\n",
      "\n",
      "People are pretty incredible in the fact that they basically do whatever they want to do.\n",
      "\n",
      "This is true of everything in human life. Regardless of the potential consequences, human nature has revealed itself to  be  incredibly  self-serving.  People  have  an  almost  superhuman way of doing whatever they feel compelled to do, regardless of whom it could hurt, what wars it could spawn, or what future would be put at risk. When you consider this, you begin to realize that if you're keeping something in your life, there has to be a reason you want it there. The only question is why.\n",
      "\n",
      "Some  people  can't  figure  out  why  they  can't  seem  to motivate themselves enough to create a new business to facilitate  their  goal  of  becoming  significantly  wealthier, perhaps not realizing that they have a subconscious belief that to be rich is to be egocentric or disliked. Or perhaps they actually don't want to be super-wealthy. Maybe it's a cover-up for wanting to feel secure and 'taken care of,' or their real desire is to be recognized for their art, and as this feels too unlikely to ever happen, they fall back on a secondary dream that doesn't actually motivate them.\n",
      "\n",
      "Some people say that they want to be successful at any\n",
      "\n",
      "cost and yet don't want to log the hours of work it would take to get there. Perhaps it is because they understand at  some level that being 'successful' doesn't really make you happy nor liked. In fact, the opposite tends to be true. Success usually exposes you to jealousy and scrutiny. Successful people are not loved in the way that we imagine they would be; they are usually picked apart because envious people need to humanize them in some way. Perhaps instead of being 'successful,' what many really want is just to  be  loved,  and  yet  their  ambition  for  success  directly threatens that.\n",
      "\n",
      "Some people can't figure out why they keep choosing the 'wrong'  relationships,  people  whose  patterns  of  rejection, abuse, or refusal to commit seem to be consistent. Perhaps they don't realize that they are actually re-creating the relationship dynamics they experienced when they  were  young  because  they  associate  love  with  loss or abandonment. Perhaps they want to re-create family relationships in which they felt helpless, but to live them again as an adult where they can help the addict, the liar, or the broken person.\n",
      "\n",
      "When it comes to self-sabotaging behaviors, you have to understand that sometimes, it's easy to get attached to having problems.\n",
      "\n",
      "Being successful can make you less liked.\n",
      "\n",
      "Finding love can make you more vulnerable.\n",
      "\n",
      "Making yourself less attractive can guard you.\n",
      "\n",
      "Playing small allows you to avoid scrutiny.\n",
      "\n",
      "Procrastinating puts you back in a place of comfort.\n",
      "\n",
      "All the ways in which you are self-sabotaging are actually ways that you are feeding a need you probably do not even realize you have. Overcoming it is not only a matter of learning to understand yourself better, but realizing that your problems are not problems; they are symptoms.\n",
      "\n",
      "You cannot get rid of the coping mechanisms and think you've solved the problem.\n",
      "\n",
      "WHAT DOES SELF-SABOTAGE LOOK LIKE?\n",
      "\n",
      "It's  impossible  to  say  decisively  what  self-sabotage  does or doesn't look like, because certain habits and behaviors that can be healthy for one person can be unhealthy in another context.\n",
      "\n",
      "With that said, there are definitely some specific behaviors and patterns that are typically indicative of self-sabotage, and they usually relate to being aware that there's a problem in your life, yet feeling the need to perpetuate it regardless. Here are some of the main signs that you're probably in a cycle of self-sabotage.\n",
      "\n",
      "RESISTANCE\n",
      "\n",
      "Resistance is what happens when we have a new project that we need to work on and simply can't bring ourselves to do it. It's when we get into a great new relationship and  then  keep  bailing  on  plans.  It's  when  we  get  an amazing idea for our business and then feel tension and anger when it comes time to sit down and actually get to work.\n",
      "\n",
      "We often feel resistance in the face of what's going right in our lives, not what's going wrong. When we have a problem to solve, resistance is usually nowhere to be found. But when we have something to enjoy, create, or build, we are tapping into a part of ourselves that is trying to thrive instead of just survive, and the unfamiliarity can be daunting.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "Resistance is your way of slowing down and making sure that it's safe to get attached to something new and important. In other cases, it can also be a warning sign that something isn't quite right, and you might need to step back and regroup.\n",
      "\n",
      "Resistance  is  not  the  same  thing  as  procrastination  or indifference and shouldn't be treated as such. When we are experiencing resistance, there is always a reason, and we have to pay attention. If we try to force ourselves to perform in the face of resistance, it usually intensifies the\n",
      "\n",
      "feeling, as we are strengthening the internal conflict and triggering the fear that's holding us back in the first place.\n",
      "\n",
      "Instead,  releasing  resistance  requires  us  to  refocus.  We have to get clear on what we want as well as when and why we want it. We have to identify unconscious beliefs that are preventing us from showing up, and then we have to step back into the work when we feel inspired. Wanting is the entryway to showing up after resistance.\n",
      "\n",
      "HITTING YOUR UPPER LIMIT\n",
      "\n",
      "As discussed before, there is only a certain amount of happiness  that  most  of  us  will  allow  ourselves  to  feel.  Gay Hendricks calls this your 'upper limit.'\n",
      "\n",
      "Your upper limit is essentially the amount of 'good' that you're comfortable having in your life. It is your tolerance and threshold for having positive feelings or experiencing positive events.\n",
      "\n",
      "When you begin to surpass your upper limit,  you  start to unconsciously sabotage what's happening in order to bring  yourself  back  to  what's  comfortable  and  familiar. For some people, this manifests physically, often as aches, pains, headaches, or physical tension. For others, it manifests emotionally as resistance, anger, guilt, or fear.\n",
      "\n",
      "It  might  seem  totally  counterintuitive,  but  we  are  not\n",
      "\n",
      "really wired to be happy; we are wired to be comfortable, and anything that is outside of that realm of comfort feels threatening or scary until we are familiar with it.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "Hitting your upper limit is a really great sign. It means that  you're  approaching  and  surpassing  new  levels  of your  life,  and  that  is  first  and  foremost  something  to congratulate  yourself  for.  The  way  you  resolve  an  upper-limit  problem  is  by  slowly  acclimating  yourself  to your new 'normal.'\n",
      "\n",
      "Instead of shocking yourself into big changes, allow yourself to slowly adjust and adapt. By taking it slow, you are allowing  yourself  to  gradually  reinstate  a  new  comfort zone around what you want your life to be. Over time, you gradually shift your baseline to a new standard.\n",
      "\n",
      "UPROOTING\n",
      "\n",
      "Uprooting  happens  when  someone  finds  themselves jumping  from  relationship  to  relationship  or  changing their  business  website  again  and  again,  when  they  really need to focus on confronting relationship issues when they arise or taking care of clients they already have. In uprooting, you are not allowing yourself to blossom; you are only comfortable with the process of sprouting.\n",
      "\n",
      "It  might  be  constantly  needing  a 'fresh  start,'  which  is often the result of not having healthy ways to deal with stress or struggling with conflict resolution. Uprooting can be a way of diverting attention from the actual problems in your life, as your attention must go toward reestablishing oneself at a new job or in a new town.\n",
      "\n",
      "Ultimately,  uprooting  means  you  are  always  just  beginning your new chapter but never really finishing it. Despite  your  efforts  to  keep  moving  on,  you  end  up  more stuck than ever before.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "First, recognize the pattern.\n",
      "\n",
      "One of the primary symptoms of uprooting is not realizing that one is doing it. Therefore, the most important step is to become aware of what's happening. Trace back your steps over the past few years: How many places have you moved or worked? Then figure out what is driving you away from each new thing you find.\n",
      "\n",
      "Next,  you  need  to  get  clear  on  what  you  really  want. Sometimes, uprooting occurs because we step too quickly toward what we think we want, only to find that we didn't think  it  through  and  don't  really  want  that  thing  very much. Clarity is key, because you're thinking long-term now. What would it look like to choose one place to live, then build connections there? What would it look like to\n",
      "\n",
      "work at the same place and move up in your position or build your business?\n",
      "\n",
      "Remember  that  healing  from  an  uprooting  pattern  is not about settling for something you don't want, nor is it about staying in an unsafe or unhealthy situation because you don't want to move again. It's about getting clear and determined  on  what's  the  right  path  for  you  and  then making a plan for how you can thrive, not just survive. When the moment comes that you would typically flee, confront the discomfort and stay where you are. Figure out why you are uncomfortable getting attached to one thing or another, and determine what a healthy attachment would look like for you.\n",
      "\n",
      "PERFECTIONISM\n",
      "\n",
      "When we expect that our work must be perfect the first time we do it, we end up getting into a cycle of perfectionism.\n",
      "\n",
      "Perfectionism  isn't  actually  wanting  everything  to  be right. It's not a good thing. In fact, it is a hindering thing, because it sets up unrealistic expectations about what we are capable of or what the outcomes of our lives could be.\n",
      "\n",
      "Perfectionism holds us back from showing up and trying, or really doing the important work of our lives. This happens because when we are afraid of failing, or feeling vulnerable, or not being as good as we want others to think\n",
      "\n",
      "we are, we end up avoiding the work that is required to actually become that good. We sabotage ourselves because it  is  the  willingness  to  show  up  and  simply do it, again and again and again, that ultimately brings us to a place of mastery.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "Don't worry about doing it well; just do it.\n",
      "\n",
      "Don't worry about writing a bestseller, just write. Don't worry about making a Grammy-winning hit, just make music.  Don't  worry  about  failing,  just  keep  showing  up and trying. At first, all that matters is that you do what you really want to do. From there, you can learn from your mistakes and over time get to the place where you really want to be.\n",
      "\n",
      "The truth is that we actually do not accomplish great feats when we are anxious about whether or not what we do will indeed be something impressive and world-changing. We accomplish these sorts of things when we simply show up and allow ourselves to create something meaningful and important to us.\n",
      "\n",
      "Instead of perfection, focus on progress. Instead of having something done perfectly, focus on just getting it done. From there, you can edit, build, grow, and develop it to exactly what your vision is. But if you don't get started, you'll never arrive.\n",
      "\n",
      "LIMITED EMOTIONAL PROCESSING SKILLS\n",
      "\n",
      "In life, there are going to be people, situations, and circumstances that are upsetting, infuriating, saddening, and even  enraging. There  will  likewise  be  people,  situations, and circumstances that are inspiring, hopeful, helpful, and truly offer purpose and meaning in your life.\n",
      "\n",
      "When you are only able to process half of your emotions, you stunt yourself. You start going out of your way to avoid any possible situation that could bring up something frustrating  or  uncomfortable,  because  you  have  no  tools  to be able to handle that feeling. This means that you start avoiding the very risks and actions that would ultimately change your life for the better.\n",
      "\n",
      "In  addition,  an  inability  to  process  your  emotions means you get stuck with them. You sit and dwell on your anger and sadness because you don't know how to make them go away. When we can only process half of our emotions, we ultimately only live half of the life we really want to.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "Healthy emotional processing looks different for everyone but generally involves these steps:\n",
      "\n",
      " · Get clear on what happened.\n",
      "\n",
      " · Validate your feelings.\n",
      "\n",
      " · Determine a course correction.\n",
      "\n",
      "First, you need to understand why you're upset or the reason why something is bothering you so much. Without clarity on this, you'll continue to waste your time mulling over the details without really understanding what's hurting you so much.\n",
      "\n",
      "Next, you have to validate how you feel. Recognize that you are not alone; anyone in your situation would probably  feel  similarly  (and  does)  and  that  what  you  feel  is absolutely  okay.  In  doing  this,  you  can  allow  yourself  a physical release such as crying, shaking, journaling about what you feel, or talking to a trusted friend.\n",
      "\n",
      "Once  you  are  clear  on  what's  wrong  and  have  allowed yourself to fully express the extent of your emotions, you can  determine  how  you  will  change  your  behavior  or thought process so that you get an outcome that you really want in the future.\n",
      "\n",
      "J U S T I F I C A T I O N\n",
      "\n",
      "Your  life  is  ultimately  measured  by  your  outcomes,  not your intentions. It is not about what you wanted to do or would have done but didn't have the time. It's not about why you thought you couldn't; it's just whether or not you\n",
      "\n",
      "eventually did. When you're in a pattern of self-sabotaging behavior, you're often treating those excuses the same way you would treat measurable outcomes: You're using them to make yourself feel momentarily satisfied, using them as a replacement for the accomplishment itself.\n",
      "\n",
      "When we have a goal, dream, or plan, there is no measure of  intent.  It  is  only  whether  you  did  it  or  did  not.  Any other reason you offer for not showing up and doing the work is simply you stating that you prioritize that reason over your ultimate ambition, which means that it will always take precedence in your life.\n",
      "\n",
      "You may also be using excuses to help navigate away from uncomfortable feelings that are ultimately necessary for your growth.\n",
      "\n",
      "HOW YOU RESOLVE THIS\n",
      "\n",
      "Start measuring your outcomes and focusing on at least doing one productive thing each day.\n",
      "\n",
      "It's  no  longer  about  how  many  days  you  really  wanted to go to the gym; it's about how many days you did. It's no longer about wanting to show up for your friends; it's whether  or  not  you  did.  It's  no  longer  about  the  great ideas  you  had  about  how  to  change  your  business;  it's about whether or not you did.\n",
      "\n",
      "Stop accepting your own excuses. Stop being complacent\n",
      "\n",
      "with your own justifications. Start quantifying your days by how many healthy, positive things you accomplished, and you will see how quickly you begin to make progress.\n",
      "\n",
      "DISORGANIZATION\n",
      "\n",
      "By leaving our lives and spaces in disarray, we are not just mindlessly  forgetting  to  take  care  of  our  surroundings. We are often actually creating distractions and chaos that serve an unconscious purpose.\n",
      "\n",
      "A clean, organized space-both for work and for livingis  essential  to  thriving. This means a tidy home, clothes that are easy to reach and put together each morning, a clean kitchen, and an organized desk. Paperwork should be filed in one space, your bedroom should be calming, and everything should have a 'home' that it can return to at the end of the day.\n",
      "\n",
      "Without  cleanliness,  we  create  fewer  opportunities  for ourselves.  Nothing  positive,  nor  beautiful,  flows  from chaos.  Deep  down,  we  know  this.  Often,  when  we  are self-sabotaging  through  disorganization,  it  is  because when we are very clean or organized, we get an uneasy feeling. That uneasy feeling is what we are trying to avoid, because it is the recognition that now that everything is in order, we must get to work on doing what we need to do or who we want to become.\n",
      "\n",
      "When we leave our spaces messy, we are always a few tasks or priorities away from stepping out and showing up.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "Like  anything,  you  need  to  start  slow  and  adjust  yourself over time. To declutter and reorganize, start with one room, and if that is too much, try one corner, drawer, or closet. Work on that, and only that, and then implement a routine that maintains the organization.\n",
      "\n",
      "From there,  start  arranging  your  space  so  that  it  works for you, not against you. Put something soothing on your bedside table like a diffuser, or create an organized family calendar in the kitchen so appointments and schedules are visible to others. If you have trouble with the mail being disorganized, create a spot for it to go when it comes in each day. If you have trouble with laundry being disorganized, create a system for it and decide on a day or two that you do the wash, and do it in bulk.\n",
      "\n",
      "You must slowly let yourself get used to working at a clean desk, and eventually it will become second nature. You'll begin to realize that you also feel so much less stressed and much more in control of your life.\n",
      "\n",
      "It is very hard to show up as the person you want to be when you are surrounded by an environment that makes you feel like a person you aren't.\n",
      "\n",
      "ATTACHMENT TO WHAT YOU DON'T REALLY WANT\n",
      "\n",
      "Sometimes, your dreams for your life are adopted from other people's preferences. In other cases, you determine what you want and then you outgrow your old ambitions.\n",
      "\n",
      "Sometimes, we fight endlessly to try to force ourselves to want something that we do not really want, and it always leaves us empty, because it isn't a genuine desire. This is different  than  lacking  motivation  or  experiencing  resistance. Our inability to perform is not based in fear or lack of skill, it is based in an inherent knowing that this is not what we want for our lives, and perhaps we're feeling lost or unable to change our path.\n",
      "\n",
      "When you find yourself struggling with something, you have to ask yourself: Do I actually want to do this? Do you want the job, or do you just like how the title sounds? Are you in love  with  the  person,  or  do  you  like  the  idea  of the relationship? Are you still holding an outdated idea of what your greatest success will be, and if so, what would it look like to let that go?\n",
      "\n",
      "At the end of the day, self-sabotage sometimes functions to show us that we aren't quite on the right path yet, and that we need to reevaluate to determine what would feel best for our lives, even if that means we disappoint some people or even our younger selves.\n",
      "\n",
      "We do not have to live the rest of our lives trying to achieve some measure of success we thought was ideal when we were  too  young  to  understand  who  we  even  were.  Our only responsibility is to make decisions for the person we have become.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "Be willing to accept that maybe your 'success story' doesn't look the way that you once thought it might.\n",
      "\n",
      "Maybe the kind of success you're really hungry for is to feel  at  peace  each  day,  or  making  your  life  about  travel instead of work. Maybe it's about having thriving friendships or a happy relationship. Maybe the business you got into 10 years ago isn't the business you want to be in forever. Maybe the work you thought you'd love isn't coming as naturally to you as you'd hoped.\n",
      "\n",
      "When we let go of what isn't right for us, we create space to discover what is. However, doing so requires the tremendous courage to put our pride aside and see things for what they really are.\n",
      "\n",
      "J U D G I N G   O T H E R S\n",
      "\n",
      "We all know that gossiping, or judging other people's lives and choices, is not a healthy or positive way to connect with other people. However, it does far more damage than we realize, as it sets up barriers to our own success.\n",
      "\n",
      "If  we  feel  bad  about  not  being  as  successful  as  another person,  we  might  try  to  find  something  negative  about them to make ourselves feel better. If we do that every time we come across a person who is more successful than we are,  we  begin  to  associate  that  level  of  success  with being disliked. When it comes time for us to take action to move our lives forward, we're going to resist doing it, because becoming more successful will create a breach in our self-concept.\n",
      "\n",
      "In other cases, you might have heard people you grew up around  villainizing  others  who  had  money. They  might have  said  things  like,  'Ugh,  rich  people  are  the  worst.' Maybe they chalked all wealthy people up to being morally corrupt. This sweeping characterization sealed itself in your subconscious, and now you find yourself sabotaging your own attempts to become financially healthy, because you associate it with guilt and being disliked.\n",
      "\n",
      "When we set up judgments for others, they become rules that we have to play by, too. By judging others for what we don't have or because we envy them, we sabotage our own lives far more than we ever really hurt anybody else.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "Many people say that you have to love yourself first before you can love others, but really, if you learn to love others, you will learn to love yourself.\n",
      "\n",
      "Practice non-judgment through non-assumption. Instead of  reaching  a  conclusion  about  a  person  based  on  the limited information you have about them, consider that you're not seeing the whole picture and don't know the whole story.\n",
      "\n",
      "When you are more compassionate about other people's lives,  you  become  more  compassionate about your own. When you see someone who has something  you  want, congratulate them, even if it feels hard at first. It will extend back and open you up to receiving it as well.\n",
      "\n",
      "PRIDE\n",
      "\n",
      "Pride is often involved in many of our worst decisions.\n",
      "\n",
      "Sometimes,  we  know  a  relationship  is  wrong,  but  the shame of leaving seems worse than staying. Sometimes, we start a business and realize we don't really like it very much or refuse to accept that we need to change or ask for help. In these cases, our pride is getting in the way. We are  making  decisions  based  on  how  we  imagine  people view our lives, not how they actually are. This is not only inaccurate, but it is also very unhealthy.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "To overcome our attachment to pride, we have to start to see ourselves more wholly and honestly.\n",
      "\n",
      "Instead  of  thinking  that  we  need  to  prove  to  everyone around us how perfect and flawless we are, we can imagine ourselves more realistically: as people who, despite our weaknesses, are trying our best.  In  the  end,  it  looks  far worse to hold onto what's wrong because you care about what others think than it is to let go because that's what's right for you. People will respect you far more if you can acknowledge that you are an imperfect person-like everyone else-learning, adapting, and trying your best.\n",
      "\n",
      "In  reaching  this  mindset,  you  also  open  yourself  up  to learning. By not assuming you know everything or that you  need  to  seem  perfect,  you  can  admit  when  you're wrong, ask for assistance, and lean on others sometimes. Basically, you open yourself back up to growth, and your life is better for it over the long term.\n",
      "\n",
      "GUILT OF SUCCEEDING\n",
      "\n",
      "In a world of so much pain, horror, and misfortune, who are we to have happy, abundant lives?\n",
      "\n",
      "That's the thought process that so many people go through. One of the biggest mental barriers people face is the guilt that comes with finally having enough or more than one needs. This can come from many different sources, but it ultimately boils down to feeling as though you 'don't deserve' to have it.\n",
      "\n",
      "This feeling often comes up when we start to earn more money or have nicer things. Often, people will sabotage their higher incomes with reckless discretionary spending or by being less vigilant about their clientele or workload, because they are not quite comfortable having more than the basic necessities, and so they put themselves back into a comfortable feeling of lack.\n",
      "\n",
      "When it comes to success, guilt is an unfortunately common  emotion,  especially  for  good-hearted  people  who want to do the right thing and live authentic lives.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "Please  realize  that  most  extremely  successful  people  have no guilt whatsoever. In fact, this feeling usually only comes up when you're stepping between not having enough and finally having enough.\n",
      "\n",
      "What you have to realize is that money and success are tools. They buy you back time and offer you the opportunity  to  help,  employ,  influence,  and  change  the  lives of  others.  Instead  of  looking  at  your  success  as  a  status differentiator-which will always make you feel bad and uncomfortable-see it instead as a tool with which you can  do  important  and  positive  things  in  the  world  and your own life.\n",
      "\n",
      "FEAR OF FAILING\n",
      "\n",
      "How often do we not even attempt something because we are afraid to look bad or fail immediately?\n",
      "\n",
      "The fear of failing is often something that holds people back from putting in the work they would need to become truly great at something, but it can also take another, more insidious form. Once we have established something new in our lives,  this  fear  can  come up as a constant irrational worry that  we're  'missing  something,'  that  our  partner  is  being unfaithful, or that we're one misstep away from losing it all.\n",
      "\n",
      "These  catastrophic  thoughts  happen  when  we  want  to shield ourselves from potential loss. They only come up when we finally have something we care enough about and really want to keep.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "There is a difference between failing because you are trying something new and daring, and failing because you are not showing up, doing the work, or being responsible for your actions.\n",
      "\n",
      "These  are  two  very  different  experiences  and  should  be separated in your mind.\n",
      "\n",
      "As scary as it might be to not be great at something initially, or perhaps even experience a loss, it is even worse\n",
      "\n",
      "to fail by virtue of never trying and always playing small. Failure is inevitable, but you have to make sure it's happening for the right reasons.\n",
      "\n",
      "When  we  fail  out  of  negligence,  we  take  a  step  back. When we fail because we are attempting new feats, we take one step closer to what will work.\n",
      "\n",
      "DOWNPLAYING\n",
      "\n",
      "When we downplay our successes in life, we are either trying to make ourselves seem less impressive so others do not feel threatened and therefore like us more, or we are trying to avoid the sense that we have 'made it,' because we are afraid of peaking.\n",
      "\n",
      "Though  so  many  of  us  long  for  the  moment  when  we feel  as  though  we  have  finally  arrived  and  achieved  the measures of success we so deeply desire, we often receive them only to then feel as though they aren't that great, impressive, or that they don't make us feel as good as we thought they would.\n",
      "\n",
      "This happens because of downplaying. The idea of having 'made it' makes us afraid that we are reaching the pinnacle and therefore will fall off of it. If we acknowledge that we've arrived, what goals remain? It is a feeling akin to death, so we instead find another measure to work toward.\n",
      "\n",
      "Likewise, when we are around other people, we do not stand firmly in our pride because we are taught it is a bad thing (and when done in an unhealthy way, it is). What we are sensing is the feeling of being 'better than' others because we have achieved something. This makes us uncomfortable because we know it's both untrue and unkind.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "We can all acknowledge and appreciate other people's diverse accomplishments and talents while still being happy about our own. Instead of shrugging off a compliment, we can respond by saying: 'Thank you, I worked very hard, and I'm happy to be here.'\n",
      "\n",
      "If the fear is that we are 'peaking' too soon, we have to reform our idea of progress. We do not get better only to get worse again. We do not achieve one thing only to lose it  and  return  to  what  we  were  before. That  instinct  is  a self-sabotaging behavior, one that wants to keep us within our old comfort zone.\n",
      "\n",
      "Instead, we can acknowledge that when one part of our life improves, it radiates out to everything else. When we achieve one thing, we are better equipped for the future. Life tends to gradually get better as we keep working on it;  it  only  gets  worse  if  we  accomplish  something  then shut down because we are intimidated by our own power.\n",
      "\n",
      "UNHEALTHY HABITS\n",
      "\n",
      "This is the most common way that people sabotage their own success: by maintaining habits that are actively keeping them away from their goals.\n",
      "\n",
      "This  is  when  someone  declares  that  they  want  to  be  in better shape but doesn't change anything they do each day to facilitate that. Or when they want to make a change professionally but find ways to make it difficult if not impossible for them to actually do it.\n",
      "\n",
      "At the core of all these behaviors is the fact that one part of our psyche understands that we should be evolving and moving forward with our lives and another part is intimidated by the potential discomfort it would bring. Usually, this culminates in so much inner tension and frustration that a breaking point is reached, and changes are made from there.\n",
      "\n",
      "However, the goal is to not have to get to a crisis point in your life before you can become aware of the ways you're holding yourself back from living peacefully and comfortably.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "Define health on your own terms. What does a healthy life look like for you? How would it make you feel, and what would you be doing?\n",
      "\n",
      "It is difficult to look solely to anyone else's definition of healthfulness,  particularly  because  we  are  all  different people with varying needs, preferences, and schedules.\n",
      "\n",
      "Instead, figure out what makes you feel best. Decide what combination of healthy eating, exercise, and sleep is right for you, and stick to it. Like so many things, healthy habits are best established gradually. Instead of trying to force yourself to take an hour at the gym at 6 AM, try instead to do 15 minutes, or perhaps swap out with a class you really enjoy, or go at a time that works better for your schedule.\n",
      "\n",
      "Make it easy for yourself to succeed. Prep your meals or keep water by your desk so you can sip it throughout the day. Gradually recondition yourself to prefer healthy habits, ones that actually work for your lifestyle.\n",
      "\n",
      "BEING 'BUSY'\n",
      "\n",
      "Another  very  common  way  that  people  sabotage  is  by distracting  themselves  to  the  point  of  being  completely phased out of their lives.\n",
      "\n",
      "People  who  are  constantly  'busy'  are  running  from themselves.\n",
      "\n",
      "Nobody is 'busy' unless they want to be busy, and you will know that because so many people with extremely hectic schedules would never describe themselves that way. This\n",
      "\n",
      "is because being 'busy' is not a virtue; it only signals to others that you do not know how to manage your time or your tasks.\n",
      "\n",
      "Being busy communicates importance; it often makes you seem a little untouchable to others. It also overwhelms the body so that it can only focus on the tasks at hand. Being busy is the ultimate way to distract ourselves from what's really wrong.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "If your schedule is unmanageable, you're never going to be as effective or productive as you could be. If this is the case, your first job has to be to streamline and prioritize your  tasks  in  order  of  importance,  outsource  whatever else you can, and then let go of the rest.\n",
      "\n",
      "If your issue is that you intentionally create chaos and busy-ness in your day when there is no need for it, you have  to  get  comfortable  with  simplicity  and  routine. Start with writing down your top 5 tasks that need to be done each day, and then focus on doing those and only those.\n",
      "\n",
      "You might also need to confront the sense of 'protection' that  being  busy  gives  you.  Does  it  make  you  feel  more important than others? Does it give you an excuse to say 'no' to plans or to avoid some people? You need to find healthier  and  more  productive  ways  to  cope  with  these\n",
      "\n",
      "feelings, such as finding genuine self-confidence in what you do by creating something you're proud of, or getting better at calmly but clearly stating your boundaries and needs in relationships.\n",
      "\n",
      "SPENDING TIME WITH THE WRONG PEOPLE\n",
      "\n",
      "It's true that so much of our lives is shaped by the people we spend them with, and the company you keep is another common way that people self-sabotage.\n",
      "\n",
      "Certainly you can think of some people in your life who stress you out, make you feel insecure, and yet keep you coming  back  for  more.  These  relationships  exist  at  the lighter end of the toxicity spectrum, but they are self-defeating nonetheless.\n",
      "\n",
      "If you find yourself preoccupied with a certain friendship or relationship that is making you feel almost addicted to the feeling of being 'less than' or 'jealous of,' you need to gradually phase out of it. You don't need to be mean, rude, or even cut anyone out of your life.\n",
      "\n",
      "You do, however, need to understand that the people you spend the most time with will shape your future irrevocably, and so you must choose them wisely.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "Work on building a circle of people who support and inspire you, who have similar goals and enjoy spending time with you. You should leave a get-together feeling energized and inspired, not exhausted and angry.\n",
      "\n",
      "It takes time to find your group of friends, and you may not discover that all at once. It could start with offering to take someone you admire out for coffee, or reaching out to do something with a person with whom you'd like to reconnect. Slowly but genuinely rebuild your connections, and then foster and care for them as much as you can.\n",
      "\n",
      "WORRYING ABOUT IRRATIONAL FEARS AND LEAST LIKELY CIRCUMSTANCES\n",
      "\n",
      "Another  very  common  way  that  people  sabotage  without realizing is by preoccupying themselves with fears of worst-case scenarios.\n",
      "\n",
      "You're probably familiar with this, at least to some degree: You have a weird or highly unlikely thought that evokes a deep sense of dread, fear, and series of 'doomsday' scenarios in your head. You then keep coming back to it to the point that it even controls some part of your life.\n",
      "\n",
      "Irrational fears, especially the kind that are least likely to become reality, are often what we project real fears onto.\n",
      "\n",
      "These irrational fears are safe, because deep down we know they aren't going to happen. They are placeholders, a way for us to express the feeling we really have onto something we know isn't going to happen.\n",
      "\n",
      "When  you  find  yourself  in  a  fear  cycle,  constantly  repeating  some  strange,  random,  or  unimportant  one-off circumstance or situation that has a very low probability of becoming reality, ask yourself if you have any feelings about something related that is actually valid.\n",
      "\n",
      "For example, if you get anxious about being a passenger in a car, consider if your fear is of 'moving forward' or being 'out of control.' Or, if you're anxious about being fired from your job, the fear might really be the idea that you aren't worthy of another job or being humiliated by a higher-up.\n",
      "\n",
      "HOW TO RESOLVE THIS\n",
      "\n",
      "Instead  of  wasting  all  of  your  energy  trying  to  control some  wo st-case  scenario,  consider  what  the  message r of  the  fear  may  be  and  what  it  is  telling  you  that  you need in your life.\n",
      "\n",
      "If  the  fear  was  an  abstract  metaphor,  what  would  the meaning be? Is the abrupt loss of income a symbol of your desire for security? Is the fear of the future a symbol for not living fully right now? Is the anxiety about making decisions a symbol for knowing what you really want and being too afraid to choose it?\n",
      "\n",
      "At the core of the things we most fear is a message that we are trying to send ourselves about what we really care about. If we can identify what we want to protect, we can find healthier and more secure ways to do it.\n",
      "\n",
      "HOW TO TELL IF YOU'RE IN A SELF-SABOTAGE CYCLE\n",
      "\n",
      "Even  if  you  can  cognitively  understand  self-sabotaging behaviors, sometimes the most difficult shift is recognizing that we are engaging in them.\n",
      "\n",
      "In fact, sometimes the signs are so subtle, they are barely recognizable and often don't come to our attention until they become highly problematic or someone else points them  out.  Some  of  the  most  prominent  symptoms  of self-sabotage are as follows:\n",
      "\n",
      "YOU ARE MORE AWARE OF WHAT YOU DON'T WANT THAN WHAT YOU DO.\n",
      "\n",
      "You spend more of your time worrying, ruminating, and focusing on what you hope doesn't happen than you do imagining, strategizing, and planning for what you do.\n",
      "\n",
      "YOU SPEND MORE TIME TRYING TO IMPRESS PEOPLE WHO DON'T  LIKE  YOU  THAN  YOU  SPEND  WITH  PEOPLE  WHO LOVE YOU FOR WHO YOU ARE.\n",
      "\n",
      "You are more focused on growing into the kind of person who evokes the envy of your supposed enemies rather than the kind of person who is beloved by their family and friends and prioritizes them no matter what.\n",
      "\n",
      "YOU'RE PUTTING YOUR HEAD IN THE SAND.\n",
      "\n",
      "You  don't  know  basic  facts  about  your  life,  like  how much debt you have or what other people in your field are being paid for similar work. When you get into an argument,  you  run  away  until  you  forget  rather  than talking about what's wrong and coming up with a solution. In other words, you are in denial, and so any hope of healing is futile.\n",
      "\n",
      "YOU  CARE  MORE  ABOUT  CONVINCING  OTHER  PEOPLE YOU'RE OKAY THAN ACTUALLY BEING OKAY.\n",
      "\n",
      "You'd rather post photos that make it look like you had a great time than being concerned about whether you actually had a good time. You put more effort toward trying to convince everyone you're doing well rather than being honest  and  connecting  with  people  who  could  help  or support you.\n",
      "\n",
      "YOUR  MAIN  PRIORITY  IN  LIFE  IS  TO  BE  LIKED,  EVEN  IF THAT COMES AT THE EXPENSE OF BEING HAPPY.\n",
      "\n",
      "You think more about whether or not your actions will earn you the approval of 'people' (who are 'people,' by the\n",
      "\n",
      "way?) rather than whether or not they will actually make you feel fulfilled and content with who you are.\n",
      "\n",
      "YOU'RE MORE AFRAID OF YOUR FEELINGS THAN ANYTHING ELSE.\n",
      "\n",
      "If you get to the point in life at which the scariest, most detrimental thing you face is the fear of whether or not you will be able to handle your own emotions, you are the one standing in your own way-nothing else is.\n",
      "\n",
      "YOU'RE BLINDLY CHASING GOALS WITHOUT ASKING YOURSELF WHY YOU WANT THOSE THINGS.\n",
      "\n",
      "If you are doing 'everything you are supposed to be doing' and yet you feel empty and depressed at the end of the day, the issue is probably that you're not really doing what you want to be doing; you've just adopted someone else's script for happiness.\n",
      "\n",
      "YOU'RE  TREATING  YOUR  COPING  MECHANISMS  AS  THE PROBLEM.\n",
      "\n",
      "Instead of trying to incite war on yourself to overcome your  overeating,  spending,  drinking,  sexing-whatever it  is  you  know you need to improve-ask yourself what emotional need that thing is filling. Until you do, you will battle it forever.\n",
      "\n",
      "YOU VALUE YOUR DOUBT MORE THAN YOUR POTENTIAL.\n",
      "\n",
      "Negativity  bias  makes  us  believe  that  'bad'  things  are more real than good, and unless we keep that inclination in check, it can leave us believing that everything we fear to be true is more real than the good things that are actually true.\n",
      "\n",
      "YOU ARE TRYING TO CARE ABOUT EVERYTHING.\n",
      "\n",
      "Your  willpower  is  a  limited  resource.  Y ou  only  have  so much in a day. Rather than using it to try to become good at  everything,  decide  what  matters  most  to  you.  Focus your attention on that, and let everything else slip away.\n",
      "\n",
      "YOU ARE WAITING FOR SOMEONE ELSE TO OPEN A DOOR, OFFER  APPROVAL,  OR  HAND  YOU  THE  LIFE  YOU  HAVE BEEN WAITING FOR.\n",
      "\n",
      "We grow up with the illusion that success is what's handed to people who are most deserving, talented, or privileged. When we arrive, however, we realize it is constructed by those who find an intersection of their interests, passions, skills,  and  a  market  gap.  Sprinkle  on  a  little  bit  of  persistence, and the only way to fail is to give up.\n",
      "\n",
      "YOU DON'T REALIZE HOW FAR YOU'VE COME.\n",
      "\n",
      "You are not the person you were five years ago. You evolve as your self-image does, so make sure that it's an accurate one. Give yourself credit for everything you've overcome that you never thought you would, and everything you've\n",
      "\n",
      "built that you never thought you could. You've come so much farther than you think, and you're so much closer than you realize.\n",
      "\n",
      "I D E N T I F Y I N G   Y O U R   S U B C O N S C I O U S COMMITMENTS\n",
      "\n",
      "Part of the reason we often experience intense inner conflict or self-sabotage is because of something called a core commitments, which is essentially your primary objective or intention for your life. 5\n",
      "\n",
      "Your subconscious commitments are basically what you want more than anything else, and you often aren't even aware of them. You can identify your core commitments by looking at the things that you struggle with most and the things you are most driven by. If you can peel back the layers of your motivations toward each, you'll find a root cause. When you find the same root cause for everything, you've found a core commitment.\n",
      "\n",
      "People only seem irrational and unpredictable until you understand what they are fundamentally committed to.\n",
      "\n",
      "For example, if someone has a core commitment to feel free,  they  may  find  themselves  sabotaging  work  opportunities in order to achieve that. If someone's core commitment  is  to  feel  wanted,  they  could  find  themselves in  a  series  of  relationships  in  which  they  have  intense\n",
      "\n",
      "connections but refuse to make commitments out of fear that the spark will 'fade.' If someone's core commitment is to be in control of their lives, they might have irrational anxiety about things that represent a loss of control. If someone's core commitment is to be loved by others, they might pretend to be helpless in certain areas of life because if they don't need others, they might be left by them.\n",
      "\n",
      "But the most important thing to understand is that your core commitments are actually a cover-up for core needs. Your core need is the opposite of your core commitment. Your core need is also another way to identify your purpose. For example, if your subconscious core commitment is to be in control, your core need is trust. If your subconscious core commitment is to be needed, your core need is to know you are wanted. If your subconscious core commitment is to be loved by others, your need is self-love.\n",
      "\n",
      "The less that you feed your core need, the 'louder' your core commitment symptoms will be.\n",
      "\n",
      "If you are a person who needs trust and is therefore committed to staying in control, the less that you believe you are supported, the more your negative coping mechanisms are  going  to  flare  up.  Perhaps  this  could  happen  in  the form of disruptive  eating  patterns,  isolating  yourself,  or hyper-fixation on physical appearance. If you are committed to freedom and therefore need a sense of autonomy,\n",
      "\n",
      "the less that you build a life on your own terms, the more you are going to sabotage opportunities and feel drained and exhausted when you 'should' feel happy.\n",
      "\n",
      "The more you lean into fulfilling your core needs, the more your commitment symptoms will disappear.\n",
      "\n",
      "Once  you  understand  what  a  person  really  wants,  you will be able to explain the intricacies of their habits and behaviors. You will be able to predict down to the detail what they will do in any given situation. More importantly,  once  you  start  asking  yourself  what  you  really  want, you'll be able to stop battling the symptoms and start addressing the only issue that has ever really existed in your life, which is living out of alignment with your core needs and, therefore, your core purpose.\n",
      "\n",
      "CONFRONTING REPRESSED EMOTIONS AND TAKING ACTION\n",
      "\n",
      "There  is  a  difference  between  understanding  why  we self-sabotage and the act of no longer self-sabotaging.\n",
      "\n",
      "This means that once we understand the root and purpose of the behavior, we adjust it. We adapt. Overcoming self-sabotage is not just a matter of understanding why you're holding yourself back; it is being able to take action in the direction that you want and need to, even if it is initially uncomfortable or triggering.\n",
      "\n",
      "This is a very important part of the process, because you are essentially going to be confronting the exact emotions you have been trying to avoid.\n",
      "\n",
      "When you stop engaging in self-sabotaging behavior, repressed emotions that you weren't even aware of will start to come up, and you might feel even worse than you did before.\n",
      "\n",
      "The thing about overcoming self-sabotage is that we don't often need to be told what to do. We know what we want to do, and we know what we need to do. It is simply that we are being held back by our fear of feeling. To begin to  unravel this emotional holding pattern, we can work through the following to find more ease and space and freedom while we change our lives.\n",
      "\n",
      "THE MOST COMMON EMOTIONS THAT ARISE WHILE YOU'RE BREAKING SELF-SABOTAGING BEHAVIORS\n",
      "\n",
      "The first  feeling  you  are  likely  to  confront  is  resistance. This is that generalized sense of being 'stuck' or your body feeling so tense that it is almost 'hard,' as though you are hitting a wall. This feeling is usually a masking emotion that is preventing you from actually being aware of the sensations beneath it which are more acute.\n",
      "\n",
      "When you start to feel resistance, you don't want to just 'push through it.' In fact, trying to do that means you'll\n",
      "\n",
      "keep hitting the same wall that you're up against already. You're  going  to  strengthen  the  self-sabotaging  behavior because you aren't really solving the problem by just trying to override it.\n",
      "\n",
      "Instead, start asking the right questions.\n",
      "\n",
      "Why do I feel this way?\n",
      "\n",
      "What is this feeling trying to tell me about the action I am trying to take?\n",
      "\n",
      "Is there something I need to learn here?\n",
      "\n",
      "What do I need to do to honor my needs right now?\n",
      "\n",
      "Then you have to reconnect to your inspiration or your vision for life. Get clear on why you want to take this action  and  make  a  change.  When  your  motivation  is  the fact that you want to live a different and better existence, you're going to find that a lot of the resistance fades because you're being pushed by a vision that's greater than your fear.\n",
      "\n",
      "In other cases, you might run into other emotions such as  anger,  sadness,  or  inadequacy.  When  those  feelings come up, it is very important to make space for them. This means to allow them to rise up in your body and observe them. Watch where they make you tense up or constrict. Feel what they want you to feel. There is nothing worse\n",
      "\n",
      "than the fear of feeling the emotion, as the experience itself is ultimately often just some physical tension around which we've crafted a story.\n",
      "\n",
      "Remember that a lot of these feelings may very well have a root in something related to the self-sabotaging behavior. If you are angry about how one of your parents treated you, it probably won't come as a surprise that the core feeling of why you are sabotaging your relationships is anger and  mistrust. The  feelings  associated  with  self-sabotage are not usually random. In fact, they can lead us to deeper insights  about  what  we  really  need  and  what  problems within us are still unresolved.\n",
      "\n",
      "To fully release those feelings once you are aware of them, try  writing  yourself  a  letter.  Write  something  to  your younger self or from the perspective of your future self. Write  down  a  mantra  or  a  manifesto.  Remind  yourself that you love yourself too much to settle for less, or that it is okay to be angry in unfair or frustrating circumstances. Give yourself space to experience the depth of your emotions so that they do not control your behaviors.\n",
      "\n",
      "DISCONNECTING ACTION AND FEELING\n",
      "\n",
      "The final and most important lesson to overcome self-sabotage is to learn to disconnect action from feeling.\n",
      "\n",
      "We are not held back in life because we are incapable of making change. We are held back because we don't feel like making change, and so we don't.\n",
      "\n",
      "The truth is that you can have a vision of what you want, know that it is undoubtedly right for you, and simply not feel like taking the action required to pursue that path.\n",
      "\n",
      "This is because our feelings are essentially wired as comfort systems. They produce a 'good' feeling when we are doing what we have always done-staying in familiarity. This, to our bodies, registers as 'safety.' In other cases, the accomplishments or changes that we are very happy about are those that we also perceive to offer us a greater measure  of  safety.  If  the  achievement  potentially  puts  us  at risk in any way or exposes us to something unfamiliar, we aren't going to be happy about it initially, even if it is a net positive for our lives.\n",
      "\n",
      "However, we can actually train ourselves to prefer behaviors that are good for us. This is how we restructure our comfort zones. We begin to crave what we repeatedly do, but the first few times we do it, we often feel uncomfortable. The trick is being able to override that initial hesitation so we are guiding our lives with logic and reason, not emotionality.\n",
      "\n",
      "Though your emotions are always valid and need to be validat ed, they are hardly ever an accurate measure of what you are capable of in life. They are not always an accurate\n",
      "\n",
      "reflection of reality. All your feelings know is what you've done in the past, and they are attached to what they've drawn comfort from.\n",
      "\n",
      "You may feel as though you are worthless, but you most certainly are not. You may feel as though there is no hope, but there most certainly is. You may feel as though everyone dislikes you, but that is probably a gross overexaggeration. You may think everyone is judging you, but that is a misperception.\n",
      "\n",
      "Most importantly, you may feel as though you cannot take action, when you most certainly can. You simply do not feel willing, because you are not used to it.\n",
      "\n",
      "By using logic and vision to guide ourselves, we are able to  identify  a  different  and  better  life  experience.  When we imagine this, we feel peaceful and inspired. To rise up to meet this version of our lives, we must overcome our resistance and discomfort. We will not feel happy initially, no matter how 'right' for us those actions are.\n",
      "\n",
      "It is essential that you learn to take action before you feel like doing it. Taking action builds momentum and creates motivation. These feelings will not come to you spontaneously; you have to generate them. You have to inspire yourself, you have to move. You have to simply begin and allow your life and your energy to reorient itself to prefer the behaviors that are going to move your life forward, not the ones that are keeping you held back.\n",
      "\n",
      "CHAPTER 3\n",
      "\n",
      "YOUR TRIGGERS ARE THE GUIDES TO YOUR FREEDOM\n",
      "\n",
      "NOW THAT YOU HAVE BEGUN to  identify  your self-sabotaging behaviors, you can use them to uncover deeper and more important truths about who you are as a person and what you really want and need out of life.\n",
      "\n",
      "This is an important part of the process, because overcoming  our  self-defeating  habits  is  not  just  about  knowing what they are or why we engage in them. It is also about better understanding what our inherent needs are, what we really desire, and how we can use this as a pivot point to begin building a life that is aligned with who we really are and what we are here to do.\n",
      "\n",
      "Our triggers do not actually exist just to show us where we are storing unresolved pain. In fact, they show us something much deeper.\n",
      "\n",
      "Each 'negative' emotion we experience comes with a message, one that we do not yet know how to interpret. This is\n",
      "\n",
      "when a single challenge begins to become a chronic issue. Unable to honor and use the guidance of the emotion, we shut the feeling down, store it in our bodies, and try to avoid anything that might bring it up again. This is when we become sensitive to the world around us, because there are a lot of repressed feelings mounting.\n",
      "\n",
      "On the surface, it seems as though the thing that triggers our emotional response is the problem. It is not. The problem is that we don't know what to do with how we feel and therefore do not have all of the emotional processing skills that we need.\n",
      "\n",
      "When we can identify why something is triggering us, we can use the experience as a catalyst for a release and positive life change.\n",
      "\n",
      "HOW TO INTERPRET NEGATIVE EMOTIONS\n",
      "\n",
      "Though everyone's particular triggers are unique to them, it helps to better understand the function of some of the feelings that we often condemn.\n",
      "\n",
      "Some of the emotions that are most strongly connected with self-sabotaging behaviors are actually important for us  to  better  understand.  It  is  not  about  simply 'getting over' them; it is about listening to what they are trying to tell us about our experience.\n",
      "\n",
      "ANGER\n",
      "\n",
      "Anger is  a  beautiful,  transformative  emotion.  It  is  mischaracterized by its shadow side, aggression, and therefore we try to resist it.\n",
      "\n",
      "It is healthy to be angry, and anger can also show us important aspects of who we are and what we care about. For example, anger shows us where our boundaries are. Anger also helps us identify what we find to be unjust.\n",
      "\n",
      "Ultimately, anger is trying to mobilize us, to initiate action.  Anger  is  transformative,  and  it  is  often  the  peak state  we  reach  before  we  truly  change  our  lives.  This is  because  anger  is  not  intended  to  be  projected  onto someone  else;  rather,  it's  an  influx  of  motivation  that helps us change what we need to change within our lives. When we do not see it as such, we tend to bury it, not ever resolving the real issue at hand. This is when anger starts to cross over into aggression-when we take that energy out on those around us as opposed to using it as an impetus to change ourselves.\n",
      "\n",
      "Instead of being afraid of anger, we can instead use it to help us see our limits and priorities more clearly. We can also use it to help us make big, important changes both for ourselves and the world around us.\n",
      "\n",
      "SADNESS\n",
      "\n",
      "Sadness is the normal and correct response to the loss of something you very much love.\n",
      "\n",
      "This is an emotion that often comes up in the aftermath of a disappointment. This could be the loss of a relationship, a job, or just a general idea of what you thought your life would be.\n",
      "\n",
      "Sadness only becomes problematic when we do not allow ourselves to go through the natural phases of grief. Sadness does not release itself all at once. In fact, we often find that it happens in waves, some of which strike us at unexpected times.\n",
      "\n",
      "We do not ever need to feel embarrassed or wrong for needing to cry, feel down, or miss what we no longer have. In fact,  crying  at  appropriate  times  is  one  of  the  biggest signs of mental strength, as people who are struggling often find it difficult to release their feelings and be vulnerable.\n",
      "\n",
      "GUILT\n",
      "\n",
      "Guilt tends to affect us more for what we didn't do than what we did. In fact, people who struggle the most with guilt are the people who are not actually guilty of something terrible. People who commit heinous acts tend to not feel much remorse. The fact that you feel bad that you\n",
      "\n",
      "could have done wrong by someone is a good sign in itself.\n",
      "\n",
      "However, guilt requires us to look deeply at what behaviors, if any, we feel badly about, as well as what we may have done that was not in our best interest. If we have treated others unfairly, we must be able to admit, apologize, and correct that behavior. However, if the feeling of guilt is more generalized and not specifically relating to  any  one  incident,  we  need  to  look  closely  at  who  or what made us always feel as though we were 'wrong' or inconveniencing others.\n",
      "\n",
      "Guilt is often an emotion we carry from childhood and then project onto current circumstances when we felt as though we were burdens to those around us.\n",
      "\n",
      "EMBARRASSMENT\n",
      "\n",
      "Embarrassment is what we feel when we know that we did not behave in a way that we are proud of.\n",
      "\n",
      "Other people can never make us feel as embarrassed as we make ourselves feel. When you are truly and completely confident that you are doing the best you can with what you have in front of you, you stop feeling embarrassed all the time. Sure, others can make you feel bad with their comments or ideas, but even their worst judgments tend to be neutralized when we accept ourselves and feel proud of who we are.\n",
      "\n",
      "Shame  is  the  shadow  side  of  embarrassment.  This  is when  the  natural,  occasional  feeling  of  being  embarrassed  turns  into  a  way  for  us  to  completely  condemn ourselves as human beings and begin to see ourselves as worthless and invalid.\n",
      "\n",
      "When we do not process the feeling of embarrassment, it tends to turn into something far darker.\n",
      "\n",
      "J E A L O U S Y\n",
      "\n",
      "Jealousy is a cover-up emotion. It presents as anger or judgment, when in reality it is sadness and self-dissatisfaction.\n",
      "\n",
      "If you want to know what you truly want out of life, look at  the  people  who  you  are  jealous  of.  No,  you  may  not want exactly what they have, but the feeling that you are experiencing is anger that they are allowing themselves to pursue it while you are not.\n",
      "\n",
      "When we use our jealousy to judge other people's accomplishments, we are siding into its shadow function. When we  use  our  jealousy  to  show  us  what  we  would  like  to accomplish, we begin to recognize the self-sabotaging behavior and get ready to commit to what we actually desire.\n",
      "\n",
      "You can think of it this way: When we see someone who has something we really want but we are suppressing our willingness to pursue it, we must also condemn it in them\n",
      "\n",
      "so we can justify our own course of action. Instead of this, we can see what we'd also like to create.\n",
      "\n",
      "RESENTMENT\n",
      "\n",
      "When we resent people, it is often because they did not live up to the expectation of them that we had in our minds.\n",
      "\n",
      "Resentment in some ways is like a projected regret. Instead  of  trying  to  show  us  what we should  change,  it seems to want to tell us what other people should change. However, other people are under no obligation to live up to our ideas of them. In fact, our only problem is that we have an unrealistic expectation that someone was meant to be exactly as we think they should or love us exactly as we imagined they would.\n",
      "\n",
      "When  we  are  faced  with  resentment,  what  we  instead must do is reinvent our image of those around us or those we have perceived as having wronged us. Other people are not here to love us perfectly; they are here to teach us lessons to show us how to love them-and ourselves-better.\n",
      "\n",
      "When we release the ideas we have about who they should be, we can see them for who they are and the role they are meant to play in our lives. Instead of focusing on how they should change, we can focus instead on what we can learn.\n",
      "\n",
      "REGRET\n",
      "\n",
      "Much like jealousy, regret is also another way that we show ourselves  not  what  we  wish  we  could  have  done  in  the past, but what we absolutely need to create going forward.\n",
      "\n",
      "The truth  is  that  most  people  regret  what  they  did not do more than they ever regret what they did. This isn't a coincidence. Regret isn't actually trying to just make us feel bad that we didn't live up to our own expectations. It is trying to motivate us to live up to them going forward. It is trying to show us what it is absolutely imperative to change in the future and what we really care about experiencing before we die.\n",
      "\n",
      "Didn't  travel  when  you  were  young?  Regret  is  showing you that you should do it now. Didn't look as nice as you wanted  to?  Regret  is  showing  you  that  you  should  try harder. Made choices that didn't reflect your best self? Regret is showing you that you should make different ones now. Didn't love someone while you had them? Regret is showing you that you should appreciate people now.\n",
      "\n",
      "CHRONIC FEAR\n",
      "\n",
      "When we cannot stop returning to fearful thoughts, it is not  always  because  there  is  an  actual  threat  in  front  of us. Often, it is because our internal response systems are underdeveloped or sidelined by trauma.\n",
      "\n",
      "When  we  are  in  a  state  of  fearful  thinking,  it  doesn't matter what we are afraid of; the thought process follows us from problem to problem. Often, there's a metaphor encoded within it. For example, we may be afraid of an ultimate 'loss of control' or some external force coming in and unraveling our progress.\n",
      "\n",
      "Regardless,  chronic  fearful  thinking  often  comes  back down to feeling the need to focus our energy and attention on a potential threat so we can protect ourselves from it. We imagine that if we are worried, anxious, or angry about it, it will remain within our awareness and therefore cannot surprise us. We can retain some control over it.\n",
      "\n",
      "The very act of holding these fearful thoughts within our minds is exactly how the fear is controlling us in the first place.  It  is  derailing  our  lives right now, because we are channeling our energy into something that is outside of our control, as opposed to using it for everything that is actually within our control-the habits, actions, and behaviors that would actually move our lives forward.\n",
      "\n",
      "In this sense, what we are afraid of is really a projection of what's already happening.\n",
      "\n",
      "The only true way to get over chronic fear is actually to get through it. Instead of trying to battle, resist, and avoid what we cannot control, we can learn to simply shrug and say, and if that happens, it happens. The second we are able to shrug, laugh, or even just throw our hands up and say,\n",
      "\n",
      "'Whatever, it will be fine,' we instantly take back all of our power.\n",
      "\n",
      "What keeps the fire of fear raging is the idea that if we accept what we are afraid of, we are giving in to the worst potential outcome. The truth is that when we stop being afraid of what we cannot control and know instead that nothing can possibly ruin our lives more than we are ruining  them  with  our  negative,  distracted,  and  irrational thinking and focus, we are completely freed.\n",
      "\n",
      "When we are in full acceptance, fear leaves our consciousness and becomes a non-issue. It is at this point we realize that it always was.\n",
      "\n",
      "OUR INTERNAL GUIDANCE SYSTEMS WHISPER UNTIL THEY SCREAM\n",
      "\n",
      "The things that are bothering you most right now are not external forces trying to torture you for the sake of itthey are your own mind identifying what in your life can be fixed, changed, and transformed . If you continue to not take action, the siren will only get louder, and if you never learn  to  listen  to  it,  you  will  probably  just  disassociate from it and then be a victim to it.\n",
      "\n",
      "You  already  have  the  answers.  You  already  know  what you're here to do. You are here to create everything that would make you happier than you can imagine. It is only\n",
      "\n",
      "a matter of quieting your mind enough so you can feel all of the unlimited potential that is begging you to be used.\n",
      "\n",
      "There's no such thing as self-sabotage because the behaviors  that  you  think  are  holding  you  back  are  really  just meeting your needs. It's not a matter of trying to push yourself  beyond  them;  it's  a  matter  of  seeing  them  for what they are and then finding better, healthier ways to fulfill them.\n",
      "\n",
      "Though we live in an age where people tend to tell us that we should be entirely self-sufficient, and to want or need another person's presence, validation, or company is a sign of  self-insufficiency,  that  is  not  an  accurate  portrayal  of what it means to be human, and it severely overlooks the reality of human nature and connection.\n",
      "\n",
      "Though many people are codependent and lean far too heavily on others to give them a sense of safety and self, leaning  too  far  the  other  way-where  you  believe  that you don't need anyone or anything and that you can do everything yourself-is not healthy, either. They are two opposite manifestations of the same wounds, which are mistrust and the inability to connect.\n",
      "\n",
      "Your need to feel validated is valid.\n",
      "\n",
      "Your need to feel the presence of another person is valid.\n",
      "\n",
      "Your need to feel wanted is valid.\n",
      "\n",
      "Your need to feel secure is valid.\n",
      "\n",
      "Often,  the  first  reason  we  start  neglecting  our  essential needs is because we think we are weak for having them. We only believe this because when we were young we did have to rely almost entirely on others to meet our essential needs. Eventually, this fails us, because another person cannot fulfill us entirely, nor are they responsible for it. As we grow up, we learn self-sufficiency. In fact, reliance on oneself for the foundation of our basic needs is an important part of a person's development.\n",
      "\n",
      "In the same way, it is also important that we recognize we cannot meet every single one of our needs on our own.\n",
      "\n",
      "Human beings are hardwired for connection to others and to a group. This is why we exist in subsets, like communities, and families, and generally feel happiest and most fulfilled when we are doing things that serve the greater good. This is a fundamental and healthy part of who we are, and it is not a sign of weakness.\n",
      "\n",
      "In  other  cases,  your  need  to  feel  financially  secure  is healthy;  it  is  not  always  a  product  of  you  being  greedy or ill-intentioned. Your need to be validated for the work that you do is healthy, and it is not always a product of you being vain. Your need to live in a space and area that you enjoy being in is healthy, and it is not always a product of you being ungrateful for what you have.\n",
      "\n",
      "YOUR SUBCONSCIOUS MIND IS TRYING TO COMMUNICATE WITH YOU\n",
      "\n",
      "Within our self-sabotaging behaviors lies incredible wisdom. Not only can they tell us how and what we have been traumatized by, they can also show us what we really need. Embedded within each self-sabotaging behavior is actually the key to unlock it, if only we can understand it first.\n",
      "\n",
      "These are a few brief examples of how your subconscious mind might be trying to communicate with you through your behaviors.\n",
      "\n",
      "THE WAY YOU ARE SELF-SABOTAGING: Going back to the same person who broke you in a relationship. This could be a platonic friend but is most commonly a former romantic partner.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU  TO KNOW: It could be time to evaluate your childhood relationships. If you find something comforting or appealing about someone who hurts you, there's usually a reason.\n",
      "\n",
      "THE  WAY  YOU  ARE  SELF-SABOTAGING: Attracting people who are too broken to commit in a real way.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU  TO KNOW: You are not too broken to find someone who actually wants you, and when you begin to recognize that you\n",
      "\n",
      "are  worthy of being committed to, you'll start choosing partners who do just that.\n",
      "\n",
      "THE  WAY  YOU  ARE  SELF-SABOTAGING: Feeling  unhappy, even if nothing is wrong, and really, you've gotten everything you've wanted in life.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU  TO KNOW: You are probably expecting outside things to make you feel good rather than relying on changing how you think  and  what  you  focus  on.  No  outward  accomplishment is going to give you a true and lasting sense of inner peace,  and  your  discomfort,  despite  your  accomplishments, is calling your attention to that.\n",
      "\n",
      "THE  WAY  YOU  ARE  SELF-SABOTAGING: Pushing  people away.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU  TO KNOW: You want people to love and accept you so much that the stress of it all makes you isolate yourself from the pain, effectively creating the reality you're trying to avoid. Alternatively,  needing  solitude  too  often  usually  means there is a discrepancy between who you pretend to be and who you actually are. When you show up to your life more authentically, it becomes easier to have people around you, as it requires less effort.\n",
      "\n",
      "THE WAY YOU ARE SELF-SABOTAGING: Automatically believing what you think and feel is true.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU  TO KNOW: You want to worry because it feels comfortable, and therefore safer. The more you blindly trust every random thought or feeling that passes through you, the more you are going to be at the whim of what's happening around you. You must learn to steady yourself in clarity, truth, and groundedness, and to be able to mentally discern between what is helpful and what is not.\n",
      "\n",
      "THE  WAY  YOU  ARE  SELF-SABOTAGING: Eating poorly when you don't want to.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU TO  KNOW: You are doing too much, or you're not giving yourself  enough  rest  and  nourishment.  You  are  being too extreme. This is why your body is requiring that you continue to fuel it. Alternatively, it could be that you are emotionally hungry, and because you are not giving yourself the true experiences you crave, you are satisfying your 'hunger' another way.\n",
      "\n",
      "THE WAY YOU ARE SELF-SABOTAGING: Not doing the work you know would help move your career forward.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU  TO KNOW: You might not be as clear  as  you  think  you  are on what you want to be doing. If it isn't flowing, there is  a  reason.  Instead of trying to push through and continually hitting the same wall over and over again, take a  step  back.  Maybe  it's  time  to  regroup,  restrategize,  or\n",
      "\n",
      "seriously think about why you're trying to take the steps you are. Something needs to change, and it's probably not just your motivation.\n",
      "\n",
      "THE WAY YOU ARE SELF-SABOTAGING: Overworking.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU  TO KNOW: You do not have to prove your value. You do, however, have to stop running from the discomfort of being alone with your feelings, which is very often the reason that people overwork. There is a difference between being passionately committed to something and feeling obligated to outperform everyone else. One is healthy; the other is not.\n",
      "\n",
      "THE  WAY  YOU  ARE  SELF-SABOTAGING: Caring too much about what other people think.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU  TO KNOW: You are not as happy as you think you are. The happier you are with something, the less you need other people to be. Instead of wondering whether or not someone else will think you are enough, stop and ask yourself: Is my life enough for me? How do you really feel about your life when you aren't looking at it through the eyes of others?\n",
      "\n",
      "THE WAY YOU ARE SELF-SABOTAGING: Spending too much money.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU  TO\n",
      "\n",
      "KNOW: Things will not make you feel more secure. You will not be able to purchase your way into a new life or identity. If you are overspending or spending outside of your means on a regular basis to the point that it is detrimental to you, you need to look at what function buying or shopping serves. Is it a distraction, a replacement for a hobby, or an addiction to the feeling of being 'renewed' in some way? Determine what your needs really are, and then go from there.\n",
      "\n",
      "THE  WAY  YOU  ARE  SELF-SABOTAGING: Dwelling on past relationships or continually checking up on exes.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU  TO KNOW: This relationship affected you more than you are letting  yourself  believe. The  ending  hurt  you  more  than you  acknowledged,  and  you  need  to  process  that.  Your continued interest in this person means there's something about  the  relationship  that  is  still  unresolved,  and  it  is probably some kind of closure or acceptance that you need to find for yourself.\n",
      "\n",
      "THE  WAY  YOU  ARE  SELF-SABOTAGING: Choosing friends who always make you feel like you're in competition with them.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU TO  KNOW: Wanting to feel 'better' than people is not a replacement for needing to feel connected to them, but that's  often  how  we  use  it.  We  do  this  not  because  we\n",
      "\n",
      "actually want to be superior, but because we want to seem valuable and valued. What we want is authentic connection and to feel important to others, but making them feel inferior is not the way to accomplish it.\n",
      "\n",
      "THE  WAY  YOU  ARE  SELF-SABOTAGING: Having  self-defeating thoughts that hold you back from doing what you want.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU  TO KNOW: Being mean to yourself first will not make it hurt less if other people judge or reject you, though that is why you are using this defense mechanism. Thinking the worst of yourself is a way of trying to numb yourself to what you really fear, which is that someone else could say those things about you. What you don't realize is that you're acting as your own bully and enemy by doing it to yourself. What could someone else's judgment realistically do to your life? Honestly, it could stop you from pursuing your dreams,  ambitions,  and  personal  happiness.  And  that's exactly what you're doing when you stay fixated on those damaging ideas. It's time to get out of your own way.\n",
      "\n",
      "THE  WAY  YOU  ARE  SELF-SABOTAGING: Not  promoting your work in a way that would help move you forward.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU  TO KNOW: You're not creating the best possible work you can, and you sense it. The reason why you're holding back is a fear of judgment, but that wouldn't exist if you weren't\n",
      "\n",
      "already judging yourself. You have to create things you are proud to share, and when sharing them in a positive way that helps grow your business or career feels natural and authentic, you will know that you are doing the work that is at the best of your ability or potential.\n",
      "\n",
      "THE WAY YOU ARE SELF-SABOTAGING: Ascribing intent or worrying that things are about you when they aren't.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU  TO KNOW: You think about yourself too often. Other people's lives  do  not  revolve  around  you,  nor  do  their  thoughts. They are busy thinking about themselves in the same way that you are thinking about yourself. Remember that patterns in your life are indicative of your own behaviors, but imagining that every time someone cuts you off in traffic is a personal attack, you're going to severely hold yourself back, because you'll always be the victim of something.\n",
      "\n",
      "THE WAY YOU ARE SELF-SABOTAGING: Staying in a city or town you claim to dislike.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU  TO KNOW: Home is where you make it, not where you find it. Is this an issue of you being unable to move, or are you simply unwilling? Usually when we stay in the same place, there's a reason. There's something we love about it, and it's  where want to spend our lives. The resistance comes in because of the judgment we imagine others may think if they know we don't live in the coolest, biggest, or the\n",
      "\n",
      "best area. You might also fear that people will judge you for not having 'progressed' enough. The truth is that you are judging yourself, and you need to make peace or take pride in why you choose to live where you do.\n",
      "\n",
      "THE WAY YOU ARE SELF-SABOTAGING: Mindlessly scrolling through social media as a way to pass the time.\n",
      "\n",
      "WHAT  YOUR  SUBCONSCIOUS  MIND  MIGHT  WANT  YOU TO KNOW: This is one of the easiest ways to numb yourself,  because  it  is  so  accessible  and  addictive. There  is  a world-altering difference between using social media in a healthy way versus as a coping mechanism. Mostly, it has to do with how you feel after you're finished. If you don't put  the  phone  down  feeling  inspired  or  relaxed,  you're probably trying to avoid some kind of discomfort within yourself-the very discomfort that just might be telling you that you need to change.\n",
      "\n",
      "LEARNING TO LISTEN AGAIN\n",
      "\n",
      "Now that you're starting to pay attention to your internal cues, it is important to understand how to listen to yourself and respond in real time.\n",
      "\n",
      "You are in the situation you are in now because you did not know how to understand or meet your needs in the moment. If you do not want to constantly have to be 'fixing' your choices and behaviors, you have to learn how to\n",
      "\n",
      "process and interpret your feelings in real time. This will be done by a process of building emotional intelligence, which will be primarily done in the next chapter. However, this is where we begin: by understanding how to listen to our instincts.\n",
      "\n",
      "HOW TO FOLLOW YOUR 'GUT' WITHOUT GETTING SCARED OF THE FUTURE\n",
      "\n",
      "One of the most essential tenets of modern wisdom is the idea that deep down, you know the truth about everything in your life and, by extension, your future. The idea is that you are an oracle unto yourself, and your feelings are apertures  into  not  only  what's  happening  now,  but  what's going to happen soon.\n",
      "\n",
      "We're not to blame for believing this. There's a significant amount of research that proves the interconnectedness of our brains and bodies-explaining why when we have a 'gut feeling' or an instinct that precedes logic, it is often correct.\n",
      "\n",
      "This is because the lining of our gastrointestinal system functions as a 'second brain,' given how it stores a backlog of information that your conscious mind can't recall faster than your body can sense. It is this incredible skill that makes your instinct almost always correct.\n",
      "\n",
      "Your gut-though intelligent-isn't psychic.\n",
      "\n",
      "If you want to tune into yourself more, follow your heart, pursue your passion, find your soul-whatever it is-the first thing that you have to understand is that your 'gut instinct'  can  only  respond  to  what's  happening  in  the present.  If  you  have  an  'instinct'  about  a  future  event, you're projecting.\n",
      "\n",
      "This is how you can start breaking down your 'gut feelings.' Are you responding to someone who is in front of you, or are you responding to your idea of them in your head? Are you reacting to a situation that's playing out right now, or are you reacting to one you imagine, assuming you know how it will go? Are your feelings regarding what's happening right now or what you hope and fear will happen in the future?\n",
      "\n",
      "Aside from only really being able to function in the present, your gut instinct is also quiet. The 'little voice' within is just that… little.\n",
      "\n",
      "It does not scream. It does not panic. It does not pump your body with adrenaline to get your attention. It is not angry. It is the wave of clarity that overcomes you in the middle  of  your  darkest  moments,  in  which  something tells you: It's going to be all right; it's not as bad as you think, everything is okay.\n",
      "\n",
      "Your gut instinct functions to make things better, whereas your imagination can often make things worse.\n",
      "\n",
      "But this is often confusing to people, because which feelings are your instincts, and what are your fears, or doubts, or limiting beliefs? How do we know the difference?\n",
      "\n",
      "Well,  your  instincts  aren't  actually  feelings;  they  are responses.\n",
      "\n",
      "If  you  find  yourself  particularly  drained  after  spending time with someone or are feeling like you don't want to see them again, that's your instinct. If the work that you do exhausts you and every bit of it is forced and undesirable, that is your instinct. Instinct is not a feeling (you don't have an 'instinct' that you're sad today); instinct is quickly moving yourself out of harm's way without having to think about it.\n",
      "\n",
      "You have to remember that your feelings, while valid, are not  often  real.  They  are  not  always  accurate  reflections of  reality. They  are,  however,  always  accurate  reflections of our thoughts. Our thoughts change our feelings. Our thoughts do not change our instincts. What you naturally gravitate toward or away from is your instinct. It's not something you feel or interpret; it's something you naturally do.\n",
      "\n",
      "When people talk about using their instincts to craft a life they love, this is what they mean: that they are obeying what their subtle intuition tells them they feel best doing.  Sometimes,  your  instinct  can  move  you  toward your art, even if it makes you uncomfortable and resistant.\n",
      "\n",
      "Sometimes, your instinct can move you to keep working on a relationship, even when it's hard.\n",
      "\n",
      "Your instinct doesn't exist to ensure you feel comfortable and ecstatic at all hours of the day. It moves you toward what you're meant to do, because it shows you where your interests, skills, and desires intersect.\n",
      "\n",
      "I NSTINCT AND FEAR CAN FEEL SIMILAR\n",
      "\n",
      "To trust your gut is not to treat it as an oracle.\n",
      "\n",
      "This is when the concept becomes so problematic. We are not only believing random feelings blindly, but also applying  future  meaning  to  them,  assuming  that  everything we feel is actually warning us or showing us what's ahead.\n",
      "\n",
      "Let's unpack why and how this happens and how you can prevent it from ruining your life.\n",
      "\n",
      "Feelings do not inform you of the right decision to make.\n",
      "\n",
      "Right decisions create the right feelings.\n",
      "\n",
      "Your feelings are not intended to guide you throughout life; that is what your mind is for.\n",
      "\n",
      "If  you  were  to  honestly  follow  your  every  impulse,  you would be completely stuck, complacent, and possibly dead\n",
      "\n",
      "or at the very least in severe trouble. You aren't, because your brain is able to intervene and instruct you on how to make choices that reflect what you want to be experiencing long-term.\n",
      "\n",
      "You begin experiencing feelings of peace and joy in your life  when  you  condition  yourself  to  take  repeated  daily actions that facilitate clarity, calmness, healthfulness, and purposefulness, not the other way around.\n",
      "\n",
      "If you want to master your life, you have to learn to organize your feelings. By becoming aware of them, you can trace  them  back  to  the  thought  process  that  prompted them, and from there you can decide whether or not the idea is an actual threat or concern, or a fabrication of your reptilian mind just trying to keep you alive.\n",
      "\n",
      "Remember: Your brain was built  for  nature.  Your  body was designed to survive in the wild. You have an animalistic  form  trying  to  navigate  a  highly  civilized,  modern world. Forgive yourself for having these impulses, and at the same time, understand that your choices are ultimately yours. You can feel something and not act on it.\n",
      "\n",
      "SO WHY ARE WE EVEN TOLD TO 'LISTEN TO OUR INSTINCTS' I N THE FIRST PLACE?\n",
      "\n",
      "Your gut is deeply connected to your mind. There's a physiological connection between your gastrointestinal system and serotonin production in your brain. Your vagus nerve\n",
      "\n",
      "runs from your gut to your head, acting as a communication device to help your system regulate. 6\n",
      "\n",
      "Your stomach and your mind are inherently connected, which is  why  people  allude  to  just  knowing  something 'deep down' or explain that when they're upset, they're 'sick to their stomach' or had a 'gut reaction' to something.\n",
      "\n",
      "What isn't  being  addressed  is  the  fact  that  listening  to your  instinct  is  something  that  happens  in  the  present moment. You cannot have an instinct about a future event, because it doesn't exist yet. You can have a fear-based or memory response that you are projecting into the future, but you cannot instinctively know something about another person or a future event until it is in front of you.\n",
      "\n",
      "When you have a 'gut instinct' about someone, it is after interacting with them. When you know whether or not a job is right for you, it is only after having done it for a while.\n",
      "\n",
      "The problem is that we are trying to use our instincts as fortune-telling  mechanisms,  our  brain's  creative  way  of trying to manipulate our body to help us avoid pain and increase pleasure in the future. But that's not what happens. We end up stuck because we are literally trusting every single thing that we feel instead of discerning what's an actual reaction and what's a projection.\n",
      "\n",
      "I D E N T I F Y I N G   T H E   D I F F E R E N C E BETWEEN INSTINCT AND FEAR\n",
      "\n",
      "First and foremost, understand that your instinct can serve you immensely in the present moment. Your first reaction to  something  is  very  often  the  wisest  reaction,  because your body is using all of the subconscious information you have logged away to inform you about something before your brain has an opportunity to second-guess it.\n",
      "\n",
      "You can use this to your advantage by staying in the moment and asking yourself what is true right here and right now. What is true when you are with another person, activity, or behavior? What is the deep, gut instinct that you get when you're presently engaging with something?\n",
      "\n",
      "Does it differ from what you think and feel about it when you are just imagining it, making guesses about it, recalling details of it, or imagining what it will be like? Typically, those projections are fear, and your present reaction is your honest instinct.\n",
      "\n",
      "Overall, your honest gut instinct won't ever frighten you into panic. Your gut is always subtle and gentle, even if it's telling you that something isn't for you. If your gut wants you to know not to see someone or to stop engaging in a relationship or behavior, the impulse will be quiet. That's why it's called the 'little voice' within. So easy to miss. So easy to shout over.\n",
      "\n",
      "I N T U I T I V E   N U D G E S VS. INTRUSIVE THOUGHTS\n",
      "\n",
      "When you start listening to  yourself,  you  might  find  it hard to tell the difference between thoughts that are helpful and intuitive, and thoughts that are damaging and intrusive. They both function similarly-they are immediate, reactive, and offer some kind of previously unseen insightand yet they function so completely differently in practice.\n",
      "\n",
      "This is how to start telling the difference between thoughts that are informed by your intuition and thoughts that are informed by fear:\n",
      "\n",
      " · Intuitive thoughts are calm. Intruding thoughts are hectic and fear-inducing.\n",
      "\n",
      " · Intuitive thoughts are rational; they make a degree of sense. Intruding thoughts are irrational and often stem from aggrandizing a situation or jumping to the worst conclusion possible.\n",
      "\n",
      " · Intuitive thoughts help you in the present. They give you information that you need to make a better-informed decision. Intruding thoughts are often random and have nothing to do with what's going on in the moment.\n",
      "\n",
      " · Intuitive thoughts are 'quiet'; intruding thoughts are 'loud,' which makes one harder to hear than the other.\n",
      "\n",
      " · Intuitive thoughts usually come to you once, maybe twice,  and  they  induce  a  feeling  of  understanding. Intruding thoughts tend to be persistent and induce a feeling of panic.\n",
      "\n",
      " · Intuitive thoughts often sound loving, while invasive thoughts sound scared.\n",
      "\n",
      " · Intuitive  thoughts  usually  come  out  of  nowhere; invasive  thoughts  are  usually  triggered  by  external stimuli.\n",
      "\n",
      " · Intuitive thoughts don't need to be grappled withyou  have  them  and  then  you  let  them  go.  Invasive thoughts begin a whole spiral of ideas and fears, making it feel impossible to stop thinking about them.\n",
      "\n",
      " · Even  when  an  intuitive  thought  doesn't  tell  you something you like, it never makes you feel panicked. Even if you experience sadness or disappointment, you don't feel overwhelmingly anxious. Panic is the emotion you experience when you don't know what to  do  with  a  feeling.  It  is  what  happens  when  you have an invasive thought.\n",
      "\n",
      " · Intuitive thoughts open your mind to other possibilities; invasive thoughts close your heart and make you feel stuck or condemned.\n",
      "\n",
      " · Intuitive thoughts come from the perspective of your\n",
      "\n",
      "best self; invasive thoughts come from the perspective of your most fearful, small self.\n",
      "\n",
      " · Intuitive thoughts solve problems; invasive thoughts create them.\n",
      "\n",
      " · Intuitive  thoughts  help  you  help  others;  invasive thoughts tend to create a 'me vs. them' mentality.\n",
      "\n",
      " · Intuitive thoughts help you understand what you're thinking and feeling; invasive thoughts assume what other people are thinking and feeling.\n",
      "\n",
      " · Intuitive thoughts are rational; invasive thoughts are irrational.\n",
      "\n",
      " · Intuitive thoughts come from a deeper place within you and give you a resounding feeling deep in your gut; invasive thoughts keep you stuck in your head and give you a panicked feeling.\n",
      "\n",
      " · Intuitive thoughts show you how to respond; invasive thoughts demand that you react.\n",
      "\n",
      "HOW TO START TRULY MEETING YOUR NEEDS\n",
      "\n",
      "Though the term self-care has become an umbrella term that more often refers to behaviors that distract one from\n",
      "\n",
      "the actual problem at hand rather than really taking action to  fix  the  problem  at  hand,  actual  self-care  is  the  most fundamental aspect of meeting your own needs.\n",
      "\n",
      "Aside from your own basic security, your needs are to be nourished, to sleep well, to live in a clean environment, to dress appropriately, and to allow yourself to feel what you feel without judgment or suppression.\n",
      "\n",
      "Finding  ways  to  meet  these  needs  on  your  own  is  the foundation of overcoming self-sabotage.\n",
      "\n",
      "You are going to feel far more willing to exercise if you got a good night's sleep. You are going to feel much better about work if you don't have to sit there with an ongoing backache and instead seek out a professional who can help you with your posture or chiropractic care or massage. You are  going  to  enjoy  spending  time  in  your  home  if  your home is organized and meaningful to you. You are going to feel better about yourself each day if you take the time to put yourself together with care.\n",
      "\n",
      "These things are not little things; they are big things. Y ou just can't see it because their impact is that you do them every day.\n",
      "\n",
      "Understanding your needs, meeting the ones you are responsible for, and then allowing yourself to show up so others can meet the ones you can't do on your own will help you break the self-sabotage cycle and build a healthier, more balanced and fulfilling life.\n",
      "\n",
      "CHAPTER 4\n",
      "\n",
      "BUILDING EMOTIONAL I N T E L L I G E N C E\n",
      "\n",
      "SELF-SABOTAGE  IS  ULTIMATELY  JUST a  product  of  low emotional intelligence.\n",
      "\n",
      "To move on with our lives in a healthy, productive, and stable  way,  we  need  to  understand  how  our  brains  and bodies work together. We need to understand how to interpret feelings, what different emotions mean, and what to  do  when  we  are  faced  with  big,  daunting  sensations that we don't know how to handle.\n",
      "\n",
      "We are going to specifically focus on aspects of emotional  intelligence  that  relate  to  self-sabotaging  behaviors, though there is an incredible body of work on EI from experts  around  the  world  that  is  continually  growing with time.\n",
      "\n",
      "WHAT IS EMOTIONAL INTELLIGENCE?\n",
      "\n",
      "Emotional  intelligence  is  the  ability  to  understand,\n",
      "\n",
      "interpret, and respond to your emotions in an enlightened and healthy way.\n",
      "\n",
      "People with high emotional intelligence are often able to better get along with different types of people, feel more contentment and satisfaction in their everyday lives, and consistently  take  time  to  process  and  express  their  authentic feelings.\n",
      "\n",
      "Mostly,  though,  emotional  intelligence  is  the  ability  to interpret the sensations that come up in your body and understand what they are trying to tell you about your life.\n",
      "\n",
      "The root of  self-sabotage  is  a  lack  of  emotional  intelligence,  because  without  the  ability  to  understand  ourselves, we inevitably become lost. These are some of the most misunderstood aspects of our brains and bodies that inevitably leave us stuck.\n",
      "\n",
      "YOUR BRAIN IS DESIGNED TO RESIST WHAT YOU REALLY WANT\n",
      "\n",
      "Something interesting happens in the human brain when we get what we want.\n",
      "\n",
      "When we  imagine  what  goals  we  want  to  achieve,  we often do so with the expectation that they will elevate our quality of life in some tangible way, and once we have arrived at that place, we will be able to 'coast.'\n",
      "\n",
      "'Coast' as in, let go. Relax into life. Let things be for a while.\n",
      "\n",
      "That is not what happens.\n",
      "\n",
      "Neurologically, when we get something we really want, we just start to want more. New research in the nature of the chemical  dopamine-which  was  previously  believed  to be the driving force behind desire, lust, and acquisitionproves that it is more complex than previously thought.\n",
      "\n",
      "In The  Molecule  of  More ,  Daniel  Z.  Lieberman  explains that experts who studied the hormone found that when an  individual  was  introduced  to  something  they  highly desired, the dopamine surge would diminish after acquisition. Dopamine, it turns out, is not the chemical that gives you pleasure; it's the chemical that gives you the pleasure of wanting more. 7\n",
      "\n",
      "So the big, huge goal that you're working toward? You'll get there, and then there will be another mountain to scale.\n",
      "\n",
      "This is one of the many reasons that we deeply sabotage what we truly want. We know instinctively that 'arriving' won't really give us the ability to abstain from life; it will only make us hungrier for more. Sometimes, we don't feel up to that challenge.\n",
      "\n",
      "So, while we're on the way, a toxic cocktail of neurological biases start piling up on one another, and we start to resent, judge, and even vilify the object of our greatest desire.\n",
      "\n",
      "What  happens  when  we  start  to  chase  what  we  really want: We resist doing the work that it takes to actually get it because we are so afraid of not having it, any brush with failure makes us rescind our effort and tense up.\n",
      "\n",
      "When we go so long not having what we really want, we create  subconscious  associations  between  having  it  and 'being bad,' because we have judged others for having it.\n",
      "\n",
      "When we get it, we fear losing it so badly that we push it away from ourselves so as to not have to withstand the pain.\n",
      "\n",
      "We are so deeply enmeshed in the mental state of 'wanting,' we cannot shift to a state of 'having.'\n",
      "\n",
      "First,  when  we  want  something  really,  really  badly,  it  is often because we have unrealistic expectations associated with it. We imagine that it will change our lives in some formidable way, and often, that's not the case.\n",
      "\n",
      "When we are relying on some goal or life change to 'save' us in some unrealistic way, any incident of failure will trigger  us  to  stop  trying.  For  example:  If  we  are  absolutely certain  that  a  romantic  partner  will  help  us  stop  being depressed, we are going to be extremely sensitive to rejection, because it makes us feel as though we will never get over depression.\n",
      "\n",
      "Of course, the obvious issue here is that dating is a process of trial and error. Y ou have to fail first to succeed.\n",
      "\n",
      "Then, for all the time we spend not having the thing we want, such as a romantic relationship, our brains have to justify and validate our stance in life as a form of self-protection.  This  is  why  we  unconsciously  vilify  those  who do have what we want. Instead of being inspired by their success, we doubt them. We become a skeptic about relationships, being so jealous of others' happiness we assume that they must be faking it, or that love 'isn't real,' or that they'll split eventually, anyway.\n",
      "\n",
      "If we hold tightly to these beliefs for long enough, guess what will  happen  when  we  finally  get  that  relationship we really want? Of course, we are going to doubt it and assume it will also fail.\n",
      "\n",
      "This  is  what's  going  on  when  people  push  others  away or  give  up  on  their  big  dreams  the  moment  something challenging comes up. When we are so scared that we are going to lose something, we tend to push it away from ourselves first as a means of self-preservation.\n",
      "\n",
      "So let's say that you work through the limiting beliefs that are creating this much resistance in your life, and you do eventually allow yourself to build and have the thing you really, really want. Next, you'll be upon the last and most trying challenge, which is the shift from 'survival mode' to 'thriving mode.'\n",
      "\n",
      "If  you  have  spent  the  majority  of  your  life  in  a  state  in which you are 'just getting by,' you are not going to know\n",
      "\n",
      "how to adapt to a life in which you are relaxed and enjoying  it.  Y ou  are  going  to  resist  it,  feel  guilty,  perhaps overspend or  disregard  responsibilities.  You  are,  in  your head, 'balancing out' the years of difficulty with years of complete relaxation. However, this is not how it works.\n",
      "\n",
      "When we are so deeply enmeshed in the feeling of 'wanting,' it becomes extremely hard to adjust to the experience of 'having.'\n",
      "\n",
      "This is because any change, no matter how positive, is uncomfortable until it is also familiar.\n",
      "\n",
      "It  is  difficult  to  acknowledge  the  ways  in  which  we  are so deeply inclined to self-validate, so we end up standing in our own way out of pride. It is even more difficult to acknowledge that very often, the things we envy in others are fragments of our deepest desires, the ones we won't allow ourselves to have.\n",
      "\n",
      "Yes, your brain is predisposed to want greater things, and more  of  them.  But  by  understanding  its  processes  and tendencies, you can override the programming and start governing your own life.\n",
      "\n",
      "YOUR BODY IS GOVERNED BY A HOMEOSTATIC IMPULSE\n",
      "\n",
      "Your brain is built to reinforce and regulate your life.\n",
      "\n",
      "Your subconscious mind has something called a homeostatic impulse, which regulates functions like body temperature, heartbeat, and breathing. Brian Tracy explained it  like  this:  'Through  your  autonomic  nervous  system, [your  homeostatic  impulse]  maintains  a  balance  among the hundreds of chemicals in your billions of cells so that your entire physical machine functions in complete harmony most of the time.' 8\n",
      "\n",
      "But what many people don't realize is that just as your brain is built to regulate your physical self, it tries to regulate your mental self. Your mind is constantly filtering and bringing to your attention information and stimuli that affirm your preexisting beliefs (this is known in psychology as confirmation bias) as well as presenting you with repeated  thoughts  and  impulses  that  mimic  and  mirror what you've done in the past.\n",
      "\n",
      "Your subconscious mind is the gatekeeper of your comfort zone.\n",
      "\n",
      "It is also the realm in which you can either habituate yourself  to  expect  and routinely seek the actions that would build and reinforce the greatest success, happiness, wholeness, or healing of your life.\n",
      "\n",
      "What this teaches us is that when we are going through a healing or changing process in our lives, we have to allow our bodies to adjust to their new sense of normalcy. This is why all change, no matter how good, will be uncomfortable\n",
      "\n",
      "until it is also familiar. This is also why we can get stuck in self-destructive habits and cycles. Even though they feel good, that does not mean they are good for us.\n",
      "\n",
      "We have to use our minds to practice discernment. We have to use our supreme intelligence to decide where we want to go, who we want to be, and then we have to allow our bodies to adjust over time.\n",
      "\n",
      "We cannot live being governed by how we feel. Our emotions are temporary and not always reflective of reality.\n",
      "\n",
      "YOU DON'T CHANGE IN BREAKTHROUGHS; YOU CHANGE IN MICROSHIFTS\n",
      "\n",
      "If you're stuck in life, it's probably because you're waiting for the big bang, the breakthrough moment in which all your fears dissolve and you're overcome with clarity. The work that needs to happen happens effortlessly. Your personal transformation rips you from complacency, and you wake up to an entirely new existence.\n",
      "\n",
      "That moment will never come.\n",
      "\n",
      "Breakthroughs  do  not  happen  spontaneously.  They  are tipping points.\n",
      "\n",
      "Revelations occur when ideas that were sitting in the margins of your mind finally get enough attention to dominate  your  thoughts.  These  are  the  'clicking'  moments,\n",
      "\n",
      "the moments when you finally understand advice you've heard your entire life. The moments when you've habituated yourself to a pattern of behavior for long enough that it becomes instinctive.\n",
      "\n",
      "A  mind-blowing,  singular  breakthrough  is  not  what changes your life. A microshift is.\n",
      "\n",
      "Breakthroughs  are  what  happen  after  hours,  days,  and years of the same mundane, monotonous work.\n",
      "\n",
      "But a mind-blowing, singular breakthrough is not what changes your life. A microshift is.\n",
      "\n",
      "As writer and media strategist Ryan Holiday has noted, epiphanies  are  not  life-altering.   It's  not  radical  mo9 ments  of  action  that  give  us  long-lasting,  permeating change-it's the restructuring of our habits. The idea is what science philosopher Thomas Kuhn dubbed a 'paradigm shift.' Kuhn suggested we don't change our lives in flashes of brilliance, but through a slow process in which assumptions unravel and require new explanations. It's in these periods of flux that microshifts happen and breakthrough-level change begins to take shape.\n",
      "\n",
      "Think of microshifts as tiny increments of change in your day-to-day life. A microshift is changing what you eat for one part of one meal just one time. Then it's doing that a second time and a third. Before you even realize what's happening, you've adopted a pattern of behavior.\n",
      "\n",
      "What you do every single day accounts for the quality of your life and the degree of your success. It's not whether you 'feel' like putting in the work, but whether or not you do it regardless.\n",
      "\n",
      "This is because the outcomes of life are not governed by passion; they are governed by principle.\n",
      "\n",
      "You may not think what you did this morning was important, but it was. Y ou may not think that the little things add up, but they do. Consider the age-old brainteaser: Would you rather have $1 million in hand today or a penny that doubles in value every day for the next month? The $1 million right now sounds great, but after a 31-day month, that one penny would be worth over $10 million.\n",
      "\n",
      "Making big, sweeping changes is not difficult because we are  flawed,  incompetent beings. It's difficult because we are not meant to live outside of our comfort zones.\n",
      "\n",
      "If  you want to change your life, you need to make tiny, nearly  undetectable  decisions  every  hour  of  every  day until those choices are habituated. Then you'll just continue to do them.\n",
      "\n",
      "If you want to spend less time on your phone, deny yourself the chance to check it one time today. If you want to eat healthier, drink half a cup of water today. If you want to sleep more, go to bed 10 minutes earlier tonight than you did last night.\n",
      "\n",
      "If you want to exercise more, do it now for just 10 minutes. If you want to read, read one page. If you want to meditate, do so for 30 seconds.\n",
      "\n",
      "Then keep doing those things. Do them every single day. You'll get used to not checking your phone. You'll want more water, and you'll drink more water. You'll run for 10 minutes, and you won't feel like you have to stop, so you won't. You'll read one page, grow interested, and read another.\n",
      "\n",
      "At our most instinctive, physiological level, 'change' translates to something dangerous and potentially life-threatening. No wonder why we build our own cages and stay in them, even though there's no lock on the door.\n",
      "\n",
      "Trying  to  shock  yourself  into  a  new  life  isn't  going  to work, and that's why it hasn't yet.\n",
      "\n",
      "You don't need to wait until you feel like changing to start changing. All you need is to make one microshift at a time and then let the energy and momentum build.\n",
      "\n",
      "YOUR MIND IS ANTIFRAGILE\n",
      "\n",
      "Is your brain the greatest antagonist in your life?\n",
      "\n",
      "Is irrational fear at the core of the majority of your greatest stressors?\n",
      "\n",
      "Do you ever have the hunch that you're almost seeking out problems, creating issues where they don't exist, overreacting, overthinking, and catastrophizing?\n",
      "\n",
      "If you said 'yes' to these, congratulations, you're self-aware.\n",
      "\n",
      "You're also just like anybody else.\n",
      "\n",
      "If you feel like you're always subconsciously scanning your life trying to identify the next thing to worry about, the next potential threat to fear, you'd be right.\n",
      "\n",
      "What we fear most is what our minds identify as the least likely threat that we cannot control. If the threat is highly likely, we don't fear it-we respond to it. That's why most worry comes from not just identifying the one thing we cannot control, but the one small, unlikely thing we cannot control.\n",
      "\n",
      "So why do our minds need this, though?\n",
      "\n",
      "Can't we just enjoy what we have and be grateful?\n",
      "\n",
      "To a point, absolutely.\n",
      "\n",
      "But our minds also need adversity, and that's why it's instinctual to keep creating problems-even if there aren't any real ones in front of us.\n",
      "\n",
      "The human mind is something called antifragile, which\n",
      "\n",
      "means that it actually  gets  better  with  adversity.  Like  a rock that becomes a diamond under pressure or an immune  system  that  strengthens  after  repeated  exposure to germs, the mind requires stimulation in the form of a challenge.\n",
      "\n",
      "If you deny and reject any kind of real challenge in your life, your brain will compensate by creating a problem to overcome. Except this time, there won't be any reward at the end. It will just be you battling you for the rest of your life.\n",
      "\n",
      "The cultural obsession with chasing happiness, shielding oneself from anything triggering, and the idea that life is primarily 'good' and any challenge we face is a mistake of fate are what actually weaken us mentally.\n",
      "\n",
      "Shielding  the  mind  from  any  adversity  makes  us  more vulnerable to anxiety, panic, and chaos.\n",
      "\n",
      "Those who can't help but create problems in their minds often  do  so  because  they  have  ceased  creative  control of  their  existence.  They  move  into  the  passenger's  seat, thinking that life happens to them, rather than being a product of their actions.\n",
      "\n",
      "Who wouldn't be afraid if that were the case?\n",
      "\n",
      "But what most people don't tell you is that adversity makes you creative. It activates a part of you that is often latent.\n",
      "\n",
      "It makes things interesting. Part of the human narrative is wanting something to overcome.\n",
      "\n",
      "The trick is keeping it in balance. Choosing to exit your comfort zone and endure pain for a worthy cause.\n",
      "\n",
      "Focusing on problems that are real problems in the world, like hunger or politics or whatever else.\n",
      "\n",
      "But  most  importantly,  it's  about  staying  engaged  with what we can control in life, which is most things if you really think about it. Antifragile things need tension, resistance, adversity, and pain to break and transform. We get  this  by  deeply  communing  with  life  and  being  part of it, rather than fearing our emotions and sitting on the sidelines.\n",
      "\n",
      "You can't stay there forever, nor do you really want to. Embracing the grit of it all was what you were made for. Lean in and start living.\n",
      "\n",
      "NEW CHANGE CREATES ADJUSTMENT SHOCK\n",
      "\n",
      "Of all the things that nobody tells you about life, that you might not experience instantaneous happiness after a positive life change is perhaps the most confusing.\n",
      "\n",
      "The truth about your psyche is this: Anything that is new,\n",
      "\n",
      "even if it is good, will feel uncomfortable until it is also familiar.\n",
      "\n",
      "Our brain works the opposite way, too, in that whatever is familiar is what we perceive to be good and comfortable, even if those behaviors, habits, or relationships are actually toxic or destructive.\n",
      "\n",
      "Positive life events can actually trigger depressive episodes. This happens for a few reasons: First, a spike and then decline in mood or attitude can exacerbate stress. Second, the expectation that a positive event will eliminate all stress and bring unprecedented happiness is a destructive one, because the event rarely does that. This is why weddings, childbirth, or a new job can be so incredibly stressful. On top of being a massive life change, there's also the silent assumption that this should be a wholly positive thing, and anxiety and tension should be eliminated.\n",
      "\n",
      "It is jarring to discover this isn't the case.\n",
      "\n",
      "Overall, it comes down to the simple fact that any accomplishments, achievements, or life changes, no matter how positive, elicit change. Change elicits stress. This is particularly true for those who are already predisposed to anxiety and depression, because the concept of one's comfort zone is absolutely essential to stabilizing their mood. This is also why those people can often seem overwhelmingly particular or narrow-minded.\n",
      "\n",
      "WHAT ARE THE SIGNS OF ADJUSTMENT SHOCK?\n",
      "\n",
      "Adjustment shock can manifest as simply an increase in anxiety or irritability. However, it is often more complex than that.\n",
      "\n",
      "Adjustment shock often comes across as hypervigilance. If you make financial gains, your mind immediately shifts to what could potentially derail your progress, a big bill that  could  come up, or the loss of the job you just got. If you have a new, happy relationship, you could become paranoid about infidelity or lies.\n",
      "\n",
      "Adjustment shock can bring to light unconscious attachments and beliefs. If you are someone who was raised to think  that  wealthy  people  are  morally  corrupt,  you  are going to resist having more money. If you wanted to be famous to be more loved, you are going to resist public success, because 'famous' people are often more criticized and disliked than the average person.\n",
      "\n",
      "Adjustment shock can bring feelings of intense fear. This is because when we attain something we very much care about  or  have  worked  toward  for  a  long  time,  our  instinct can be to shield ourselves from the potential loss of it by putting up walls and desensitizing ourselves to the experience.\n",
      "\n",
      "We often resist most deeply the things that we want most.\n",
      "\n",
      "This is because of adjustment shock, though we don't always know that's what's causing the resistance.\n",
      "\n",
      "It is scary to receive everything we want, because it forces us to shift out of a survivalist, fear-based mindset and into a more stabilized one. If all we are accustomed to is doing what we need to do to survive, we are then confronted with the next phases of our self-actualization.\n",
      "\n",
      "If we are no longer worried about basic survival, our minds are free to turn to the bigger questions in life: What is our purpose?  Have  we  lived  meaningfully?  Are  we  who  we want to be?\n",
      "\n",
      "We often think of big achievements as a 'get out of life easier' card. They rarely are that. In fact, the opposite tends to happen. They level us up, force us into greater responsibilities, to think more deeply about big issues, to question ourselves and what we previously knew to be true.\n",
      "\n",
      "Big achievements actually pressure us to become increasingly better versions of ourselves. This is a net positive for our lives but can be just as uncomfortable as struggling was, if not more so.\n",
      "\n",
      "HOW DO I OVERCOME ADJUSTMENT SHOCK?\n",
      "\n",
      "When something positive happens in your life, you are going to have to adjust your mindset about other things to create alignment and a new, more accurate and sustainable perspective.\n",
      "\n",
      "If  you  have  anxiety  about  having  more  money,  you  will need to learn how to manage it better. If you have anxiety about relationships, you will need to learn to relate to others like you never have before.\n",
      "\n",
      "Your big life change is going to force you to level up in every way imaginable, and the way to overcome the initial  fear  of  stepping  into  the  unknown  is  to  familiarize yourself with it, to make it a part of you, one that you are certain you are prepared for-and that you deserve.\n",
      "\n",
      "PSYCHIC THINKING ISN'T WISDOM\n",
      "\n",
      "When we talk about 'psychic thinking,' we are not referring to the palm-reading, neon-sign-advertised occultist professionals  you  can  hire  to  evaluate  your  energy  and predict your future.\n",
      "\n",
      "Psychic thinking is far more insidious than that.\n",
      "\n",
      "Psychic thinking is assuming you know what somebody else  is  thinking  or  what  they  intend  to  do.  It  is  assuming that the least likely outcome is the most viable outcome,  because  you  feel  it  most  strongly.  It  is  believing that you have missed out on 'another life,' a path you did not choose, that you were possibly more meant for. It is believing that the person with whom you have the most electric connection is your most ideal life partner.\n",
      "\n",
      "Of course, the way other people see us is dynamic. Their thoughts, feelings, and intentions are largely if not entirely unknown to us. The least likely outcome is just that: the least likely outcome. There is no such thing as the path we could have taken, only a projection of our needs and desires  onto  another  fantastical  idea  of  what  our  lives might be.  Electric  connection  is  not  soulmateship;  love and compatibility are not the same thing.\n",
      "\n",
      "Psychic thinking detaches us from reality. In place of logic, we put emotions, ones that are often incorrect, unreliable, and wholly biased toward what we want to believe.\n",
      "\n",
      "Beyond  being  inconveniencing,  psychic  thinking  is  absolutely terrible for your mental health. Psychic thinking breeds anxiety and depression. It's not just that something scares or upsets us; it's that we believe that the thought must not only be real, but predictive of future events. Instead  of  feeling  like  we  are  having  a  down  day,  psychic thinking makes us assume we are having a terrible life.\n",
      "\n",
      "We heard 'trust yourself ' and then began to liken ourselves  to  oracles,  that  when  a  particularly  triggering thought  or  feeling  passes  through  us,  it  must  indicate something more to come.\n",
      "\n",
      "Indeed, psychic thinking as a whole has begun taking on an entirely new light because of the popularity of pop psychology, dating back to the 50s and 60s. Trust yourself, the gurus tell you. Deep down, you know the truth.\n",
      "\n",
      "This  is  valid.  Y our  intestines  are  literally  connected  to the stem of your brain; the bacteria in your stomach respond  to  subconscious  intelligent  awareness  faster  than your mind can. This is why your 'gut' is indeed correct on instinct. But when this advice is given to people who cannot differentiate a gut feeling from fear or from a passing thought that has no bearing on reality or their lives as a whole, it becomes a dangerous practice in which they become completely stuck and limited because they assume their random feelings are all real-and then not only real, but a prediction for what's to come.\n",
      "\n",
      "Psychic thinking is nothing more than a series of cognitive biases, the most prominent of which are the following:\n",
      "\n",
      "CONFIRMATION\n",
      "\n",
      "At any moment in time, your brain is inundated with stimuli. To help you process, your conscious mind is aware of about 10% of it or less. Y our subconscious mind is still paying attention, logging away information you might one day need.\n",
      "\n",
      "However, what determines what makes it to that 10% of our conscious awareness has a lot to do with what we already believe. Our brains are literally working to filter out information that does not support our preexisting ideas, and then to draw our attention to information that does. This means that we are subject to a 'confirmation bias,' which is that we literally seek out and sort through stimuli that supports what we want to think.\n",
      "\n",
      "EXTRAPOLATION\n",
      "\n",
      "Extrapolation is when we take our current circumstances and then project them out into the future. Ryan Holiday says it best: 'This moment is not my life. It is a moment in my life.'\n",
      "\n",
      "Extrapolation makes us think that we are the sum of our past  or  current  experiences,  that  whatever  stressors  or anxieties we are currently experiencing are ones that we will grapple with for the rest of our lives. Unable to see through the problem at hand, we assume it will never resolve itself. Unfortunately, this can become a self-fulfilling prophecy. If we are so easily defeated and exhausted by the idea that we will never get over our problems, then we make it more likely that we will hang onto them instead of  logically  trying  to  resolve  them,  for  a  lot  more  time than is necessary.\n",
      "\n",
      "SPOTLIGHTING\n",
      "\n",
      "Everyone  thinks  that  the  world  revolves  around  them. You are thinking about you and your own interests all day, every day. It can be challenging to forget that others are not thinking about us with such intensity; they are thinking about themselves.\n",
      "\n",
      "The  spotlight  effect  is  what  happens  when  we  imagine that our lives are performative, or 'on display' for others to consume. We remember the last two or three embarrassing\n",
      "\n",
      "things we have done and imagine that others are thinking about them actively as well. Can you recall the last two or three  embarrassing  things  someone  else  did?  Of  course you can't. Because you aren't paying attention.\n",
      "\n",
      "Spotlighting gives us the false impression that the world is all about us, when it is not.\n",
      "\n",
      "These  biases  plus  others,  when  combined  with  psychic thinking,  or  the  idea  that  our  assumptions  and  feelings about  the  world  will  transpose  into  reality,  are  harmful and mostly incorrect. Instead of trying to predict what will happen next, our energy is better used when it's focused intently on the moment-the infinite 'now,' the mystics would say-because the truth is that the past and future are illusions in the present, and all we have is the present.\n",
      "\n",
      "Instead of trying to use your intelligence to hack what's next,  try  to  get  better  at  where  you  are  currently. That's what's really going to change the outcomes of your life.\n",
      "\n",
      "LOGICAL LAPSES ARE GIVING YOU PROFOUND ANXIETY\n",
      "\n",
      "Most of the anxiety you experience in life is the result in inefficient critical-thinking skills. You might assume that because you are anxious, you are an overthinker, someone who obsesses about unlikely and scary outcomes more than is reasonable. The reality is that you are an under-thinker.\n",
      "\n",
      "You're missing a part of your reasoning process.\n",
      "\n",
      "Let's start at the beginning. Anxiety is a normal emotion that every person experiences at some point in their lives, typically when circumstances are stressful, tense, or scary. When anxiety is chronic and begins to interfere with dayto-day functioning, it becomes a clinical disorder.\n",
      "\n",
      "We understand the importance of speaking about mental health with the same degree of legitimacy as physical health.  However,  in  the  same  way  that  we'd  question what  someone  keeps  tripping  over  if  they  repeatedly sprain their ankle, a lot of anxiety is similarly circumstantial, as many illnesses are. Specifically, anxiety tends to be the result of an inability to process acutely stressful and ongoing circumstances.\n",
      "\n",
      "If we want to heal, we have to learn to process.\n",
      "\n",
      "This applies to everyone, not just those with a diagnosis.\n",
      "\n",
      "One of the hallmarks of anxiety is rapid thinking. Because you are focusing on some issue so deeply and for so much time, you assume that you are also thinking through the issue thoroughly and arriving at the most likely conclusion. However, the opposite is happening.\n",
      "\n",
      "You're experiencing a logical lapse. You're jumping to the worst-case  scenario  because  you  aren't  thinking  clearly, and then you are engaging your fight-or-flight response\n",
      "\n",
      "because the worst-case scenario makes you feel threatened. This is why you obsess about that one, terrifying idea. Your body  is  responding  as  though  it's  an  immediate  threat, and until you 'defeat' or overcome it, your body will do its job, which is to keep you in defense mode, which is really a heightened state of awareness to the 'enemy.'\n",
      "\n",
      "WHAT IS A LOGICAL LAPSE?\n",
      "\n",
      "Think of something that you aren't afraid of, maybe something that other people might find scary.\n",
      "\n",
      "Maybe you aren't  afraid  of  flying  in  an  airplane.  Many people are. Maybe you aren't afraid of being single. Many people are. Maybe you aren't afraid of commitment. Many people are. Surely you can think of at least one thing in your life that you are truly unafraid of.\n",
      "\n",
      "Why aren't you afraid of it? Because you don't have a logical lapse there.\n",
      "\n",
      "You can visualize yourself going on an airplane and successfully getting off without freaking out. You can visualize yourself being happily single or happily committed. Even if the worst were to happen, you can think a situation through in its entirety, from exposition to climax to conclusion. Y ou know what you would do. You have a plan.\n",
      "\n",
      "When you experience a logical lapse, the climax becomes the conclusion. You imagine a situation, you figure that\n",
      "\n",
      "you  would  panic,  and  then  because  you're  scared,  you never think through the rest of the scenario. You never think about how you'd get through it, what you'd do to respond, and how you'd eventually move on with your life afterwards. If you were able to do this, you wouldn't be scared of it, because you wouldn't think it had the power to 'end' you.\n",
      "\n",
      "This is why exposure is the most common treatment for irrational fear. By reintroducing the stressor into your life in a safe way, you are able to reestablish a line of thinking that is healthier and calmer. Basically, you prove to yourself  that  you  will  be  okay,  even  if  something  scary  does happen (which most of the time it does not).\n",
      "\n",
      "Either way, mental strength is not just hoping that nothing ever goes wrong. It is believing that we have the capacity to handle it if it does.\n",
      "\n",
      "Maybe you don't have that self-belief yet. That's okay. It's not something you're born with; it's something you build slowly  and  over  time.  It's  something  you  develop  with practice, by addressing small problems, and then learning healthy coping mechanisms and effective reasoning skills.\n",
      "\n",
      "The thing is that there are millions of scary things that can happen to us in our lives. That is true for everyone. When we are hung up on one scary thing over another, it's not because it's a more imminent or likely threat; it's because we are less convinced we would be able to respond to it.\n",
      "\n",
      "To heal, we don't need to avoid it. We need to develop logic to see situations for what they are and respond appropriately to them.\n",
      "\n",
      "So often in life, our biggest anxiety comes not from what's actually happening, but how we think about what is happening. In that, we reclaim our emotional freedom and power.\n",
      "\n",
      "FAULTY INFERENCES ARE HOLDING YOU BACK FROM SUCCESS\n",
      "\n",
      "If you're familiar with body typing, you'll probably be familiar with the terms endomorph, mesomorph, and ectomorph. Though everyone actually falls somewhere within the spectrum of these (meaning that everyone has varying degrees of each), the traits you default to are typically your primary body type. 10\n",
      "\n",
      "If you've studied these types, you'll know that endomorphic bodies are often associated with increased fat retention. The assumption here is that these people have the worst metabolisms, but that is false. Endomorphs actually have the best metabolisms of anyone. They are alive today because  their  ancestors  adequately  adapted  to  survive. Their metabolisms do precisely what they were intended to do: store fat for later use.\n",
      "\n",
      "Something similar happens with highly intelligent people who experience high levels  of  anxiety.  You  assume  that\n",
      "\n",
      "because these people are smart, they would be able to use logic to disrupt illogical fears. (Logical lapses, or an inability to adequately reason, often generate anxiety.)\n",
      "\n",
      "However, their brains are doing exactly what they were meant to do, which is to piece together unrelated stimuli and identify potential threats.\n",
      "\n",
      "Highly intelligent  people  have  a  psychological  function others do not, which is the ability to infer. They can extract meaning and understanding from things that others simply take at face value. This is why people who have extremely high IQs often struggle with basic things such as social skills or driving a car. Where others see the world as one-dimensional, the highly intelligent see it as three-dimensional. They think more deeply than is often necessary. This  gives  them  their  ability  to  create,  understand, strategize, and invent.\n",
      "\n",
      "In the same way that the endomorph's excellent metabolism can work against them, so too can a highly intelligent person's brain. This is because at times, they make something called 'faulty inferences,' which are when fallacies, biases,  and  incorrect  assumptions  are  made  from  valid evidence.\n",
      "\n",
      "What's happening in your brain when you're very anxious is that you're taking an often innocuous stimulus and extracting  some kind of meaning or prediction from it. When you're scared, your brain is working in overdrive to\n",
      "\n",
      "identify the thing that can potentially hurt you and then creatively come up with ways to completely avoid that experience. The smarter you are, the better you become at this.\n",
      "\n",
      "However, the more you avoid a fear, the more intense it becomes.\n",
      "\n",
      "WHAT'S A FAULTY INFERENCE?\n",
      "\n",
      "A faulty inference is when you come up with a false conclusion based on valid evidence.\n",
      "\n",
      "This means that what you're seeing, experiencing, or understanding might be real, but the assumptions that you are piecing together from it are either not real or are highly unlikely.\n",
      "\n",
      "One example is a hasty generalization, which is when you make a claim about an entire group of people based on one or two experiences you've had. This is the bias at the base  of  a  lot  of  racism  and  prejudice.  Another  example is post hoc ergo propter hoc ,  which is what happens when you assume that because two things happened around the same time, they must be related, even if they aren't.\n",
      "\n",
      "A false dichotomy happens when you assume that there are only two possibilities that could be valid, when in reality,  there  are  far  more  that  you  simply aren't aware of. An example of this is when your boss calls you to a private meeting, and you assume you must either be getting\n",
      "\n",
      "a promotion or getting fired. A slippery slope, to play off of that example, is another false inference in which you assume that one event will set off a series of others, even if they certainly will not.\n",
      "\n",
      "These are just some of the myriad ways your brain can, in a sense, betray you. Though it intends to keep you alert and aware, sometimes, the threat becomes overinflated. Unable to decipher the difference, your body responds regardless.\n",
      "\n",
      "HOW DO I CORRECT THIS?\n",
      "\n",
      "Correcting faulty inferencing begins with first being aware that you're doing it. In the majority of cases, once you realize that you're thinking in a false dichotomy or making a hasty generalization, you stop doing it. You understand what it is, and you let it go.\n",
      "\n",
      "Training your brain to stop doing it automatically takes time. Think of your mind like a search engine that autofills your terms. If it's something you've input many times over the years, it's still going to come up for a while. You have to  work  on  consistently  adding  new  thoughts,  options, and stimuli to shift what it comes up with naturally.\n",
      "\n",
      "This is not only possible; it's inevitable. What you consistently do is what you adapt to. Your brain will start to reorient your comfort zone, and eventually it will feel as natural to think logically as it once did to think dramatically. It will feel as natural to be calm as it does now to\n",
      "\n",
      "feel anxious. It takes awareness, and it takes time. But it is always possible.\n",
      "\n",
      "WORRYING IS THE WEAKEST DEFENSE SYSTEM\n",
      "\n",
      "Rumination  is  the  birthplace  of  creativity. They're  controlled by the same part of the brain. 11\n",
      "\n",
      "That's the neurological reason there's a stereotype about 'depressed  creatives.'  Any  artist  will  tell  you  that  the toughest times in their lives  inspired  the  most  groundbreaking work. What they won't tell you, though, is that crisis is not necessary to function.\n",
      "\n",
      "Well, of course it's not, you're thinking. Crisis is the worstcase scenario. And yet how many of us place ourselves in a state of panic over fear of that 'least likely scenario' coming true? How many of us, in an effort to shield ourselves from panic, actually create a crisis out of our fear each day?\n",
      "\n",
      "We're  not  just  masochists.  We're  wildly  intelligent  unconsciously  functioning  beings.  Our  brains  understand something: If we imagine our worst fears, we can prepare for them. If we mull them over again and again, we can feel protected in a way. If we are ready for the storm, it can't hurt us.\n",
      "\n",
      "Except it can.\n",
      "\n",
      "Worrying excessively is not a malfunction. You are not of lesser  character  because  you  can't 'just  stop'  and 'enjoy life.' Worrying is a subconscious defense mechanism. It's what we do when we care about something so much we are equally terrified that it could hurt us, so we prepare to fight for it.\n",
      "\n",
      "What is the exact opposite of your fear? That's what you want. That's what you want so much that you'd go to the ends of your sanity to defend it.\n",
      "\n",
      "There's nothing wrong with you for thinking this way, but there's  also  nothing  wrong  with  you  for  being  ready  to move in a new direction.\n",
      "\n",
      "The reality is that worrying does not protect us in the way that we think it might. We cannot beat fear to the finish line. Worrying sensitizes us to an infinity of negative possible outcomes. It shifts our mindset to expect, seek out, and create a worst-case scenario. If a crisis were to occur, we'd start panicking, because our brains and bodies had been preparing for this epic war for a long time.\n",
      "\n",
      "Had we not premeditated these fears so excessively, we wouldn't be as impacted were they to actually happen. We'd see the situation for what it is and respond accordingly.\n",
      "\n",
      "That's where the nasty cycle forms: Once we worry ourselves sick over something that is totally delusional, and it doesn't happen-because, of course, it was never going\n",
      "\n",
      "to-we start to associate worry with safety. See? I thought this through so many times that I've avoided it.\n",
      "\n",
      "But that's not what's happening at all.\n",
      "\n",
      "Just  telling  someone  to  stop  worrying  and  be  present strengthens their impulse to be fearful, because you are effectively asking them to place their guards down. Making yourself feel more vulnerable when you're already at your edge is not the answer.\n",
      "\n",
      "Instead, you have to find a different way to feel safe.\n",
      "\n",
      "Rather  than  spending  your  time  rehearsing  how  much you'd panic if such-and-such a situation were to come to fruition, imagine how a third party would handle it if they were in your shoes. Imagine getting to the other side of the  issue,  perhaps  even  treating  it  as  an  opportunity  to create something you otherwise couldn't.\n",
      "\n",
      "Rather than spending your time shrinking yourself and your life out of fear of potentially confronting some kind of  hardship,  work  on  developing  your  self-esteem  and know that even if you were to fail, you wouldn't be judged, exiled, or hated in the way you fear.\n",
      "\n",
      "Rather than spending your life trying to identify the next thing to worry about and then 'overcome,' learn to move into  a  new  pattern  of  thinking  in  which  you  recognize that you don't need to balance out the bad with the good\n",
      "\n",
      "to live a full and fair life. Stability and wholeness, health and vitality are your birthright. You are allowed to have everything you want. You are permitted to be at peace.\n",
      "\n",
      "Worrying is so primal in the way that it fulfills a deep need within us to feel as though we've conquered, and thus are protected and saved. Yet at the same time, our discomfort with it is a higher aspect of ourselves informing us that it isn't necessary, and in fact, it's holding us back from the people we want and are meant to be.\n",
      "\n",
      "There's a better way to feed your emotional hunger, and it's not fighting yourself for your own inner peace.\n",
      "\n",
      "CHAPTER 5\n",
      "\n",
      "RELEASING THE PAST\n",
      "\n",
      "THROUGHOUT THE COURSE of our lives, we will routinely go through a process of self-reinvention.\n",
      "\n",
      "Over time, we are meant to change, and we are designed to evolve. Our bodies show us this as we eliminate and replace cells to the point that some argue we are essentially completely made 'new' again every seven years. 12\n",
      "\n",
      "Our mental and emotional growth follow a similar process, though it tends to occur much more often. It makes sense,  then,  that  some  of  our  most  profound  suffering comes from resistance to this natural process. We are in pain  because  though  we  must  change  our  lives,  we  are holding onto baggage and debris from the past. As we carry unresolved emotions from day to day, we gradually move our past trauma into our future lives.\n",
      "\n",
      "Releasing the past is a process, and a practice-one that we have to learn. This is where we begin.\n",
      "\n",
      "HOW TO START LETTING GO\n",
      "\n",
      "You cannot force yourself to let go, no matter how much you know you want to.\n",
      "\n",
      "Right now, you are being called to release your old self: your prior afflictions, past relationships, and all of the guilt from the time you spent denying yourself what you really wanted and needed out of life. Recovering from self-sabotage always necessitates a process of letting go.\n",
      "\n",
      "However, you cannot force something out of your brain space, no matter how much you don't want it to be there.\n",
      "\n",
      "You cannot simply loosen your grip, relax a little, and will yourself to stop thinking entirely about something around which your entire world used to orbit.\n",
      "\n",
      "This is not how it goes.\n",
      "\n",
      "You are not going to let go the moment someone tells you to 'move on,' the day you realize you have to admit certain defeat,  the  heart-dropping  second  it  occurs  to  you  that hope is, indeed, futile.\n",
      "\n",
      "You do not let go by simply willing yourself not to care anymore. This is something that people who have never been really, really hung up on something would assume. This is something that people who have never been deeply attached to something for a sense of safety and security and love and their future believe.\n",
      "\n",
      "There is nothing wrong with you because you almost get angry when people tell you to just 'let go' so nonchalantly, as though they couldn't fathom the storms in your head and heart.\n",
      "\n",
      "How can you become so passive about something you have spent so much of your time in your life actively working to maintain and then restore?\n",
      "\n",
      "You can't, and you don't.\n",
      "\n",
      "You start to let go on the day you take one step toward building a new life and then let yourself lie in bed and stare at the ceiling and cry for as many hours as you need.\n",
      "\n",
      "You start to let go on the day you realize that you cannot continue to revolve around a missing gap in your life, and going on as you were before will simply not be an option.\n",
      "\n",
      "You start to let go at the moment you realize that this is the impetus, this is the catalyst, this is that moment the movies are made about and the books are written around and songs are inspired by.\n",
      "\n",
      "This is the moment you realize that you will never find peace standing in the ruins of what you used to be.\n",
      "\n",
      "You can only move on if you start building something new.\n",
      "\n",
      "You let go when you build a new life so immersive and engaging and exciting, you slowly, over time, forget about the past.\n",
      "\n",
      "When we try to force ourselves to 'let go' of something, we grip onto it tighter, and harder, and more passionately than ever before. It's like if someone tells you to not think of a white elephant, that's the only thing you'll be able to focus on.\n",
      "\n",
      "Our hearts work the same way as our minds in this regard. As long as we are telling ourselves that we must let go, the more deeply we feel attached.\n",
      "\n",
      "So don't tell yourself to let go.\n",
      "\n",
      "Instead, tell yourself that you can cry for as long as you need. That you can fall to pieces and be a mess and let your life collapse and crumble. Tell yourself that you can let your foundation fall through.\n",
      "\n",
      "What you will realize is that you are still standing.\n",
      "\n",
      "What you build in the wake and the aftermath of loss will be so profound, so stunning, you will realize that maybe the loss was part of the plan. Maybe it awakened a part of you that would have remained dormant had you not been pushed the way you were.\n",
      "\n",
      "If you are certain that you cannot let go of what is hurting you, then don't.\n",
      "\n",
      "But take one step today, and another tomorrow, to rebuild a new life for yourself. Piece by piece, day by day.\n",
      "\n",
      "Because sooner or later, you're going to go an hour and realize you didn't think about them or it. Then a day, then a week…and then years and swaths of your life drift by and everything you thought would break you becomes a distant memory, something you look back at and smile.\n",
      "\n",
      "Everything  you  lose  becomes  something  you  are  profoundly grateful for. With time, you see that it was not the path. It was what was standing in your way.\n",
      "\n",
      "THE PSYCHOLOGICAL TRICK TO RELEASE OLD EXPERIENCES\n",
      "\n",
      "Just because an experience has ended doesn't mean it's over.\n",
      "\n",
      "We  store  unfinished  and  unresolved  emotional  experiences within our bodies. Cognitively, we often find that we are stunted by the time in our lives in which we were damaged or traumatized. We got scared, we never got over the fear, and as a result, we stopped growing.\n",
      "\n",
      "Often what we don't realize is that the experiences that hurt us most aren't usually the ones that we are indifferent about: There is something within them that we deeply wanted or still desire. We weren't broken by a breakup; we were broken by wanting love that wasn't right for us. We weren't devastated by a loss; we were devastated because we wanted, so badly, for that person or thing to remain in our lives.\n",
      "\n",
      "We mentally become trapped in these places from which we still crave an experience. What we don't realize is that we have to sort of free ourselves from it so that we can go forth and create it in real time.\n",
      "\n",
      "Instead of accepting the ways we think life did not work out,  we  have  to  be  able  to  see  what  was  at  the  core  of our desire and figure out a way to still give ourselves that experience now.\n",
      "\n",
      "If you truly want to let go of a past experience, you have to reenter it through your memory. Close your eyes and find the feeling in your body that is uncomfortable.\n",
      "\n",
      "This is your portal to its root. Follow the feeling and ask it to show you where it started. You'll remember a time, place,  or  experience.  Sometimes,  the  memory  is  fresh enough that you don't need to do this, and you can simply reenter the memory by imagining that you are back where it all began.\n",
      "\n",
      "Now what you have to do is to superimpose a narrative to your younger self. You need to imagine that you, your healed and happy older self, is imparting some wisdom.\n",
      "\n",
      "Imagine sitting next to your younger self as they got their heart  broken  and  giving  them  very  specific  instructions about why this is absolutely for the best and although they cannot know it yet, there is another relationship out there that is far, far better.\n",
      "\n",
      "Imagine sitting next to your younger self when they felt really  down  and  giving  them  the  exact  instructions  regarding what they need to do to feel better: who they need to call, where they need to go, what they need to begin doing, and what they need to stop doing.\n",
      "\n",
      "Most importantly, imagine telling your younger self that absolutely everything-yes, everything-is going to be okay. That their fears are largely unfounded, that good things are coming, and that life will turn out well in the end.\n",
      "\n",
      "You  have  to  do  this  to  release  the  old  attachment  and allow that part of yourself to reattach to the present moment and what exists within it.\n",
      "\n",
      "Though you cannot change what happened in the past, by shifting your perspective of it, you can change how you are right now. You can change the story, and you can change your life. You can stop holding onto the old life in which you were required to be someone you inherently are not.\n",
      "\n",
      "The  truth  is  that  when  we  are  unhealthily  attached  to something in the past, our perspective of it is often distorted. We aren't  seeing  reality  for  what  it  was,  and  we need  to  assist  ourselves  in  being  able  to  broaden  our mindset and open up to the truth. Instead of longing for what we didn't get then, we have to release ourselves from the past and start putting our energy into building that experience right now.\n",
      "\n",
      "When we do this, we become free to step into the field of infinite potential. We become free to be who we always wanted,  to  create  what  we  always  wanted,  and  to  have what we always wanted. The time is now, and the place is here.\n",
      "\n",
      "Ruminating over the past doesn't mean you want to return to it.\n",
      "\n",
      "Not  being  able  to  forget  what  happened  doesn't  mean you're  content  to  keep  reliving  it  again  and  again,  even though right now, you very much are.\n",
      "\n",
      "The wildest thing about life is how unassumingly it keeps moving. You lose the person closest to you and the world affords you a few days of grieving, and then you're expected to just keep going. Y ou go through something so life-shifting,  mind-altering,  and  deeply  traumatic,  then  find  that society only has a small bandwidth for tolerating your fear.\n",
      "\n",
      "Here's what you're allowed: You're okay to cry and you're forgiven for being sad or canceling a few plans here and there. You're permitted a few days off of work and someone to listen to you vent a handful of times.\n",
      "\n",
      "But  processing  and  accepting  the  gravity  of  something that touched every last inch of you is not something you can  do  on  a  mental-health  day.  It's  not  something  the world affords you enough time for, and so you botch the job. You carry on.\n",
      "\n",
      "One day, you wake up and discover that by every identifiable measure, you have moved on. You're so many miles from where you started, you can't even remember it clearly.  What  you're  underestimating is  the  fact  that  though you can leave a place, or a person, or a situation…you can't leave yourself.\n",
      "\n",
      "Why would it ever come as a surprise that you keep thinking about the past? You weren't given the opportunity to shine a light on that particular darkness and deem it okay. You weren't given much of anything at all.\n",
      "\n",
      "When your mind is stuck in the past, it isn't because it wants to return there; it's because you were impacted far more deeply than you ever realized, and the aftershocks are still rippling through you.\n",
      "\n",
      "They surface as thoughts here and there, but under the surface  is  a  deep  echo  that  has  the  power  to  place  you right back where you were as though you never left.\n",
      "\n",
      "You can leave the country, get remarried, build a whole new  career,  date  12  other  people,  find  an  entirely  new friend group, feel happier and more fulfilled than ever, and still grieve for what your younger self went through.\n",
      "\n",
      "Even though you're different on the outside, that part of you still very much exists within. That younger self doesn't just want you to keep walking; it wants you to turn around and acknowledge it.\n",
      "\n",
      "You will, with time.\n",
      "\n",
      "You are not wrong or broken for feeling the way that you do. You responded to your circumstances as any healthy person would have. If anyone else was in your shoes, they would have reacted the exact same way. They would feel the exact same way.\n",
      "\n",
      "You were a healthy person who went through something traumatic and responded accordingly.\n",
      "\n",
      "You are someone who moved on because they had to, but who wasn't sick enough to disassociate entirely from the past.\n",
      "\n",
      "The fact that you can still recall what happened is a signal that you're healthier than you think, more willing to heal than you realize, and more forgiving than you ever imagined you could be. Everything that's haunted you is rising in your consciousness so you can see it and bow out with grace.\n",
      "\n",
      "You are not the person you were, even if all those pieces are still very much a part of you.\n",
      "\n",
      "You are not broken for being in pain; you're seeing yourself out of it.\n",
      "\n",
      "LETTING GO OF UNREALISTIC EXPECTATIONS\n",
      "\n",
      "It is not that brave to say you love your body only after you've contorted it to precisely what you want it to look like.\n",
      "\n",
      "It is not that brave to say you don't care about possessions when you have access to everything in the world.\n",
      "\n",
      "It is not that brave to say you aren't motivated by money when you have enough of it.\n",
      "\n",
      "When  you  only  find  happiness  and  peace  after  you've fixed every flaw, mastered every challenge, and are living decidedly in the 'after' part of the picture of your life, you have not resolved anything.\n",
      "\n",
      "You have only reinforced the idea that you cannot be okay until everything is perfect.\n",
      "\n",
      "The truth is that you do not change your life when you fix every piece and call that healing.\n",
      "\n",
      "You change your life when you start showing up exactly as you are. You change your life when you become comfortable  with  being  happy  here,  even  if  you  want  to  go forward. You change your life when you can love yourself even though you don't look exactly the way you want to. You change your life when you are principled about money and  love  and  relationships,  when  you  treat  strangers  as\n",
      "\n",
      "well as you do your CEO, and when you manage $1,000 the same way you would $10,000.\n",
      "\n",
      "You change your life when you start doing the truly scary thing, which is showing up exactly as you are.\n",
      "\n",
      "Most of the problems that exist in our lives are distractions from the real problem, which is that we are not comfortable in the present moment, as we are, here and now.\n",
      "\n",
      "So we must heal that first. We must address that initially. Because everything else builds from it.\n",
      "\n",
      "We must be brave and confront our discomfort, sit with it even if it churns our stomachs and pinches our faces and makes us certain we will never find a way out. (We will.)\n",
      "\n",
      "We must listen to what's wrong, feel it, move through it, allow it to be.\n",
      "\n",
      "The truth is that this discomfort is the true problem, and we are running around trying to fix one thing after another because those are all just symptoms.\n",
      "\n",
      "If we become okay with money, we're onto our bodies. If we're okay with our bodies, we're onto our relationships. Once we master all the things we care about, we start at the beginning, we try to level up, to change, to fix, to identify a problem that is any problem but the actual problem at hand.\n",
      "\n",
      "When you start showing up as exactly who you are, you start radically changing your life.\n",
      "\n",
      "You start  receiving  authentic  love. You  start  doing  your best  and  most  profitable  and  effortless  work.  You  start laughing;  you  start  enjoying  things  again.  You  start  realizing  that  you  just  needed  anything  to  project  all  this fear onto, so you chose the most vulnerable and common issues in life.\n",
      "\n",
      "When you start showing up exactly as you are, you cut the bullshit.\n",
      "\n",
      "You declare to the world that you will not only love yourself when it sees you as worthy.\n",
      "\n",
      "You will not only have values when you have everything you could ever need.\n",
      "\n",
      "You will not only be principled once you get where you want to be.\n",
      "\n",
      "You will not only be happy once someone loves you.\n",
      "\n",
      "When you show up as you are, you disrupt this pattern.\n",
      "\n",
      "The goodness of life is no longer reserved for some version of you that you'll probably never be.\n",
      "\n",
      "This was always a game for you to explain to yourself why\n",
      "\n",
      "it is you didn't feel good naturally, before you knew how to start showing up and allowing your feelings. When you still  lived  in  the  darkness,  you  had  to  suppress that and project it onto other issues. No longer.\n",
      "\n",
      "You are showing up as you are today and taking what's yours,  not  what  belongs  to  some  imaginary  version  of yourself. Not what you think the world thinks you're worthy of. You, here, now.\n",
      "\n",
      "That is the true healing.\n",
      "\n",
      "In  fact,  the  universe  does  not  allow  perfection.  Without breaks and gaps, there would be no growth. Nature depends on imperfection.  Fault  lines  make  mountains,  star  implosions become supernovas, and the death of one season creates the rebirth of the next.\n",
      "\n",
      "You are not here to live up to the exact expectation that you've mustered up in your head. You are not here to do everything precisely right and precisely on time. To do so would require stripping your life of spontaneity, curiosity, and awe.\n",
      "\n",
      "WHAT LEAVES THE PATH IS CLEARING THE PATH\n",
      "\n",
      "There is nothing that you can do to win someone or something that is not meant to be yours.\n",
      "\n",
      "You can fight with everything you have. You can hold on for as long as you can. You can force yourself into mental gymnastics to pick apart signs. You can have your friends read into texts and emails. You can decide that you know what's best for you and right for you. Mostly, you can wait.\n",
      "\n",
      "You can wait forever.\n",
      "\n",
      "What isn't right for you will never remain in your life.\n",
      "\n",
      "There is no job, person, or city that you can force to be right  for  you  if  it  is  not,  though  you  can  pretend  for  a while. You can play games with yourself, you can justify and make ultimatums. You can say you'll try just a little longer,  and  you  can  make  excuses  for  why  things  aren't working out right now.\n",
      "\n",
      "The truth is that what is right for you will come to you and stay with you and won't stray from you for long. The truth is that when something is right for you, it brings you clarity,  and  when  something is wrong for you, it brings you confusion.\n",
      "\n",
      "You  get  stuck  when  you  try  to  make  something  that's wrong for you right. When you try to force it into a place in your life in which it doesn't belong. You get split; you breed this internal conflict which you cannot resolve. The more it intensifies, the more you mistake it for passion. How could you ever feel so strongly about something that isn't right?\n",
      "\n",
      "You can, because you can use your mind to get attached. You can fall in love with potential as opposed to reality. You can orchestrate and choreograph dances of how you'll live out your days when things finally settle into their rightful place. You can hinge on a fantasy life in which everything you think you want has taken root in your everyday life.\n",
      "\n",
      "But if  it  isn't  showing  up,  it's  just  that-a  fantasy.  And when we start to deeply believe in an illusion, it becomes a delusion. And a delusion can be a really compelling thing.\n",
      "\n",
      "The truth is that what is not right for you will never remain with you. Though you might want to pretend that you don't know if this is the case, you do. You can feel it. It's why you have to grip so hard and with so little give. The things that are right for you can be free from you. You don't have to convince them that they are right. You don't have  to  line  up  the  evidence  as  though  you're  pleading your case.\n",
      "\n",
      "Sometimes, we get lost in old dreams. We get lost in the lives others wanted us to have. We get stuck on what we thought we should be, what we assumed we would have. We get derailed by all the ideas floating around our heads about what it could be and should be if only things were different, if only everything would click.\n",
      "\n",
      "That's why life gives us this kind of insurance. Sometimes, it pulls away from us what is wrong for us when we are not willing to see it for ourselves.\n",
      "\n",
      "Because the truth is that we do not want what is not right for us; we are simply attached to it. We are simply afraid. We are simply stuck in the assumption that nothing better will replace it, that its absence will open up a well of endless, infinite suffering for which there will be no solution. We do not want what is not right for us; we are just scared to let go of what we believe will make us secure.\n",
      "\n",
      "The funny part is there is nothing that makes us more insecure than hanging around what isn't right for us. There's nothing that will collapse faster. There's nothing that will bring us inner turmoil quite like it.\n",
      "\n",
      "What is not right for you will never remain in your life, and not because there are forces beyond us navigating the minutiae of our everyday lives. What is not right for you will not remain with you because deep down, you know it's not right. Y ou are the one who eventually lets go, sees reality, and walks away. You are the one resisting, you are the one holding back, you are the one concocting healing fantasies about how great it will be when you force something wrong to finally be right.\n",
      "\n",
      "What is not right for you does not remain with you because you don't want it, and so you don't choose it. You step away when you are ready, you let go when you are able, and you realize, all along, that all you were really in love with was a little trick of the light that made you feel safe.\n",
      "\n",
      "RECOVERING FROM EMOTIONAL TRAUMA\n",
      "\n",
      "You might think trauma is in your head in the metaphorical sense. It is actually in your body in the literal sense.\n",
      "\n",
      "Trauma is what happens when something scares you and you  do  not  get  over  that  fear.  If  you  do  not  resolve  or 'defeat' it, you get into, and remain in, a sustained state of  fight-or-flight,  which  is  essentially  the  human  panic response for survival.\n",
      "\n",
      "Trauma is the experience of disconnecting from a fundamental feeling of safety. Unless you are able to reestablish that  connection,  a  particularly  destructive  bias  distorts your worldview: You become hypersensitive, which means that you will ascribe intent, overthink, overreact, become triggered by innocuous stimuli, personalize neutral situations, and remain in a mental 'combat mode.'\n",
      "\n",
      "After  experiencing  trauma,  your  brain  will  rewire  itself temporarily to seek out the potential 'threat' in anything, which makes it very difficult to both move on from the initial problem and then not to develop a victim complex. After all, your brain is literally trying to show you every imaginable way the world could be 'out to get you.'\n",
      "\n",
      "This is why exposure is so effective as a treatment for fear or anxiety. By gradually reintroducing the stressor into someone's life-and showing them that they are able to handle\n",
      "\n",
      "it-the brain is able to return to a neutral state because a feeling of control and security is being reestablished.\n",
      "\n",
      "This is also why people who have stronger social ties and mental resilience prior to a traumatic event are more likely to use the event as a catalyst for self-reflection, growth, compassion, and healing as opposed to self-destruction. They had multiple ties to that essential feeling of 'safety,' so even if one was eroded or severed, others still were there to support them.\n",
      "\n",
      "What happens to your brain after a traumatic event?\n",
      "\n",
      "Neurologically, we process stress in three parts of the brain. 13\n",
      "\n",
      "The first is the amygdala, the second is the hippocampus, and the third is the prefrontal cortex. Individuals suffering from post-traumatic stress disorder (PTSD) have a smaller  hippocampus  (the  center  of  emotion  and  memory), increased  amygdala  function  (the  center  of  rumination and creativity), and decreased medial prefrontal/anterior cingulate function (the center that governs complex behaviors like planning and self-development).\n",
      "\n",
      "It becomes clear, then, why trauma tends to have the following impact on us:\n",
      "\n",
      " · Our brains stop processing memory fully, leaving us with fragments of what happened, sometimes contributing to the feeling of disassociation.\n",
      "\n",
      " · Our ability to manage a range of emotions decreases.\n",
      "\n",
      " · We become stifled and stuck, have trouble planning for the future, and our self-development and actualization come to a halt.\n",
      "\n",
      " · When we enter a state of fight-or-flight, our body literally  ceases  any  advanced  function  that  is  not necessary for our survival. The body's main receptors become extremely sensitive and reactive to stimuli. This is a beautiful and essential part of being human; it's kept us alive as a species. However, it is not a state that is meant to be sustained.\n",
      "\n",
      "Centuries  ago,  when  we  were  at  the  lower  rung  of  actualization, or the bottom of Maslow's hierarchy, 14 what concerned us most was physical survival. Today, our focus is primarily on self-actualization and meaningfulness and trying to feel 'safe' through social acceptance, money, or mental acuity.\n",
      "\n",
      "With all of this grey area, it  seems  obvious  that  more people  would  be  mentally  and  emotionally  struggling than they did prior, despite having more physical challenges to overcome.\n",
      "\n",
      "Recovery comes down to something very simple, which is restoring the feeling of one's safety.\n",
      "\n",
      "However, the most important part of this restoration is\n",
      "\n",
      "that you must reestablish a feeling of safety in the exact area of life that traumatized you.\n",
      "\n",
      "Often, if someone is traumatized by a relationship they had when they were young, they will reinvest that energy into valuing being attractive or successful. To them, they believe that if they are 'good enough,' they can never be denied or rejected again. However, we all know this is not how this works. It actually makes us have unhealthy and destructive attachments to these things.\n",
      "\n",
      "If we are traumatized by a relationship, we restore the feeling of safety by working on other healthy, safe relationships.\n",
      "\n",
      "If we are traumatized by money, we restore the feeling of safety by doing what we must to ensure we have enough and by saving for an emergency expense.\n",
      "\n",
      "If we are traumatized by job loss, we restore the feeling of safety by having a backup plan or a side gig in line in case it were to happen again.\n",
      "\n",
      "If we are traumatized by being bullied, we restore the feeling of safety by finding new friends.\n",
      "\n",
      "What  most  people  try  to  do  is  overcompensate  in  an area of life that is not the real problem. For example, if they struggled in relationships, they hoard money to keep themselves feeling 'safe.' Of course, this is always futile, because the problem never gets solved.\n",
      "\n",
      "Your trauma is not 'in your head'; it is literally a changed state in your brain, and the only way you will help your body to return to its actual state is by recreating the feeling of safety that allows you to 'turn off ' survival mode and return to normal life.\n",
      "\n",
      "RELEASING EMOTIONAL BACKLOG\n",
      "\n",
      "Your emotional backlog is like your email inbox.\n",
      "\n",
      "It  might  be  a  simple  analogy,  but  it's  an  effective  one. When you experience emotions, it's as though you're getting  little  messages  from  your  body  stacking  up  one  at a time. If you don't ever open them, you end up 1,000+ notifications deep, totally overlooking crucial information and important insights that you need to move your life forward. At the same time, you can't sit around all day and respond to every message just as it comes up; you'd never get anything done.\n",
      "\n",
      "It is a mistake to assume that emotions are optional experiences. They are not. But we are masters of avoiding our feelings, and we do it in so many ways. Often, we rely on substances that physically numb us, projections and judgments that place the attention on someone else's faults as opposed to our own, all kinds of other worldly pursuits, and on the most basic level, tensing our bodies up so efficiently that we are rendered incapable of feeling.\n",
      "\n",
      "Psychologically, you probably know that this doesn't work for  long.  The  backlog  starts  to  jam  eventually.  Y ou  are forced to sit and be still and sleep and cry and feel it all.\n",
      "\n",
      "I wish there were some poetic, mystical truth to share here, but there isn't. There is only your anatomy, the physiology of what's happening inside you when you feel.\n",
      "\n",
      "Emotions are physical experiences. We flush our bodies of  everything,  and  regularly  so.  We  defecate,  we  sweat, we  cry,  we  literally  shed  our  entire  skin  once  a  month. Feelings are no different; they are experiences that must likewise be released.\n",
      "\n",
      "When not felt, emotions become embodied. They become literally  stuck  in  your  body.  This  is  because  they  have something called a motor component, which means that the minute they begin-before you can suppress or ignore them-they create a micro-muscular activation.\n",
      "\n",
      "Our bodies respond instantaneously.\n",
      "\n",
      "We often store pain and tension in the area of the body where an expression began but was never fully materialized.\n",
      "\n",
      "This is because, neurologically speaking, the part of your brain  that  regulates  emotions,  the  anterior  cingulate, is  next  to  the  premotor  area,  which  means that when a feeling is processed, it immediately begins to generate a physical, bodied response. The premotor area connects to\n",
      "\n",
      "the  motor  cortex  and  then  spans  back  into  the  specific muscles that are going to express the emotion.\n",
      "\n",
      "Which muscles express which emotion? Well, it depends.\n",
      "\n",
      "We have a lot of language that clues us into where we have  physical  reactions  to  emotions.  We  often  feel  fear in  our  stomachs (think of a nervous stomach, or a 'gut instinct')  and  heartache  in  our  chests  (that's  where  the whole 'broken heart' thing comes from), stress and anxiety in our shoulders (think of the 'weight of the world on your shoulders'), and relationship problems in the neck (think 'they are a pain in the neck').\n",
      "\n",
      "But it actually goes even deeper than this. Let's say that someone did something to you that crossed a boundary, and your instinct was to yell at them. However, because you understood it was not effective to literally scream, you held back. Though this may have been the right thing to do in the moment, your body may be storing residual tension in the neck or throat area. In other cases, people can experience psychosomatic effects of their emotions that are a bit more abstract, such as pain in their knees or feet when they are traumatized by 'moving forward' in their lives, and so on.\n",
      "\n",
      "The truth is that our bodies are speaking to us in voiceless symbols. If we can learn to interpret what they are saying, we can heal ourselves in an entirely new way.\n",
      "\n",
      "So you know that emotions sometimes get stored in your body when they are not fully expressed. Be this as it is, how do we begin to flush ourselves from them?\n",
      "\n",
      "There are a number of strategies that you can use to do this, and what matters is that it's effective for you. There is no one-size-fits-all, but there are a few options that tend to work well for most people, particularly when they are used in tandem.\n",
      "\n",
      "STOP MEDITATING TO FEEL CALM; START MEDITATION TO JUST FEEL .\n",
      "\n",
      "I know that this goes against everything you've ever heard about meditation. But it is actually the point of meditation. If you sit down for a 10-minute session and try to force yourself to be relaxed and light, you are effectively doing the exact same type of suppression that likely gave you the need to meditate in the first place.\n",
      "\n",
      "Instead, the point of meditation is to sit idly as you experience all of those feelings come up: the rage, the fear, the sadness, the overwhelming mind chatter…and in spite of how alluring or triggering it may be, you learn to stay still and not respond to it. You learn to allow these thoughts and feelings to come up and then pass by virtue of you not reacting to them.\n",
      "\n",
      "This takes practice.\n",
      "\n",
      "USE BREATH SCANS TO FIND RESIDUAL TENSION IN THE BODY.\n",
      "\n",
      "It usually doesn't take too much extra effort to figure out where in your body you are storing pain. You feel it. It's in your chest, your stomach, your shoulders, whatever is bothering you.\n",
      "\n",
      "However,  if  you  aren't  sure,  or  if  you  want  to  zero  in specifically on where that pain is, do something called a breath scan. In this, you will breathe in and out slowly, and without  taking  a  break  between  breaths.  When  you  do this, you will begin to notice that you might hit a 'snag' or hiccup somewhere, that in the process of taking your breath, you will start to feel precisely where in your body you are storing tension.\n",
      "\n",
      "Once you know, you can start to go into that feeling more, visualizing  what  it  is,  where  it  came  from,  and  what  it needs you to know. Often in this scenario, we are brought back  to  specific  memories  or  past  versions  of  ourselves that need assistance or guidance. Use a journal to write down what you experience and see, and remember that the body often speaks in metaphors, so don't necessarily take everything literally.\n",
      "\n",
      "SWEAT, MOVE, CRY.\n",
      "\n",
      "The last, the hardest, and the most important part of releasing your emotions is really the only thing you have to do...you have to feel them.\n",
      "\n",
      "Sometimes, this means allowing yourself to feel like total shit.  Sometimes,  this  means  pushing  yourself  through  a workout, yoga, stretching, walking, or confronting triggering thoughts and letting yourself cry out what's bothering you.\n",
      "\n",
      "Remember that emotional  health  is  not  the  experience of  being  perpetually  calm  and  happy  all  of  the  time.  It is  the  experience  of  allowing  a  range  of  emotions,  both good and bad, and not getting too stuck on either one. Similarly, mental health and self-mastery is the ability to see and feel and experience a thought without responding to it. The response, or lack thereof, is where we regain our power and reclaim our lives.\n",
      "\n",
      "You were not born to be perfect.\n",
      "\n",
      "You were not born to be happy all of the time.\n",
      "\n",
      "But  if  you  can  commit  each  day  to  doing  the  work  of being fully human and feeling even when you are afraid, you can transcend in a way that is truly beautiful.\n",
      "\n",
      "WHAT IT REALLY MEANS TO HEAL YOUR MIND\n",
      "\n",
      "Healing your mind is not the same thing as healing your body.  When  you're  physically  wounded,  you  often  go through a progressive, linear repair. You get better until one day you are nearly back to where you were before.\n",
      "\n",
      "Healing your mind is completely different, because you aren't returning to what you were before. You are gutting yourself and becoming someone entirely new.\n",
      "\n",
      "If that seems a little bit violent and harsh, it should. Healing is not a lovely ascension into comfort and wellness to be experienced once and forevermore. Healing yourself is the most uncomfortable, disruptive, important thing you will ever do.\n",
      "\n",
      "Healing yourself is returning to your most natural state, which is hungry for personal freedom, irreverent to the suffocating  opinions  of  others,  creates  without  doubt, shows up without fear, and loves without stipulations and agreements and conditions. Who you truly are is at once the best version of yourself you might not have ever imagined and the most essential version of yourself that you have always been.\n",
      "\n",
      "And getting to that place? It requires a lot.\n",
      "\n",
      "Healing requires you to take an honest inventory of your grudges and aggressions and the wells of longing and fear you've been ignoring all this time. It requires you to take stock of precisely what is wrong with your life so you can work to make it right. It requires you to be completely honest about how you really feel, and then it requires you to actually feel it.\n",
      "\n",
      "Healing requires you to feel the deep heartache lingering\n",
      "\n",
      "in  you  instead  of  subconsciously  re-creating  the  experience so you have an outlet to release it. Healing is no longer trying to sanitize your experience, to cleanse it until it is made perfect.\n",
      "\n",
      "Healing  requires  you  to  go  through  the  full  expression of every emotion that you cut off and buried when you decided you were no longer comfortable with it. Healing requires you to face every ounce of darkness within you, because just beneath what appears to be an impermeable barrier is complete, radical, total freedom. When you are no longer scared to feel anything, when you no longer resist any one part of your life, something magical happens: You find peace.\n",
      "\n",
      "Let's  be  clear:  Y ou  are  not  going  to  suffer  forever. This is  not  going  to  hurt  for  long.  But  to  trick  yourself  into thinking that healing is getting progressively better until you have unraveled all of your past experiences and can return to the version of yourself you were before you got hurt...well, that is to miss the point entirely.\n",
      "\n",
      "We are meant to go through these periods of what some refer to as positive disintegration. It is when we must adapt our self-concept to become someone who can handle, if not thrive, in the situation that we are in.\n",
      "\n",
      "This is healthy. This is normal. This is how we are supposed to respond.\n",
      "\n",
      "But we cower, because it will be uncomfortable. It will not immediately give us the virtues of what we are taught is a worthwhile life: comfort and ease and the illusion that everything is perfect on the surface.\n",
      "\n",
      "Healing is not merely what makes us feel better the fastest.  It  is  building the right life, slowly and over time. It is  greeting  ourselves  at  the  reckoning,  admitting  where we've faltered. It is going back and resolving our mistakes, and going back within ourselves and resolving the anger and fear and small-mindedness that got us there in the first place.\n",
      "\n",
      "Healing is refusing to tolerate the discomfort of change because you refuse to tolerate mediocrity for one second longer. The truth is that there is no way to escape discomfort; it finds us wherever we are. But we are either going to feel uneasy pushing past our self-imposed limits, breaking boundaries and becoming who we dream of being, or we're going to feel it as we sit and mull over fears we fabricated to justify why we refuse to stand up and begin.\n",
      "\n",
      "Healing is going to be hard at first. It is going to mean looking at yourself honestly, maybe for the first time ever. It  is  going  to  mean  stepping  out  of  your  comfort  zone so you can leap toward the person you want to be. It is not what makes you more comfortable and idle. It is what conditions you to be more motivated by discomfort than you are scared of it, and inspired by your still moments more  than  you  use  them  to  forge  the  chains  of  worry.\n",
      "\n",
      "Healing is going to change everything, but it has to start with you being willing to feel what you are afraid to feel.\n",
      "\n",
      "Let's be clear about something: Becoming the best version of yourself is your natural inheritance. It is what you are born to do. Healing is simply releasing the sickness that is the limiting beliefs and fears that are holding you back from doing exactly that.\n",
      "\n",
      "Healing is not about going back to exactly who you were before,  because  that  person  wasn't  yet  capable  of  seeing the storm before it hit, and that person didn't know how to shield themselves from it.\n",
      "\n",
      "You aren't supposed to go back to being naive, less jaded, or more unaware. You aren't meant to return to your blissful  mindlessness, a life in which you didn't know about the contrasts, the pain, all of the good and bad that life can throw at you.\n",
      "\n",
      "What you get on the other side of healing is greater than that;  you  just  haven't  experienced  it  yet  to  know.  What you  get  for  going  through  something  painful  is  that you  become  more  resilient,  more  self-sufficient,  more empowered.\n",
      "\n",
      "You realize that nothing will save you, and so you must begin the work of saving yourself, which is the entire purpose of your life.\n",
      "\n",
      "When you begin this work, you find your inner strength. You realize that you have power and influence and you can strategize and redirect your life. Y ou realize that instead of what you can't control, your life can be built on what you do.\n",
      "\n",
      "When you heal, you become stronger where you've been broken. You become grounded where you've been egotistical. Y ou become responsible where you've been neglectful. You become more sensitive and able and conscious. You become more considerate, you are more empathetic, you are more mindful, more careful.\n",
      "\n",
      "But what you don't need to be more of is afraid.\n",
      "\n",
      "Fear is not going to protect you. Action is. Worrying is not going to protect you. Preparing is. Overthinking is not going to protect you. Understanding is.\n",
      "\n",
      "When we hold onto fear and pain after something traumatic has passed, we do it as a sort of safety net. We falsely believe that if we constantly remind ourselves of all the terrible  things  that  we  didn't  see  coming,  we  can  avoid them. Not only does this not work, but it also makes you less efficient at responding to them if they do.\n",
      "\n",
      "Because most of the time, you're so busy worrying about monsters in the closet, you forget to address the actual things that will erode you over time: your health, your relationships, your long-term vision, your finances, your thoughts.\n",
      "\n",
      "When you heal completely, you stop tolerating discomfort. When something is wrong, you recognize that it is wrong and take action to fix it because you've seen what happens when you don't.\n",
      "\n",
      "When you heal completely, you are able to think ahead and  rationally  consider  cause  and  effect.  Y ou  recognize that your actions will generate results, and if you want to better control the outcomes of your life, you have to better adjust your habits.\n",
      "\n",
      "When you heal completely, you realize that there is nothing more important than being able to enjoy where you are, right here and right now. Whatever obstructions are in the way of you being present and savoring your life are the challenges you have to face.\n",
      "\n",
      "Because life is quick, and it's temporary. What you have now you could lose tomorrow, and gripping it so tightly, binding it up with resistance doesn't mean it's safer. It means that when the day comes that it passes-as does everything, as does everyone-you will realize you never really enjoyed it.\n",
      "\n",
      "And healing? It's about getting to a place where you prioritize nothing over the quality of your one, short life.\n",
      "\n",
      "MOVING FORWARD ISN'T ABOUT GETTING REVENGE\n",
      "\n",
      "Your glow up might not be something others can see. It might not come across as a shift on the surface.\n",
      "\n",
      "In a world of revenge bodies and comeback relationships, a world that tries to tell you that your ultimate transformation should be splayed out across your Instagram feed, we've  lost  what  it  really  means  to  heal,  to  improve,  to move on with our lives.\n",
      "\n",
      "The real glow up isn't proving the people from your past wrong. It is finally feeling so content and hopeful about your future that you stop thinking about them entirely.\n",
      "\n",
      "When you want to change your life so it looks different, and only that, you are still orbiting around the opinions of people who didn't love you and didn't have any intention to.\n",
      "\n",
      "You can always tell the difference, too. People who have truly  transformed  are  not  concerned  solely  with  how things appear. Their lives are now focused intently on how things feel, how they really are underneath it all.\n",
      "\n",
      "A real glow up is authentic. It is lifting off all the cover-up bullshit and addressing the real problems. It is healing. It is changing, for good. It is, for the first time, prioritizing your heart over someone else's eyes.\n",
      "\n",
      "Anyone  can  piece  together  an  image  that  looks  better. Anyone can edit and filter and lay picture after picture, side by side, to create a narrative, a story, a semblance of the whole. Anyone can buy their way into beauty, anyone can look nicer if they really try, anyone can convince you that they are doing better than they really are.\n",
      "\n",
      "If they are so intent on trying to prove that, it is probably because they are still so empty inside.\n",
      "\n",
      "What if you weren't worried about whether or not you look bigger or smaller or nicer or better than you did 10 years ago?\n",
      "\n",
      "What if you were more concerned about whether or not you gain self-respect,  real  relationships,  emotional  freedom, mental clarity,  a  job  you  appreciate,  work  you  respect, a kinder and more empathetic disposition?\n",
      "\n",
      "What if your accomplishments were not something you could photograph or measure, nothing you could loosely try to communicate across some pixels and status updates? How are you feeling today? Better than you did yesterday? More whole, more confident?\n",
      "\n",
      "The truth is that there is no before and after in life. We are always in a process of shedding and becoming. That snapshot  moment you're waiting for, that instance in which someone dares to look you up again and sees, finally, that you are thriving...is a game for you, and you alone.\n",
      "\n",
      "Nobody is looking at you the way you think they are. Nobody is thinking about you the way you wish they would. They are looking at themselves. They are thinking about themselves.\n",
      "\n",
      "They are reading themselves.\n",
      "\n",
      "This isn't sad; it's freeing. This should be the crux of your ultimate liberation.\n",
      "\n",
      "The  truth  is  that  you  have  nobody  to  prove  wrong  but yourself. The people from your past probably didn't disapprove of you nearly as much as you feared they did.\n",
      "\n",
      "This closure is for you. This growth is for you. This change is yours. This is you vs. you, you meeting you, you seeing you for the first time. This is about you becoming who you know you can be. This is about you finally living up to your potential.\n",
      "\n",
      "But mostly, this is about you recognizing that you were not your best self before.\n",
      "\n",
      "You didn't behave the way you wish you would have.\n",
      "\n",
      "You didn't do what you should have.\n",
      "\n",
      "You weren't what you hoped you'd be.\n",
      "\n",
      "Whenever  we  want  desperately  to  prove  someone  else\n",
      "\n",
      "wrong, we are really trying to quell our own lingering disappointment that we didn't live up to our own expectations.\n",
      "\n",
      "So remember this: The next time you're trying to craft a glow up story that is compelling to others, ask yourself why you are still waiting for their approval.\n",
      "\n",
      "The answer, almost always, is that you still do not have your own.\n",
      "\n",
      "CHAPTER 6\n",
      "\n",
      "BUILDING A NEW FUTURE\n",
      "\n",
      "NOW THAT YOU HAVE done the challenging work of beginning to release your past experiences, you must turn your attention toward building a new present and future. When we release, we are wiping the slate clean to create something better.\n",
      "\n",
      "One of the most common pitfalls of people who try but do not succeed in releasing their past is that their focus remains on just that-the past. The work now is to envision who you want to be, connect with the most powerful version  of  yourself,  design  your  life  through  your  daily routine, and uncover your true purpose for being.\n",
      "\n",
      "MEETING YOUR HIGHEST POTENTIAL FUTURE SELF\n",
      "\n",
      "A popular tool in psychotherapy is something called inner child work 15 , or the process of imagining and reconnecting with your younger self. In this process, you can offer yourself\n",
      "\n",
      "guidance,  even  going  back  to  certain  traumatizing  events and readdressing them with the wisdom you have now.\n",
      "\n",
      "But  more  often,  the  process  of  reconnecting  with  your inner child is to let them communicate with you. It is how you can rediscover your inherent desires, passions, fears, and feelings.\n",
      "\n",
      "The process is akin to reverse engineering, which is when you  identify  the  end  goals  for  your  life  and  then  work backwards  to  see  what  you  need  to  do  each  day,  week, month, and year to get there. However, it works the opposite way as well. You can use a visualization technique to connect to your highest potential future self.\n",
      "\n",
      "STEP 1: FACE THE FEAR FIRST\n",
      "\n",
      "Sit down in a quiet place with a journal. Make sure you're doing it at a time when you feel relaxed and open to receive guidance. If you go into this with fear, you are going to get fear.\n",
      "\n",
      "Next, close your eyes and begin a meditation session. Take a  few  moments  to  breathe  deeply  and  center  yourself. Imagine sitting down at a comfortable table in a well-lit room, somewhere that you are happy and feel at peace.\n",
      "\n",
      "Then, invite your future self to come sit with you and talk. You can request that they are a certain age, but usually the age will just come to you when you see them.\n",
      "\n",
      "Specifically ask for the highest possible version of yourself to sit down. If you see anything scary at first, know that it is your fear of what could happen manifesting in your mind, not the truth of what will happen.\n",
      "\n",
      "Once you get past that, you can start receiving advice.\n",
      "\n",
      "STEP 2: NOTICE HOW YOUR FUTURE SELF LOOKS\n",
      "\n",
      "Aside from what you imagine this version of yourself telling you, pay attention to what they look like, how they behave, and what their facial expressions communicate.\n",
      "\n",
      "The  point  of  doing  future  self  work  is  so  that  you  can merge with this aspect of yourself. You want to clearly envision the most ideal version of yourself so that you know how your own life needs to grow, shift, and change.\n",
      "\n",
      "See what they wear, how they feel, what they do each day. These will be the keys to your own becoming.\n",
      "\n",
      "STEP 3: ASK FOR GUIDANCE\n",
      "\n",
      "If you go into this process with a laundry list of scary, huge questions for your future self to answer, you're probably going to end up bound by panic rather than being open to receiving powerful guidance.\n",
      "\n",
      "Instead,  keep  yourself  open  to  whatever  this  person wants to share with you. The messages should be positive,\n",
      "\n",
      "uplifting, affirming, and helpful. Even if they communicate something to you such as, you need to let go of this relationship, it should be done in a way that is so calming and reaffirming that you are confident and at peace with it.\n",
      "\n",
      "STEP 4: IMAGINE THEM HANDING YOU THE 'KEYS' TO YOUR NEW LIFE\n",
      "\n",
      "Another  powerful  exercise  using  your  future  self  is  to imagine yourself now sitting down with yourself 3, 5, or even 7 years ago. It has to be close enough that you can relate to this person but far enough that you've changed.\n",
      "\n",
      "Imagine sitting down in a space you used to frequent or inhabit. What you're going to do now is hand them the pieces of your current life and all of the information they will need to go from who they are then to who you are now.\n",
      "\n",
      "You can hand them your car keys, your email account for your job, your bank account, an outfit, or instructions on what to do in terms of career, relationships, or just day-today habits.\n",
      "\n",
      "Or you can imagine your future self giving you aspects of your life now. Imagine them handing you the keys to the home you live in, or your wedding band, or anything else that is a part of your highest possible future life.\n",
      "\n",
      "Remember, this process should make you feel calm, affirmed,  and  more  self-assured,  not  the  opposite.  Fear  is\n",
      "\n",
      "a hallucination, a trick of the mind and gut. Your future self can step in and remind you of all that is possible and empower you to live with certainty, clarity, and grace.\n",
      "\n",
      "RELEASING YOUR PAST INTO THE QUANTUM FIELD\n",
      "\n",
      "When something happens that scares you and then you do not ever get over that fear, you become traumatized.\n",
      "\n",
      "Trauma is the experience of disconnecting with a fundamental source of safety.  It  happens  most  severely  when our attachment to our primary caretakers is compromised. But there is truly an infinite number of ways the world can traumatize you, and to varying degrees.\n",
      "\n",
      "There are lots of theories about what trauma is and where it comes from. Some believe that it is passed down physically  through  DNA. 16 Others  argue  that  it  is  shared mentally and emotionally through learned patterns and observations. Most commonly, trauma is believed to be an interpersonal experience we have in which we were challenged and then lacked the skills and coping mechanisms to rise to it.\n",
      "\n",
      "No matter where it  came  from,  if  you  have  some  kind of lingering trauma, you will know, because you will feel it.  Y ou  will  feel  it  physically  in  your  body.  Y ou  will  feel anxiety,  tension,  fear,  terror,  sadness,  or  guilt.  It  will  be\n",
      "\n",
      "displaced. It will not have a clear, direct cause. You will overreact to certain things and even when a problem is solved, you will still panic. This is the mark of trauma.\n",
      "\n",
      "TRAUMA IS NOT IN YOUR HEAD. IT IS IN YOUR BODY.\n",
      "\n",
      "This  is  the  first  and  most  important  thing  you  need  to know  in  order  to  overcome  it:  Trauma  is  a  legitimate, physical  issue.  Y ou  store  those  emotions,  energies,  and patterns at a cellular level.\n",
      "\n",
      "Thankfully, we can use the ripples at the top of the water to trace back down to the problem at the bottom. You can begin to use your body to help you heal.\n",
      "\n",
      "FIRST, IDENTIFY WHAT CAUSED THE TRAUMATIC EXPERIENCE.\n",
      "\n",
      "You do this by feeling into yourself and noticing where you are tight or tense. Our bodies harden to protect us. When we have a broken leg, our fascia tightens like a natural cast so that we do not bend ourselves that way again. Similarly, when our hearts are broken, our emotions tighten so that we do not let ourselves feel again.\n",
      "\n",
      "Of course, eventually, we have to walk. We have to love. We have to experience life again. We have to slowly soften the pieces of us that are trying to protect us so that we can move forward.\n",
      "\n",
      "Healing trauma is not just a matter of psychoanalyzing it. It is a matter of literally working through it physically. The next time you feel yourself overreacting to some kind of stimulus, you will notice that your body is starting to tense  up  and  create  a  fight-or-flight  response. To  heal this,  you  have  to  force  yourself  to  take  deep,  soothing breaths until the part of your body that was once tense is relaxed again.\n",
      "\n",
      "You will need to self-soothe in different ways: meditating,  breathing,  drinking  enough  water,  getting  enough sleep, using aromatherapy, sound therapy, or whatever else works for you.\n",
      "\n",
      "You absolutely  must  work  to  take  your  brain  and  body physically out of panic/survival mode.\n",
      "\n",
      "SECOND, REINSTATE A SENSE OF SAFETY.\n",
      "\n",
      "You are traumatized because something scared you and you are convinced that it is still 'out to get you.' This is what happens when we don't face or overcome something difficult-we assume the threat lingers indefinitely.\n",
      "\n",
      "The  psychological  aspect  of  trauma  healing  is  that  you have to literally restore the connection that was severed in the exact same way that it was broken.\n",
      "\n",
      "If  you  are  traumatized  about  relationships,  you  need  to build healthy relationships. If you are traumatized about\n",
      "\n",
      "money, you need to get really good with money. If you are traumatized about traveling, you need to travel again.\n",
      "\n",
      "We do not find the resolution in avoiding these things forever. In fact, just underneath the fear we often find that they are the things we really want more than anything else.\n",
      "\n",
      "THIRD, STOP TAKING THOUGHTS AND FEELINGS AT FACE VALUE.\n",
      "\n",
      "Last, to overcome trauma, you have to stop engaging in psychic thinking. You have to stop pretending you are able to  predict  what  will  happen,  that  you  know  other  people's intentions, or that what you feel and think is absolute truth and reality.\n",
      "\n",
      "This kind of thinking is what takes a triggering feeling and  turns  it  into  a  defeating  spiral.  Y ou  take  one  scary thing and make it into a prediction for what the future will hold.\n",
      "\n",
      "You  are  not  an  oracle.  You  do  not  know  what's  next, though you are always capable of choosing what you do now.  Almost  always,  the  thing  you  are  most  panicked about is a thing you do not know is happening for sure. It is usually an assumption, a projection, a fear turned into a terrifying potential reality.\n",
      "\n",
      "You  might  think  that  trauma  is  something  that  other, more damaged people have, but that is not true. Everyone\n",
      "\n",
      "is traumatized in one way or another, but it is how we respond to it, how we ultimately grow and develop self-mastery from it, that determines the course of our lives.\n",
      "\n",
      "BECOMING THE MOST POWERFUL VERSION OF YOURSELF\n",
      "\n",
      "Are you being the most powerful version of yourself?\n",
      "\n",
      "If you had to pause to think about it, the answer is probably no.\n",
      "\n",
      "Everybody has different facets of their personality, and we act  on  them  based  on  the  context  we're  in.  It's  a  social adaptation tool: You are not the same person with your friends  as  you  are  with  your  parents.  Moving  through these easily is a sign of high psychological function.\n",
      "\n",
      "We are acquainted with the versions of ourselves that our current life requires. We know who we need to be at work, at home, or in love. But we are often unfamiliar with the person we need to be in order to move our lives forward.\n",
      "\n",
      "In  'inner  child  work,'  you  visualize  and  address  your younger selves, often down to a specific age, depending on which version of you was traumatized. You communicate with that inner child self, learn from them, protect them, or  give  them  the  guidance  that  they  needed  when  they were young.\n",
      "\n",
      "This proves to be profoundly healing for people, mostly because we do not evolve past our former selves; we simply grow upon them.\n",
      "\n",
      "However, this practice can work the opposite way as well. You can also visualize and connect with your future selfthe version of you that you are growing into, or the person you know you are meant to be.\n",
      "\n",
      "WHAT WOULD MY MOST POWERFUL SELF DO TODAY?\n",
      "\n",
      "The first step to becoming your most powerful self is to literally  envision  that  person.  Don't  take  yourself  out  of your current context, either. Begin to ask yourself: What would  the  most  powerful  version  of  me  do  right  now? What would they do with this day? How would they respond to this challenge? How would they move forward? How would they think? What would they feel?\n",
      "\n",
      "Your most powerful self needs to be the CEO of your life. It is the person making managerial decisions, governing  everything  else.  This  is  the  editor-in-chief,  the matriarch or patriarch. You are working for your most powerful self.\n",
      "\n",
      "Once you have a clearer image of what your most powerful self  is  like,  you  then  need  to  evaluate  what  habits, traits,  and behaviors are actively holding you back from fully embodying that person.\n",
      "\n",
      "BE AWARE OF YOUR WEAKNESSES\n",
      "\n",
      "Powerful people are not delusional. They do not believe they are perfect all the time at everything. This is not what makes them mentally strong. Instead, powerful people are very aware of their varying strengths and weaknesses.\n",
      "\n",
      "In business, powerful people will often outsource the tasks at which they are less skilled. In life, powerful people know where their limits are and what their triggers might be. This allows them to move through their lives with more ease and to give themselves the time and space needed to work on their faults.\n",
      "\n",
      "The ability to say to yourself: 'I know I struggle a lot with this, so I'm going to take my time and work on it' is one of the most powerful things you can do.\n",
      "\n",
      "BE WILLING TO BE DISLIKED\n",
      "\n",
      "Powerful people are not the ones who are most universally liked.\n",
      "\n",
      "They are also not the ones vying for others' approval, and that's the key.\n",
      "\n",
      "To be a truly powerful person, you must be willing to be disliked. This  is  not  to  say  that  you  behave  in  any  way that's malicious, but it is to say that no matter what you do, others are going to judge you. Powerful people know\n",
      "\n",
      "this. There is no path in life that you can take that will be free of resistance from others, and so it is important that you not only become okay with being disliked, but you anticipate it and act anyway.\n",
      "\n",
      "ACT ON PURPOSE\n",
      "\n",
      "Powerful and purposeful are one in the same.\n",
      "\n",
      "To be a truly powerful person, you need to have complete, unwavering conviction about what you want to create. To do this, you have to shift from a 'live for the moment' to a 'live for the legacy' mindset.\n",
      "\n",
      "Your purpose is a dynamic, evolving thing. Most of the time, it is at the intersection of what you are interested in, what you are good at, and what the world needs. Having a clear vision of what you want to create and accomplish is essential to finding your inner power. You will not feel strongly about a dream that is not part of who you most essentially are.\n",
      "\n",
      "DO YOUR INNER WORK\n",
      "\n",
      "This is  perhaps the most important and yet most commonly overlooked, because it is the least comfortable.\n",
      "\n",
      "To do your inner work means to evaluate why something triggered you, why something is upsetting you, what your life is trying to show you, and the ways you could grow\n",
      "\n",
      "from  these  experiences.  Truly  powerful  people  absorb what  has  happened  to  them  and  sort  of  metabolize  it. They use it as an opportunity to learn, to develop themselves. This type of inner mental and emotional work is non-negotiable if you want to be truly powerful.\n",
      "\n",
      "Powerful people are not the most aggressive; aggression is usually a self-defense mechanism. Powerful people are the ones most unfazed by small disturbances and most willing to fully process and work through the big ones.\n",
      "\n",
      "Of course, this is the foundational stuff. Next, you have to work on simplifying your life, talking less about your ambitions, and showing more of your accomplishments once they are  completed.  Gradually  make  health  improvements.  Assume  that  everyone,  and  everything, has something to teach you. Become comfortable with vulnerability, as vulnerability precedes almost every significant part of your life, and intentionally design your daily routine.\n",
      "\n",
      "Through everything, you must be thinking as your most powerful self would. If you learn to see the world and your life through that lens, you can create a life that reflects the intentions of that side of you. It already exists; you just need to know how to tap into it.\n",
      "\n",
      "LEARNING TO VALIDATE YOUR FEELINGS\n",
      "\n",
      "If we want to be effective in therapy, in politics, in relationships, in teaching kids, in talking someone down from the edge, in keeping peace, making friends, fostering connection,  and  making  progress,  there's  one  technique  we have to employ first.\n",
      "\n",
      "It's a little secret, and it's one that requires very little effort.  But  it  disarms  people.  It  opens  them,  makes  them receptive,  willing  to  listen  and  adapt.  It  is  healing,  it  is mind-altering, but most importantly, it is the first step to progress. It is emotional validation.\n",
      "\n",
      "Validating someone's feelings doesn't mean you agree with them. It doesn't mean you concede that they are correct. It  doesn't  mean that those feelings are the healthiest; it doesn't mean they are informed by logic. Validating feelings does not mean you make them more true; it means you remind someone that it is human to feel things they don't always understand.\n",
      "\n",
      "How often do we just need a partner to stop trying to strategize and just say, that must really suck ?\n",
      "\n",
      "How much of a weight is lifted off our shoulders when we think: Yes, I really am stressed right now, and I deserve to be ?\n",
      "\n",
      "How light do we feel when we see another person's story\n",
      "\n",
      "splayed out across a screen, one that we can relate to, and understand, no matter how devastating it is?\n",
      "\n",
      "How much better do we feel when we simply allow ourselves to be aggrieved and pissed-off and irrationally mad?\n",
      "\n",
      "When  we  let  ourselves  have  it-the  feeling,  that  issomething incredible happens. We no longer have to take it out on other people, because we are no longer relying on their validation to get us through it.\n",
      "\n",
      "We can be aggrieved and pissed and mad and do our own processing without hurting anyone else.\n",
      "\n",
      "When people are crying out or acting out in their lives, they aren't just asking for help. They are most often just asking for someone to affirm that it is okay to feel the way that they do. And if they have to inflate and exaggerate circumstances for you to truly feel the weight and impact that  they  do? They'll  do  it. They'll  do  whatever  it  takes to get someone else to say: I am so sorry for what you are going through. This is not because they are incompetent or dumb. It is because in a world that does not teach us how to adequately process our own feelings, we must often rely solely on our maladaptive coping mechanisms.\n",
      "\n",
      "When we cannot validate our own feelings, we go on a never-ending quest to try to force others to do it for us, but it never works. We never really get what we need.\n",
      "\n",
      "This  looks  like  needing  attention,  affirmation,  compliments. But it also looks like being dramatic, negative, and focusing disproportionately on what's wrong in our lives. When someone is complaining about something simpleand they seem to be doing it more than the given situation would call for-they aren't trying to get your help about a small issue. They are trying to have their feelings validated.\n",
      "\n",
      "This is also a common root of self-sabotaging behaviors. Sometimes, when we have deep wells of grief within us, we absolutely cannot allow ourselves to relax and enjoy our lives and relationships. We cannot just 'have fun,' because doing so feels like a betrayal. It feels offensive. We need to feel validated, but we don't even know why.\n",
      "\n",
      "WHY IS THIS EFFECTIVE?\n",
      "\n",
      "Think of your feelings like water running through ducts in your body. Your thoughts determine whether or not the ducts are clean. The cleanliness of the ducts determines the quality of the water.\n",
      "\n",
      "If you suddenly have a feeling that you dislike and don't expect-a sudden rush of water, let's  say-it's  common to  want  to  shut  that  valve  off  and  not  allow  it  to  pass. However, stopping the flow of water does not make the water go away. Instead, it begins to intensely pressurize and create serious damage to the parts of your body that are no longer receiving flow. This begins to have a ripple effect on your entire life.\n",
      "\n",
      "Sometimes,  the  water  disperses  itself  gradually.  Other times, it implodes and creates what we see on the surface as  a  complete  emotional  breakdown.  When  all  of  that water finally comes through and we grieve and cry and fall apart, we are going through a process of being reset. It is positive disintegration: We are gutted, but at the same time, feel better when it's over.\n",
      "\n",
      "All that happened in that implosion was that your feelings became validated when you gave yourself permission to feel them-because you had no other choice. This is what we do in therapy. This is what we do when we vent. This is  what  happens when we experience a catharsis. A sad movie that we kind of enjoy being sad about allows us to feel sad in a world that otherwise does not.\n",
      "\n",
      "But there's a healthier, easier way, which is learning how to process our feelings in real time.\n",
      "\n",
      "'Validating your feelings' sounds like a big term, but it really means one thing: It's just letting yourself have them.\n",
      "\n",
      "When you are healing past trauma, often a big component is allowing yourself to experience the full expression of an emotion. You have probably done this in the past. Think about the passing of a relative whom you loved but were not overly attached to. When you learned of their death, you were undoubtedly sad. But you didn't attend their funeral, cry for an hour, and then carry on with your life as though nothing happened.\n",
      "\n",
      "Instead, you probably experienced a bout of sadness then, and then maybe the next day, and then maybe a week later. The waves of grief  came  and  went  in  varying  intensity. When you didn't resist them, you cried and felt sad, or maybe took a nap, a hot bath, or a day off from work.\n",
      "\n",
      "And  then,  without  much  effort  from  you,  the  feeling passed, and you felt better.\n",
      "\n",
      "Once we have and acknowledge an emotion, it will often go  away  on  its  own.  If  there  is  no  course  of  action  to take-if all we really need to do is accept it-then we just have to let ourselves be there.\n",
      "\n",
      "The reason we don't do this more naturally is because obviously we can't burst into tears at our desks every time we feel bothered by something. Turning off the water valve is perfectly fine, as long as we can go home and let it out later. It is okay to control when and where we process, and in fact, it's better when we learn to do it in a more stable, safe space.\n",
      "\n",
      "This can look like taking a few minutes to 'junk journal' each day, spending time by ourselves where we can simply experience how we feel, without judgment, and without trying  to  change  them.  It  can  be  as  simple  as  allowing ourselves to cry before we fall asleep. We often think of that as a sign of weakness, when really, the ability to cry freely is a huge signal of mental and emotional strength. It's  when  we  can't  cry  about  what's  truly  broken  in  our lives that we have a big problem.\n",
      "\n",
      "Validating the way someone else feels is an exercise in radial  empathy.  It  is  starting  the  conversation  with: 'It  is okay to feel this way.' Because when we point out how wrong someone is to feel the way they do, they shut down. And they shut down because they feel shame. They already know it's not right to feel the way they do. If you start the conversation by heightening someone's defenses or making them panic and suppress even harder, you make the situation worse.\n",
      "\n",
      "But if you start with reminding them that anyone in their situation would probably feel similar to how they do right now, and that it is very possible that they can have strong, overwhelming emotions that don't necessarily mean their lives are completely ruined, and that it is okay to feel devastated when devastating things are before us, we lighten their load. We know this because when we stop resisting feeling sad and just let ourselves be sad, we realize that it will not last forever. We see that sometimes, the biggest problem isn't that we are devastated, but that in refusing to accept what is in front of us, we create so much more suffering than we would if we had just had a cry when we needed to have a really good cry.\n",
      "\n",
      "Validating other people teaches us how to validate ourselves. And when we learn how to validate ourselves, we become stronger. We see that our emotions are no longer threats, but informants. They show us what we care about, what  we  want  to  savor,  and  what  we  want  to  protect. They remind us that life is fleeting, and challenging, and\n",
      "\n",
      "gorgeous. When we are willing to accept the darkness, it is only then that we find the light.\n",
      "\n",
      "ADOPTING YOUR OWN PRINCIPLES\n",
      "\n",
      "If you feel lost, or as though you don't know where you want your life to go next, or worse, fear that everything you have built could come crashing down, you don't need more inspiration. You don't need more positive thinking.\n",
      "\n",
      "When  you  have  money  problems,  you  need  money principles.\n",
      "\n",
      "When you have relationship problems, you need relationship principles.\n",
      "\n",
      "When you have work problems, you need work principles.\n",
      "\n",
      "When you have life problems, you need life principles.\n",
      "\n",
      "More money does not solve money problems. Different relationships do not solve relationship problems. New work does not solve work problems. Y our future life will not solve your life problems.\n",
      "\n",
      "This  is  because  money  does  not  make  you  good  with money.  Love  does  not  make  you  love  yourself.  Relationships  don't  make  you  good  at  relationships.  Work doesn't make you good at your job or capable of work/ life balance.\n",
      "\n",
      "Problems  don't  inherently  make  you  a  stronger  person unless you change and adapt. The variable here is you. The common denominator is whether or not you shift your foundational perspective on the world and how you behave within it.\n",
      "\n",
      "Let's be very clear: Someone who makes $500K can be as seriously in debt and struggling as someone who makes $50K, and in fact, this happens more often than you would ever think. People who make less money are required to learn how to manage it better, and people who make more think they can eschew principles because of the quantity they are attaining.\n",
      "\n",
      "You can screw up your dream relationship just as quickly as you can a hook up, because the way you relate to others is an issue with you, not something that shifts depending on whether or not you meet the most perfect person who never triggers or annoys you and relates to you with unconditional positive regard.\n",
      "\n",
      "You can be just as unhappy in your ideal job, with your perfect hours, at your most desired pay rate, if you don't know how to ration your time, relate  to  others  in  your workplace, or move your career forward. People who are 'living their dreams' and 'following their passion' can be just as unhappy as people who are not.\n",
      "\n",
      "If you don't have principles, your life is not going to get better. Problems are only going to follow you and get bigger as your life does.\n",
      "\n",
      "The good things that happen to us in life are like a magnifier.  They  show  us  where  we  still  need  to  grow.  True love shows us to ourselves. Money shows us to ourselves. Dream jobs show us to ourselves. The good, the bad, the desperately-needs-to-change-right-now.\n",
      "\n",
      "If you don't have principles now, you won't have them later. If  you don't have the money principle of living beneath your  means,  you  won't  be  able  to  do  it  when  you  have more money. If you don't have the relationship principle of  not  relying  on  others  for  your  sense  of  self,  it  won't magically resolve itself when you meet the 'right person'; you will only sabotage that relationship, too.\n",
      "\n",
      "WHAT IS A PRINCIPLE?\n",
      "\n",
      "A principle is a fundamental truth that you can use to build the foundation of your life. A principle is not an opinion or a belief. A principle is a matter of cause and effect.\n",
      "\n",
      "Principles can be personal guidelines.\n",
      "\n",
      "Some  examples  of  money  principles  are  the  following: Keep overhead costs low, get out and stay out of debt, live beneath your means, or save for a rainy day.\n",
      "\n",
      "Many financial experts advocate prioritizing debt repayment as the beginning of financial health. This is because one day of accrued interest probably won't impact you that much. But 20 years will, to the tune of tens of thousands of dollars, if not more.\n",
      "\n",
      "In the same way, one day of gained interest in investments won't make a big difference. But 20 years will, to an even more significant margin.\n",
      "\n",
      "The point of having principles is that it shifts you from short-term survival to long-term thriving.\n",
      "\n",
      "Most things in our lives are governed by principles. Stephen Covey explains this well: Principles are a natural law like gravity. It's different than a value. Values are subjective; principles are objective. 17 'We control our actions, but the consequences that flow from those actions are controlled by principles,' he says.\n",
      "\n",
      "This means that if we are committed to the principle of eating  good  food  each  day,  we  will  inevitably  reap  the benefit of better or improved health. If we write a sentence each day for many, many years, we will inevitably write a larger piece of work. If we commit to paying off a portion of our debts each month, we will inevitably clear our balance. If we invest consistently and wisely, we will eventually see a return.\n",
      "\n",
      "Our lives are governed by principles, as Benjamin Hardy explains: 'Most people cram for tests while in college. But can you cram if you're a farmer? Can you forget to plant in the spring, slack-off all summer, and then work hard during the fall? Of course not. A farm is a natural system governed by principles.' 18\n",
      "\n",
      "So are you.\n",
      "\n",
      "'The law of the harvest is always in effect. What you plant, you must harvest. Furthermore, what you plant consistently over time eventually yields a compounded or exponential harvest. You often don't experience the consequences of your actions immediately, which can be deceiving. If you smoked one cigarette, you probably wouldn't get cancer. If you spent $10 on coffee just one day, it probably wouldn't affect your financial life. However, over time, these habits have drastic outcomes. It turns out that $10 daily over 50 years at 5% compounding interest becomes $816,000.' 19\n",
      "\n",
      "When you make an investment, you don't expect to see a  return  that  day.  In  the  same  way,  you  can  go  to  sleep feeling accomplished knowing that you chipped away at your future just by adhering to your principles.\n",
      "\n",
      "Little things, done repeatedly and over time, become the big things.\n",
      "\n",
      "WHY IS INSPIRATION INEFFECTIVE HERE?\n",
      "\n",
      "Inspiration can be misleading. Big dreams not backed by strategic plans are big flops waiting to happen.\n",
      "\n",
      "Inspiration  means  you  take  a  feeling  and  elaborate  on it.  Y ou  allow  your  mind to wander; you piece together pretty  pictures  and  create  an  image  of  how  you'd  like your life to feel.\n",
      "\n",
      "Principles are boring. They aren't inspiring. They are the laws of nature.\n",
      "\n",
      "Principles are not immediately gratifying.\n",
      "\n",
      "They do not make us feel better right away.\n",
      "\n",
      "That's why we often reach for inspiration but find it to be ineffective. This is because we get our minds and hearts set on a vague idea of what we think we want without ever really evaluating whether or not we want to engage in the daily work and effort it would take to get there.\n",
      "\n",
      "When  we  don't  pair  inspiration  with  the  principles  it takes to achieve those dreams, we become more lost and disappointed than ever before.\n",
      "\n",
      "HOW DO I START DEVELOPING MY OWN PRINCIPLES?\n",
      "\n",
      "Nobody is born with excellent principles; they are something that you learn.\n",
      "\n",
      "However, there are  many  different  principles  in  life,  and some may contradict one another. That's why it's important to adopt your own, ones that fit your goals and your life.\n",
      "\n",
      "BEGIN WITH THIS:\n",
      "\n",
      " · What  do  you  value?  What  do  you  genuinely  care about?\n",
      "\n",
      " · What feelings do you want to experience in your life?\n",
      "\n",
      " · What makes you uneasy or gives you anxiety?\n",
      "\n",
      "THE ANSWERS COULD BE SOMETHING LIKE THIS:\n",
      "\n",
      "I value relationships, and so by principle, I am going to prioritize  them  when  given  the  opportunity.  Alternatively,  by principle, I value honest and positive relationships, so I'm not going to be in dating limbo anymore; unless someone commits within a reasonable amount of time, I will regard their hesitation as a 'no.'\n",
      "\n",
      "Perhaps you value financial freedom, and so by principle, you are going to put your extra cash toward repaying debt or  building  savings  or  investments.  Perhaps  you  value travel and freedom, and so by principle, you are going to start working for yourself and always prioritize being able to work remotely or make your own schedule.\n",
      "\n",
      "When you are clear on what your principles are, you can build your life from a genuine, healthy place. You can start working toward goals that support what you do and do not want to experience, that will make you the calmest and happiest version of yourself.\n",
      "\n",
      "A good life is built from the inside out and is based on a foundation of self-conduct and prioritization. It's not as dreamy as a vision board, but it's a lot more effective.\n",
      "\n",
      "FINDING YOUR TRUE PURPOSE\n",
      "\n",
      "When you live in a world that is constantly telling you to  follow  your  heart,  trust  your  gut,  quit  your  day  job, and do what you love, it can be disheartening when you don't know where to start. When you start thinking that you don't know what to do with your life, what you really mean is that you don't yet know who you are.\n",
      "\n",
      "Finding  your  purpose  is  not  necessarily  about  realizing that you are destined to live in a monastery or devote your life to a singular vocation or goal. Your purpose is not one job, it is not one relationship, it is not even one career field. Your purpose is, first and foremost, just to be here. Your existence has shifted the world in a way that it is invisible to you. Without you, absolutely nothing would exist just as it is right now. This is important to understand, because if  you  start  believing  that  your  whole  purpose  in  being alive is just a specific job or role you take on at home, what happens when you quit or retire, or when the kids grow up and you're no longer a parent?\n",
      "\n",
      "You'll  sink  because  you  will  falsely  think  that  was  your only reason for being.\n",
      "\n",
      "Your  purpose  today  may  have  been  to  offer  someone  a smile when they were at their lowest. Your purpose this decade may be the job you're in. When you realize that you are always impacting the world around you, you start to realize something: The most important thing you can do to live\n",
      "\n",
      "meaningfully is to work on yourself. To consciously become the happiest, kindest, and most gracious version of yourself.\n",
      "\n",
      "Knowing your purpose also doesn't necessarily mean your life  will  henceforth  be  easy  or  that  you'll  always  know what to do. In fact, when you are genuinely on your own path, the future won't be clear, because if it is, you're actually following someone else's blueprint.\n",
      "\n",
      "With all  of  that  said,  when  most  people  wonder  about their purpose, they are often referring to their life's work and their jobs. Your career is not nothing. It is how you will spend the majority of your day, every day, for the better part of your life. That's why figuring out how you can best serve the world through that makes the long days and difficult moments bearable.\n",
      "\n",
      "Your life purpose is the point at which your skills, interests, and the market intersect.\n",
      "\n",
      "You are the blueprint of your future. Everything that you are, everything that you have experienced, everything that you're good at, every circumstance you have found yourself in, everything that you're passionate about is not random; it's a reflection of who you are and a sign about what you are here to do.\n",
      "\n",
      "However, it's not as easy as it sounds to become self-aware. You may still be thinking that you're not sure what you're good at,  or  that  you're  even  more  passionate  about  one\n",
      "\n",
      "thing over another. That's okay because your purpose does not require you to be the best at something.\n",
      "\n",
      "It is not the thing at which you, and only you, can succeed more so than anyone else. It is the things that naturally call you, that effortlessly flow out of you, and that evoke specific emotions from you. You are here to work those out. You are here to transform them. Your ultimate purpose is to become the ideal version of yourself. Everything else flows from there.\n",
      "\n",
      "FIGURING OUT WHAT YOU WANT TO DO WITH YOUR LIFE\n",
      "\n",
      "Here are some questions you should ask yourself if you want to know what your purpose really is:\n",
      "\n",
      "WHAT, AND WHO, IS WORTH SUFFERING FOR?\n",
      "\n",
      "Even doing what you love for a living doesn't mean every day will be easy. Everything comes with its own set of challenges, so the question is really: What are you willing to work for? What are you willing to be uncomfortable for?\n",
      "\n",
      "CLOSE  YOUR  EYES  AND  IMAGINE  THE  BEST  VERSION  OF YOURSELF. WHAT IS THAT PERSON LIKE?\n",
      "\n",
      "The  best  possible  version  of  yourself-the  most  loving, kind, productive, and self-aware version-is who you really\n",
      "\n",
      "are. Everything else is the byproduct of coping mechanisms you've developed and picked up from other people.\n",
      "\n",
      "I F SOCIAL  MEDIA  DIDN'T  EXIST,  WHAT  WOULD  YOU  DO WITH YOUR LIFE?\n",
      "\n",
      "If you knew that you wouldn't be able to show off, impress, or even share what it was you chose to do with your life, how would it change your ambitions? This differentiates what you are doing because you want to do it from what you are doing for the sake of how it looks to other people.\n",
      "\n",
      "WHAT COMES MOST NATURALLY TO YOU?\n",
      "\n",
      "What  you  are  most  naturally  good  at  is  the  path  you should follow first, because it's the path on which you will most effortlessly thrive.\n",
      "\n",
      "WHAT WOULD YOUR IDEAL DAILY ROUTINE LOOK LIKE?\n",
      "\n",
      "Forget about the elevator speech. Forget about having a  fancy  title  or  impressing  people  on  LinkedIn. Think about what you want to do day-in and day-out. A lot of people get into jobs they think will make them happy but realize they only liked the idea of them and not the day-to-day reality.\n",
      "\n",
      "WHAT DO YOU WANT YOUR LEGACY TO BE?\n",
      "\n",
      "Instead  of  worrying  about  the  virtues  on  your  résumé,\n",
      "\n",
      "focus on the virtues of your eulogy. Who do you want to be remembered as? What do you want to be known for?\n",
      "\n",
      "Though it's lovely to reflect on all of the virtues and talents of your life, here is an even more important part of finding your purpose: It is often found through pain. Most people come into awareness of their purpose not because they are effortlessly clear on what their talents are and how they can best utilize them, but because at some point, they find themselves lost, depleted, exhausted, and with their backs against the wall.\n",
      "\n",
      "In  experiencing  hardship  and  challenge,  we  begin  to realize  what really matters to us. It sparks a flame that, when kindled through action and commitment, becomes a transformative fire.\n",
      "\n",
      "If you listen to the stories of many of the most successful people in the world, they often begin with unimaginable hardship. In the face of the most unlikely situations, these people are forced into action. Comfort and complacency is not an option. They realize they must become the heroes of their own lives and the creators of their own futures.\n",
      "\n",
      "At the end of your life, your purpose will be defined not by how you struggled, what circumstances you were in, or what you were supposed to do, but how you responded in the face of adversity, who you were to the people in your life,  and  what  you  did  each  day  that  slowly,  in  its  own unique way, changed the course of humanity.\n",
      "\n",
      "CHAPTER 7\n",
      "\n",
      "FROM SELF-SABOTAGE TO SELF-MASTERY\n",
      "\n",
      "MOVING  FROM  SELF-SABOTAGE to  self-mastery  sounds like an extraordinary transformation, when in reality it is the natural course of coming to understand that you were responsible for holding your life back, and so you are also capable of moving it forward.\n",
      "\n",
      "CONTROLLING YOUR EMOTIONS VS. SUPPRESSING THEM\n",
      "\n",
      "The  Buddhists  believe  that  controlling  the  mind  is  the path to enlightenment. 20 Enlightenment, by which they mean, spontaneous and true happiness.\n",
      "\n",
      "The idea is simple in theory and complex in practice: By both exploring our understanding of the mind and training it to behave in a certain way, we sort of purify ourselves to experience the essential nature of what we are, which is, as they believe, joy.\n",
      "\n",
      "If you've ever sat in a meditation class, you'll know that the first principle of mind control is the opposite of what you'd think: It's about letting go.\n",
      "\n",
      "To truly master the mind, the Buddhists practice non-attachment, in which they sit placidly, breathe steadily, and allow thoughts to rise up, cohere, and then pass.\n",
      "\n",
      "Their approach is that controlling the mind is actually a matter of surrendering to the mind, allowing it to behave as it pleases while regulating their reaction to it.\n",
      "\n",
      "HOW DO YOU KNOW IF YOU'RE SUPPRESSING YOUR EMOTIONS OR CONTROLLING THEM?\n",
      "\n",
      "Emotional suppression is a regulation strategy that people use when they do not have adequate coping mechanisms for their feelings.\n",
      "\n",
      "The  pattern  is  often  this: The  person  denies  or  ignores their true reaction to a situation or experience, believes it will simply go away if they continue to disregard it, finds that their day-to-day lives are disrupted by a sense of unease, and one day, it all comes to a breaking point and they have an emotional outburst that they cannot control.\n",
      "\n",
      "Therapy generally aims to help patients no longer suppress how they feel. Instead, they are encouraged to recognize those emotions but choose how they respond to them.\n",
      "\n",
      "In  the  healing  process,  suppressing  and  controlling  can seem like a fine line.\n",
      "\n",
      "When someone cuts you off in traffic and you choose not to yell out your window, are you suppressing how you feel or controlling it? If your partner says yet another idiotic thing and you choose not to respond to it, are you suppressing how you feel or controlling it? If your coworker aggravates you consistently about a project and you choose not to say anything, are you suppressing how you feel or controlling it?\n",
      "\n",
      "SUPPRESSING IS UNCONSCIOUS; CONTROLLING IS CONSCIOUS.\n",
      "\n",
      "Suppressed emotions function similarly to unconscious biases.  One  such  type  of  bias  is  confirmation  bias, wherein your brain sorts through stimuli to bring your attention to facts or experiences that support what you already believe. Though you're not aware of the bias, it's still affecting you.\n",
      "\n",
      "On the  other  hand,  controlling  your  emotions  involves becoming more conscious of how you feel. You are aware that you are angry, sad, or aggrieved, but you are choosing what you do about it. It is not really that you are controlling your emotions, but your behavior.\n",
      "\n",
      "When you are suppressing your emotions, you don't know how  you  feel  and  your  behavior  seems  out  of  control.\n",
      "\n",
      "When you're controlling your emotions, you do know how you feel, and your behavior seems within your control.\n",
      "\n",
      "The answer is that when you're in traffic, or in an argument, or dealing with a difficult coworker, you should be aware of how you feel but still in control of how you respond. Emotions  are  temporary,  but  behaviors  are  permanent. You are always responsible for how you choose to act.\n",
      "\n",
      "We  often  think  that  the  measure  of  physical  strength is how much weight we can bear, how long we can run, or  how pronounced our muscles are. In reality, physical strength is a measure of how efficiently the body runs itself, how capable it is of effectively performing day-to-day tasks and occasional challenges when they arise.\n",
      "\n",
      "Mental health is the exact same way. It is not a measure of  how happy we seem, how perfect things are, or how unconditionally 'positive' we can be, but that we are able to move through day-to-day life and the occasional challenge with enough fluidity and reason that we aren't stifled or held back by ourselves.\n",
      "\n",
      "Amy Morin very famously disclosed some of the things that  mentally  strong  people  don't  do.  Identifying  their habits and behaviors is essential, but what if you just aren't there yet? If you want to become a mentally strong person, this is where you begin.\n",
      "\n",
      "LEARNING TO TRUST YOURSELF AGAIN\n",
      "\n",
      "Inner peace is the state of being connected to the deep internal knowing that everything is okay and always will be. The concept of finding one's 'inner peace' has been part of spiritual and metaphysical practices for centuries and has just recently become more mainstream with the development of popular psychology.\n",
      "\n",
      "Albert Camus once said: 'In the midst of winter, I found there was, within me, an invincible summer.'\n",
      "\n",
      "That sums up the entirety of what inner peace really is: the understanding  that  no  matter  what  is  happening  around you, there is a place of total knowing and calmness within you. Not only are you capable of returning to that place when you need to, but it's possible to live your entire life from there. The challenge is learning how to connect with it in the first place and rewire how you respond to your mind, which is always jumping from one worst-case scenario to the next.\n",
      "\n",
      "You  know  when  people  reference  knowing  something 'deep down?' They say things like: 'I'm worried, but deep down, I know it's going to be okay.' Or, 'I'm angry at him, but deep down, I know he loves me.' What do you think they are referencing? Where is deep down? They're talking about the place within them that has an infinite wisdom, a better understanding, and a more insightful perspective of what's going on. It isn't shaken by the stressors or fears that the mind wants to offer.\n",
      "\n",
      "So much of the process of finding inner peace is being able to get to that 'deep down' place where you know and feel that ultimately everything will be okay.\n",
      "\n",
      "There's another metaphor in meditation in which calmness is compared to steadying a lake or a large body of water.  Your  thoughts  and  actions  are  like  stones  in  the water: They  create  a  ripple  effect. The  point  of  meditation is to make yourself quiet enough so that the water comes back to its natural stillness. You don't have to force the water to be still. It does it on its own when you stop interrupting it.\n",
      "\n",
      "The same goes for finding inner peace. It's not so much something you have to create as it is something you have to return to.\n",
      "\n",
      "CREATING ALIGNED GOALS\n",
      "\n",
      "One of the most important parts of discovering your inner peace is that you trade in your desire for 'happiness.'\n",
      "\n",
      "Unfortunately, happiness is fickle. It can lead you to being attached to certain achievements, belongings, or specific circumstances. It can lead you to become dependent on other  people's  opinions  or  life  unfolding  in  a  particular way. When your goal is happiness, you will always find just  behind  it  a  lingering  sense  of  unhappiness-that's how  balance  and  duality  works.  Inner  peace,  however?\n",
      "\n",
      "That's the state in between the scales. When it's your goal, there's no way to lose.\n",
      "\n",
      "This  is  difficult  for  most  people,  and  often,  people  will continue to create stress, problems, and drama for themselves because their egos are still very much attached to thinking they need something external to make them feel good. This is the quintessential sign of someone who has not yet found their inner peace: They are searching, often frantically, for a sense of satisfaction, belonging, or worthiness outside of themselves.\n",
      "\n",
      "So really, it's not that happiness isn't a virtuous thing to which you should aspire, or that happiness isn't something you're ever allowed to feel. The reality is that inner peace is  the  true  happiness,  and  everything  else  is  just  a  false means of trying to convince yourself that you are 'okay.'\n",
      "\n",
      "Think about it this way: What do you typically imagine will  bring  you  happiness?  Money,  a  relationship,  a  promotion? What happens when you achieve those things? Consistently, throughout all of humankind, the answer is the same: You return to your baseline. This is because this kind of happiness is not real. It is only being completely at peace with whenever you are in any given day that you will find a genuine sense of wonder, presence, and joy.\n",
      "\n",
      "WHAT DRIVES US AWAY FROM INNER PEACE IN THE FIRST PLACE?\n",
      "\n",
      "With all this talk of how we have to 'come back' to our place of inner peace, it brings up the question of why we ever got disconnected from it in the first place. This is important because understanding why we lose it is fundamental to finding it again.\n",
      "\n",
      "When we grow up, we adapt to our environments. We adopt the beliefs and ideas of those around us. We alter our personalities so that we become safer; we believe the world can't hurt us. When we are children, we are more vulnerable than ever, and it's during this time that we pick up what can easily become lifelong coping mechanisms.\n",
      "\n",
      "If we are not instructed from a young age to connect with our  inner  sense  of  peace,  we  will  instinctively  begin  to trust the voice in our head. This is where we really get lost, because the thoughts that we have on any given day are largely the product of what the Buddhists would call the 'monkey mind,' or as a neurologist could explain it, the process of different receptors firing off and making associations with things that may or may not have anything to do with reality.\n",
      "\n",
      "When we begin to trust our thoughts, we let them inform our  feelings.  This  becomes  a  cycle  and  ultimately  traps most people who aren't aware that it's happening. They have a weird or scary thought, have a subsequent strong feeling, and the combination of the two makes the situation feel real when it's actually a misunderstanding of your neurological process.\n",
      "\n",
      "Of course, that doesn't mean that our thoughts are useless. It just means that they are not always reflective of reality and should be used as more of suggestions than anything else.\n",
      "\n",
      "WHY CAN'T PEOPLE FIND THEIR INNER PEACE EASILY?\n",
      "\n",
      "The answer is that they can; most people just aren't instructed  on  how  to.  But  beyond  that,  most  people  are actually too scared to go into their own feeling states, because their inner child is too traumatized.\n",
      "\n",
      "Everyone has an 'inner child'; it is the part of you that is most innocent and pure, and it never goes away. 21 Over time, it is your responsibility for you to learn how to parent this inner child, who will honestly be the one to push you away from your inner peace. They will be the one to throw a tantrum and tell you that everything is falling apart and that you're going to die and that you should just give up.\n",
      "\n",
      "In the same way that you wouldn't let a child run your waking life now, you can't always believe what your inner child is  afraid  of. Y ou  can,  however,  learn  to  work  with them, heal them, and make them feel safe...in the way any good parent would.\n",
      "\n",
      "Stephen Diamond explains it like this: 'To begin with, the  inner  child  is  real.  Not  literally.  Nor  physically.  But figuratively,  metaphorically  real.  It  is,  like  complexes  in general, a psychological or phenomenological reality, and\n",
      "\n",
      "an extraordinarily powerful one at that.' He argues that mental  disorders  and  destructive  behavior  patterns  are usually more or less related to unconscious parts of ourselves and were most often adopted in early life.\n",
      "\n",
      "FINDING YOUR OWN PEACE\n",
      "\n",
      "Finding inner peace isn't always so much about just sitting in the lotus position until wisdom becomes you; it's about making the uncomfortable decision to stay with your discomfort and to choose differently.\n",
      "\n",
      "As Gail Brenner explains: 'The inner war is perpetuated by resistance-that is, not wanting to feel the way we feel, not wanting people to do what they are doing, not wanting events to occur as they are occurring. Resistance wants to rewrite our personal histories and ensure that our plans materialize.' She argues that inner peace is the only kind that exists because nothing else is in our control. 22\n",
      "\n",
      "Another really amazing way to find your inner peace is to constantly remind yourself that your worries are a fabrication of your mind's need to identify potential threats for survival, and true happiness is being here in the moment. If that's hard to believe, make a list of the following:\n",
      "\n",
      " · Everything you have intensely worried about in your life.  Go  back  as  many  years  as  you  can,  and  be  as detailed as you can.\n",
      "\n",
      " · Every difficult situation you swore you would never get through or never get over.\n",
      "\n",
      " · Every time you have genuinely felt happy and at peace.\n",
      "\n",
      "Guaranteed, your responses to the first will bring a smile to your face in that they will remind you that you have worried constantly in your life, and yet they were mostly unfounded.\n",
      "\n",
      "Your response to the second will also be relieving, because it will show you just how much pain you thought was insurmountable in your life and how, in retrospect, you don't really ever think about those things anymore.\n",
      "\n",
      "Finally, your answers to the last question will remind you that  your  happiness  has  never  come  from  things  being perfect on the outside, but from you being present and open and connected to yourself and to the moment.\n",
      "\n",
      "DETACHING FROM WORRYING\n",
      "\n",
      "In the same way that it's easy to become addicted to substances  or  behaviors  that  allow  us  to  avoid  the  present moment, worrying is chief among the coping mechanisms people use to distract themselves from what really matters.\n",
      "\n",
      "Over  time,  you  convince  yourself  that  worrying  equals being safe. You think that by running worst-case scenarios\n",
      "\n",
      "through your head again and again, you will be better prepared for them. This is completely false. Not only are you draining your energy imagining situations that are very often completely manufactured, but when you are already hypersensitive to any one of these fears or ideas, you will actually  create  those  circumstances  simply  out  of  your avoidance or over-responsiveness to them.\n",
      "\n",
      "You have to remember that among all the things to know about the 'monkey mind,' your head wants to constantly seek out situations and experiences that will affirm itself. If you believe something will be good, it will be. It might not look exactly how you imagined, but the outcome will be exactly what you expect.\n",
      "\n",
      "Finding  your  inner  peace  is  just  connecting  to  your deepest wisdom. It's not something you have to create, justify, imagine, or reach for. It's always within you, it's always an option, and it's constantly a choice. You just have to make it.\n",
      "\n",
      "REMEMBERING THAT YOUR FEELINGS ARE NOT ALWAYS FACTS\n",
      "\n",
      "The most challenging part of all of this is arriving at a place where you can discern between which feelings are instinctive and informative and which are rooted in fear and ego.\n",
      "\n",
      "In a world that constantly tells you that your gut knows everything and that your feelings are real, and that if you reach in deep enough, you'll uncover a well of wisdom that can guide you... it can be really easy to assume that every feeling and idea we have is not only real but is somehow forecasting what's going to happen in the future.\n",
      "\n",
      "Your feelings aren't predictions. They are not fortune-telling mechanisms. They are only reflecting back to you what your current state of mind is. It's like having a nightmare: The monsters aren't real, but they could be metaphors for something you're worried about in your waking life.\n",
      "\n",
      "What holds so many people back from finding their inner peace is the fact that they can't tell the difference between which is correct: their fear or their peaceful feeling.\n",
      "\n",
      "Remember this: The feeling of peace is the one telling you the truth.\n",
      "\n",
      "Your feelings aren't here to tell you what's going to happen. They're  only  here  to  inform  you  of  where  you  are energetically and mentally and how you should respond to what happens around you. Fear is trying to scare you into staying small and keeping safe. It is a mortal, limited thing. The feeling of peace is trying to remind you that everything will be okay because it always is...and always will be, no matter what.\n",
      "\n",
      "BECOMING MENTALLY STRONG\n",
      "\n",
      "No matter who you are or what your purpose is in life, mental strength is going to be a key component in ensuring that you actualize all of the potential latent within you.\n",
      "\n",
      "Mental strength is not a fixed trait. It's not something we inherently have or don't. Ironically, it's not necessarily easier to have if you aren't faced with many challenges in life. In fact, it is often people who are in the most difficult circumstances that are forced to develop the highest degrees of mental strength.\n",
      "\n",
      "Being mentally strong is a process, and it is a practice.\n",
      "\n",
      "This is where you can begin.\n",
      "\n",
      "GET A PLAN, BECAUSE PLANS FIX PROBLEMS\n",
      "\n",
      "Mentally strong people are planners.\n",
      "\n",
      "They think ahead. They prepare. They do what's best for the long-term outcome.\n",
      "\n",
      "You might think that this disconnects them from the moment, but the opposite is true. Worrying disconnects you from  the  moment.  Overthinking  disconnects  you  from the moment. When you are consistently sidelined from\n",
      "\n",
      "your own anxiety, it's because you don't have a plan regarding the thing that's making you scared.\n",
      "\n",
      "Think about something you aren't scared of. Do you know why you aren't scared of it? Because you have a plan for what you'd do if it were to happen. So you're able to let go and be present.\n",
      "\n",
      "Whether  it's  becoming  financially  healthy,  improving your relationships, going to therapy, getting a new job, or pursuing a new career path or dream: If you don't have a plan, you're going to keep having a problem.\n",
      "\n",
      "HUMBLE YOURSELF, BECAUSE IT'S NOT ALL ABOUT YOU\n",
      "\n",
      "It  seems  like  everyone  is  thinking  about  you,  judging you, evaluating you, and determining your status in life. They aren't.\n",
      "\n",
      "Social media has likened us all to mini-celebrities in our own circles: We become convinced that everyone around us is disproportionately concerned with the minutiae of our lives.\n",
      "\n",
      "In a number of decades, you will be gone. Your home will be sold to a new family. Y our job will be taken by someone else. Y our kids will be adults. Y our work will be done. This isn't supposed to depress you; it is supposed to liberate you.\n",
      "\n",
      "Nobody is thinking about you in the way that you think they are thinking about you. They are mostly thinking of themselves. When you feel self-conscious for going grocery shopping in your sweatpants, please know that nobody cares and nobody is looking. When you feel anxious about your accomplishments or lack thereof, please know that for the most part, nobody cares and nobody is looking. This is true of absolutely everything in life.\n",
      "\n",
      "Nobody is evaluating you the way you are evaluating you. They  mostly  take  you  at  face  value.  Stop  thinking  that you're the sun that everyone revolves around. This world is not all about you. Your life isn't even all about you. The more you can put aside your spotlight complex, the more you're going to be able to relax.\n",
      "\n",
      "ASK FOR HELP, BECAUSE YOU'RE NOT SUPPOSED TO KNOW EVERYTHING\n",
      "\n",
      "We live in a specialized society.\n",
      "\n",
      "People go to school; they apprentice and train to become very skilled at one task. They then market and sell this task in exchange for purchasing other people's expertise.\n",
      "\n",
      "You are not supposed to know everything.\n",
      "\n",
      "You are not supposed to be a financial expert; that's why you can hire one to do your taxes or advise your investing.\n",
      "\n",
      "You are not supposed to be a master chef; that's why you can buy a cookbook or ask your mother for help. You are not supposed to be a world-class trainer; that's why you can book an appointment with one and learn. You are not supposed to understand the complexities of mental health and neuropsychology; that's why you can visit with a psychotherapist and learn how to get better.\n",
      "\n",
      "You are not supposed to know everything. You're not supposed to be good at everything. This is why you have people whom you can hire or learn from. Cut yourself some slack, and focus on what you are proficient in. Outsource everything else.\n",
      "\n",
      "KNOW WHAT YOU DON'T KNOW, AND STOP FALSE DICHOTOMOUS THINKING\n",
      "\n",
      "The  main  reason  that  people  sustain  anxiety  is  due  to long-term  thinking  in  either/ors,  otherwise  known  as false dichotomies.\n",
      "\n",
      "This is a cognitive distortion in which you eschew an entire field of possibilities in favor of one or two polarized outcomes, neither of which are likely or reasonable.\n",
      "\n",
      "If I lose my job, I am a failure. False.\n",
      "\n",
      "If this relationship ends, I'll never find love again. False.\n",
      "\n",
      "If this scary thing happens, I won't be able to go on. False.\n",
      "\n",
      "Anxiety is caused by logic lapses, where there's a breach in your reasoning skills. You jump from one event to an unlikely conclusion, and because it makes you feel something strongly, you assume it to be true. Ultimately, you start  thinking  in  dichotomies,  which  are  not  only  ineffective, but also spook you so much that you are rendered incapable of actually handling your life.\n",
      "\n",
      "STOP TRYING TO BE PSYCHIC, BECAUSE THIS IS A COGNITIVE DISTORTION\n",
      "\n",
      "Given that our most fundamental human fear is of the unknown, it makes sense that we go through mental gymnastics to try to predict certain outcomes in our lives.\n",
      "\n",
      "However, psychic thinking, or the idea that your feelings are premonitions, that you can 'just know' what the future will hold, or that your fate is somewhere set in stone for you, makes you mentally weak. It places you in the passenger's seat when you need to be behind the wheel.\n",
      "\n",
      "When you are engaging in psychic thinking, you're extrapolating.  You're  taking  a  single  feeling  or  experience and making a long-term prediction about your life. It is not only false; it often becomes a self-fulfilling prophecy.\n",
      "\n",
      "Stop trying to predict what you can't know, and start putting your energy toward building what you can. You and your life will be better for it.\n",
      "\n",
      "TAKE RESPONSIBILITY FOR YOUR OUTCOMES-YES, ALL OF THEM\n",
      "\n",
      "In the grand scheme of your life, the outcomes that really matter are the ones that are almost completely within your control. It's easier and less scary to pretend as though you are simply a cog in the wheel, but you're not.\n",
      "\n",
      "If  you  actually  put  your  energy  toward  learning  to  be productive, taking care of your health and wellness, improving your relationships and self-awareness, you'd have a  radically  different  life  experience.  Every  single  one  of those things is within your ability to change or at least influence greatly.\n",
      "\n",
      "There are some things in life that are outside of your control. If you focus on them, you will miss something really important: the majority of your life is the direct result of your actions, behaviors, and choices.\n",
      "\n",
      "LEARN HOW TO FEEL BETTER BY PROCESSING COMPLEX EMOTIONS\n",
      "\n",
      "You are not supposed to feel happy all of the time. Trying to feel happy all of the time is not the solution; it's the problem.\n",
      "\n",
      "Instead  of  the  ability  to  sustain  positivity  at  all  times, mental strength requires that you develop the ability to\n",
      "\n",
      "process  complex  emotions  such  as  grief,  rage,  sadness, anxiety, or fear.\n",
      "\n",
      "When you do not know how to allow these feelings to pass through you, how to make sense of them, learn from them, or simply just allow them, you get stuck on them. You bury them, and then everything around you becomes a trigger that threatens to unleash the floodgates.\n",
      "\n",
      "You might think it's about keeping a stiff upper lip, but it's not. It's about crying when life is sad, being angry in the face of injustice, and being determined to create a solution when a problem arises. That responsiveness, instead of reactiveness, defines mental strength.\n",
      "\n",
      "FORGET WHAT HAPPENED AND FOCUS ON HOW YOU WILL MAKE IT RIGHT\n",
      "\n",
      "Reflect on what went wrong, learn from what went wrong, and figure out how you're either going to make up for it or change the outcome in the future.\n",
      "\n",
      "Then let it go.\n",
      "\n",
      "The only time you're going to really hold onto the past is when you haven't fully learned from the past. When you have, you can apply those lessons to the present moment and create what you wanted to experience then.\n",
      "\n",
      "Focusing on what happened disproportionately to what's happening now, or what you want to happen in the future, is what keeps you completely stuck. If you feel as though you truly failed yourself in some profound way, it becomes even more crucial that you move on and create the experience you desire now.\n",
      "\n",
      "Your life is not over. You did not fail indefinitely, but you will if you never let go and try again.\n",
      "\n",
      "TALK IT OUT, BECAUSE THINGS ARE OFTEN MORE COMPLICATED IN YOUR HEAD\n",
      "\n",
      "If  you  feel  really  tangled  up  in  your  thoughts,  feelings, and fears, talk to someone. Perhaps a mental-health professional or a trusted friend. If nobody is around, talk to yourself. Talk out your ideas as though you were speaking to someone else in front of you.\n",
      "\n",
      "Sometimes, we need an objective third party to help us sort through complicated parts of life. Keeping it all buried in your head and heart often makes it worse. Letting it out into the open tends to simplify the problem, release the emotion, and help you move on.\n",
      "\n",
      "TAKE YOUR TIME, BECAUSE YOU DON'T NEED TO FIGURE EVERYTHING OUT RIGHT NOW\n",
      "\n",
      "Growth usually isn't a sweeping thing. It happens incrementally. It occurs in tiny bursts and small steps. This is because when we are growing, we are actually expanding and restructuring our comfort zones. We are readjusting to a new way of life, and if we shock our systems with too much change too fast, we often revert back to what we knew.\n",
      "\n",
      "The most effective and healthy way to change your life is slowly. If you need instant gratification, make the goal the tiny step you take each day. Over time, momentum will build, and you'll realize that you're miles from where you started.\n",
      "\n",
      "TAKE TRIGGERS AS SIGNALS, BECAUSE YOUR WOUNDS NEED YOUR ATTENTION\n",
      "\n",
      "Triggers are not random; they are showing you where you are either most wounded or primed for growth.\n",
      "\n",
      "If  we can see these triggers as signals that are trying to help us put our attention toward some part of our lives that needs healing, health, and progress, we can begin to see them as helpful instead of hurtful.\n",
      "\n",
      "You cannot ignore your problems. You cannot disregard your wounds. These are issues that you will need to unpack, process, learn from, and adapt your behavior accordingly. This won't only make you mentally stronger; it will also give you a better quality of life overall.\n",
      "\n",
      "HONOR YOUR DISCOMFORT, BECAUSE IT'S TRYING TO TELL YOU SOMETHING\n",
      "\n",
      "The greatest gift that life will hand you is discomfort.\n",
      "\n",
      "Discomfort is not trying to punish you! It is just trying to show you where you are capable of more, deserving of better, able to change, or meant for greater than you have right now. In almost every case, it is simply informing you that there is more out there for you, and it is pushing you to go pursue it.\n",
      "\n",
      "Instead of trying to pacify this discomfort, mental strength requires that you listen, learn, and begin to change your course.\n",
      "\n",
      "If you can begin to see your life as a feedback mechanism that is reflecting who you are with the ultimate goal to help you live better and more fully, all of a sudden you realize that it was never the world standing in your way, but your own mind.\n",
      "\n",
      "HOW TO TRULY ENJOY YOUR LIFE\n",
      "\n",
      "If you were to ask, many people would undoubtedly agree that they believe the purpose of life is to enjoy it. However, so many people struggle to be present and actually experience their lives as they are. The culprits are varied and can include everything from unrealistic expectations\n",
      "\n",
      "to trying too hard to feel good. (It is, after all, something you have to allow.)\n",
      "\n",
      "When you're struggling, the most insulting and difficult thing that someone can tell you is to 'just relax' or 'just enjoy yourself.' When you're in survival mode, the last thing you can possibly think about is just sitting back and rolling with the punches. This is the most important part of learning how to enjoy your life again: When you're  in  a  place  of  trauma  and  pain,  you  can't  try  to force yourself to be happy. First, you have to step back into neutral.\n",
      "\n",
      "When you're struggling and you try to make yourself feel good, you are actually intensifying the polarity of your feelings. You are shoving the 'bad' feeling down in place of  trying  to  feel  something  different.  Ironically,  many people who struggle emotionally are, at their core, people who actually just have a greater desire to enjoy life.\n",
      "\n",
      "STOP TRYING TO BE HAPPY\n",
      "\n",
      "Happiness is not something you can chase. It is something you have to allow. This likely will come as a surprise to many people, as the world is so adamant about everything from positive psychology to motivational Pinterest boards.  But  happiness  is  not  something  you  can  coach yourself into.\n",
      "\n",
      "Happiness is your natural state. That means you will return to it on your own if you allow the other feelings you want to experience to come up, be felt, be processed, and not resisted. The less you resist your unhappiness, the happier you will be. It is often just trying too hard to feel one certain way that sets us up for failure.\n",
      "\n",
      "ARRIVE INTO THE PRESENT\n",
      "\n",
      "There's a saying that if you're anxious, it's because you're living  in  the  future,  and  if  you're  depressed,  it's  because you're living in the past. When you're living in the present moment, you realize that both the past and future are just current illusions in the infinite, eternal 'now' and that they are actually  ways  in  which  you  can  avoid  being  in your body.\n",
      "\n",
      "The only place to find happiness is in the present because that's the only place it truly exists. Trying to find happiness by focusing on what could or might happen in the future is actually a process of disassociation. Practice arriving into today by focusing on taking life one day at a time and doing the most with what you have in front of you currently.\n",
      "\n",
      "There's a fine balance between living for the moment and taking care of your future self.\n",
      "\n",
      "STOP TRYING TO ASSERT DOMINANCE\n",
      "\n",
      "In  his  book  on  Hygge,  the  Danish  art  of  coziness  and well-being that many attribute to the nation's staggeringly high happiness rates, Meik Wiking explains that connecting with others is not just spending time with them; it has to do with not trying to dominate, impress, or create an emotional reaction in someone. 23 You find a lot more happiness by not trying to prove yourself.\n",
      "\n",
      "People who want and need to assert their dominance in relationships are the ones who are always in arguments over  hypothetical  things,  creating  drama  at  important holidays or events, or otherwise finding that the very people they are supposed to love and cherish most receive the worst of their behavior.\n",
      "\n",
      "In order to find greater happiness, you need to see yourself as an equal to those around you. When you view yourself in a position to constantly learn from all those you know, you are no longer compensating for fearing you are 'beneath' them.\n",
      "\n",
      "LEAN INTO THE LITTLE JOYS WHEN YOU FIND THEM\n",
      "\n",
      "When we think of trying to 'enjoy' life, it's common for our  minds  to  jump  to  trying  to  achieve  things  that  are huge, overwhelming highs. We think that being happy is only what happens when we're on vacation or just landed a huge bonus.\n",
      "\n",
      "This, however, is actually the opposite of happiness, because  it's  conditional.  True  happiness  is  embracing  the little joys in life: the sunrise on a warm summer morning, your cup of coffee, or an amazing book. It is being grateful not only for when big things happen, but also for the small satisfactions that you can find every day.\n",
      "\n",
      "Most people severely overthink happiness. They assume their lives have to be in perfect working order for them to experience real joy. This is not so. Real joy is finding happiness where you are and how you are.\n",
      "\n",
      "NURTURE POSITIVE RELATIONSHIPS WHEN YOU HAVE THEM\n",
      "\n",
      "Regardless of whether or not you are introverted or extroverted, the quality of your relationships determines the quality  of  your  life  experiences.  Tons  of  research  backs this up: We become most like the people we spend time with, and our happiness is directly correlated with not the quantity of relationships we have, but the quality of each of them; being lonely is as much of a risk to your health as smoking. 24\n",
      "\n",
      "However, what most people interpret this to mean is that they should just make friends where they can find them and be close to their biological family, even if they dislike them. That's totally missing the point. Happiness is not contingent upon you forcing relationships you don't want to be in. It is, however, contingent upon you building and\n",
      "\n",
      "fostering  relationships  with  people  you  really  like  and who add value to your life.\n",
      "\n",
      "When you meet someone with whom you really have a connection, go out of your way to make sure you see that person and keep your friendship healthy.\n",
      "\n",
      "LEARN SOMETHING NEW AS OFTEN AS YOU CAN\n",
      "\n",
      "When you approach life as though you already know all there is to know, you are actually closing yourself off to potentially having new and better experiences. If you assume that you know what will happen when you try something new,  or  if  you  think  you  know  what  places  you  haven't been to would be like...you might just want to leave some room to surprise yourself.\n",
      "\n",
      "Think of life as something you can constantly learn from. Your pain teaches you what does not feel good and what you should not continue to do. Your joy teaches you what is in alignment. Everything can be your teacher, and the more you allow your life experiences to shift and change you, the better you (and they) will become.\n",
      "\n",
      "SEE CHALLENGING TIMES AS OPPORTUNITIES FOR TRANSFORMATION\n",
      "\n",
      "Happy  people  aren't  joyful  all  the  time,  and  this  is  an\n",
      "\n",
      "important distinction to make. In fact, genuinely happy people are more at peace than they are ecstatic about everything they experience.\n",
      "\n",
      "This is because happy people are inherently coachable and changeable. They  are  not  stuck  in  their  ways. They  understand that life requires growth, and when that growth stagnates, discomfort begins to arise.\n",
      "\n",
      "The true nature of life is constant movement and constant evolution.  If  you  do  not  keep  up  with  that,  life  will  all but force you to change as it becomes less and less comfortable to stay where you are. You cannot avoid all pain, but you can absolutely avoid a lot of suffering by staying focused on your internal growth.\n",
      "\n",
      "BE AWARE OF WHAT YOU GIVE YOUR ENERGY TO\n",
      "\n",
      "Sure, most people realize that if they work a job they dislike or stay in relationships they despise for the majority of their lives they aren't going to feel great about it. What many don't realize, however, is that there are far more significant things that we constantly offer our energy to that create the quality of our lives.\n",
      "\n",
      "Those disliked jobs and stale relationships aren't problems, they are symptoms, and at the root of all of it is where you allow your mind to run. When you give your energy to certain thoughts, they gain life. There's a saying that the\n",
      "\n",
      "wolf that wins is the one that you feed, and when it comes to the quality of your life, you need to be extremely careful of what you allow yourself to think. It will soon become what you feel, then what you believe, and then how you behave, and sure enough, the way you live.\n",
      "\n",
      "SCHEDULE TIME TO DO NOTHING\n",
      "\n",
      "Happiness is an active pursuit as much as it is a passive one. Though to feel fulfilled each day is absolutely a conscious choice (it isn't going to happen by accident, FYI) the irony of really feeling good is that it's not something you can force; it's something you have to allow.\n",
      "\n",
      "Happiness is refusing to fill your schedule to the absolute brim so you can wring the most you possibly can out of every second of your life. It is also taking time to embrace the mundanity of everyday moments. It's sitting back and reading  a  book,  talking  over  dinner  with  someone  you love,  or  just  enjoying  the  small  things  each  day. Taking this time won't happen on its own; you have to plan for it.\n",
      "\n",
      "SCHEDULE TIME TO PLAY\n",
      "\n",
      "When we were kids, all we did was imagine and play. Our lives  were  our  canvases,  and  we  inherently  understood that we could make believe absolutely anything and spend the day living it out.\n",
      "\n",
      "The same is true in adulthood, but over the course of a few  decades,  the  world  tends  to  have  a  way  of  beating the magic out of you. If you really want to enjoy life, you have to make time to do what you loved when you were young. Paint, play in the sand, play games you love, and be creative for the sake of it.\n",
      "\n",
      "If  that  all  sounds  childish,  good.  It  means  you're  ready to reconcile with your inner child who is and always has been there all along. Enjoying life is living it out in both the simplest and most transformative ways possible. Part of that is simply letting yourself show up and be who and how you are.\n",
      "\n",
      "BECOMING A MASTER OF YOURSELF\n",
      "\n",
      "When you get to the end of your life, you will begin to see your mountains for what they really were. Gifts.\n",
      "\n",
      "When you look back on your life, you won't remember the hardships. You'll  see  them  then  as  pivot  points,  growth opportunities, the days of awakening right before everything changed.\n",
      "\n",
      "To become a master of oneself is first to take radical and complete responsibility  for  your  life. This  includes  even that which is beyond your control. A true master knows that it is not what happens, but the way one responds, that determines the outcome.\n",
      "\n",
      "Not everybody gets there. Most people live barely realizing that they are creating most of the waves in their lives and that it is also their job to learn to ride them. Most people spend their days lost in a fog of their own thoughts and feelings, having little ability to sort through them.\n",
      "\n",
      "Mastery is to realize that we are equipped with the exact traits we need to overcome the mountains before us, and in fact, doing so is the ultimate calling of our lives. We are not only capable; we are destined.\n",
      "\n",
      "Mastery is to finally understand that the years of discomfort  you  endured  were  not  some  sort  of  purgatory  you had to just get through. They were your deepest inner self informing you that you are capable of more, deserving of better,  and  meant  to  transform  into  the  person  of  your dreams.\n",
      "\n",
      "You must claim it. You must create it. Your own healing process will create an invisible ripple effect on the collective. If we want to change the world, we change ourselves. If we want to change our lives, we change ourselves. If we want to scale the greatest mountains before us, we change how we arrive at the path.\n",
      "\n",
      "When you reach the peak of it all-whatever that may be for you-you will look back and know that every step was worth it. More than anything, you will be overwhelmingly grateful for the pain that led you to begin your journey, because really, it wasn't trying to hurt you as much as it\n",
      "\n",
      "was trying to show you that something was wrong. That something  was  the  risk  of  your  potential  remaining  untapped, your life spent with the wrong people, doing the wrong things, and wondering why you never felt quite right.\n",
      "\n",
      "Your life is just beginning.\n",
      "\n",
      "One day, the mountain that was in front of you will be so far behind you, it will barely be visible in the distance. But who you become in learning to climb it? That will stay with you forever.\n",
      "\n",
      "That is the point of the mountain.\n",
      "\n",
      "REFERENCES\n",
      "\n",
      " 1  Halifax, Joan. Standing at the Edge: Finding Freedom Where Fear &amp; Courage Meet . New York: Flatiron Books, 2018.\n",
      "\n",
      " 2  Hawking, Stephen. A Brief History of Time . New York: Penguin Random House, 1988.\n",
      "\n",
      " 3  Lachman, Gary. Jung the Mystic: The Esoteric Dimensions of Carl Jung's Life and Teachings. New York: Penguin Random House, 2010.\n",
      "\n",
      " 4  Hendricks, Gay. The Big Leap: Conquer Your Hidden Fear and Take Life to the Next Level. New York: HarperOne, 2009.\n",
      "\n",
      " 5  Swan, Teal. 'Find Your Subconscious Core Commitment,' tealswan.com.\n",
      "\n",
      " 6  Seymour, Tom. 'Vagus Nerve: Function, Stimulation, And Further Research.' Medical News Today, 2017.\n",
      "\n",
      " 7  Lieberman, Daniel Z.; Long, Michael E. The Molecule of More: How a Single Chemical in Your Brain Drives Love, Sex, and Creativity--And Will Determine the Fate of the Human Race. Dallas: BenBella Books, 2018.\n",
      "\n",
      " 8  Tracy, Brian. 'The Role Your Subconscious Mind Plays In Your Everyday Life,' briantracy.com, 2019.\n",
      "\n",
      " 9  Holiday, Ryan. 'Sorry, An Epiphany Isn't What's Going To Change Your Life.' ryanholiday.net, 2016.\n",
      "\n",
      " 10  Sims, Stacy T., Ph.D. 'The 3 Body Types: Explained.' Runner's World , 2016. https://www.runnersworld.com/health-injuries/a20818211/the-3-body-typesexplained\n",
      "\n",
      " 11  Taylor, Christa. 'Creativity and Mood Disorder: A Systematic Review and Meta-Analysis.' Perspectives on Psychological Science, 2017.\n",
      "\n",
      " 12  Cole, Adam. 'Does Your Body Really Refresh Itself Every 7 Years?' NPR, 2016. https://www.npr.org/sections/health-shots/2016/06/28/483732115/howold-is-your-body-really\n",
      "\n",
      " 13  Bremner, J. Douglas, MD. Traumatic Stress: Effects On The Brain. US National Library of Medicine National Institutes of Health, 2006.\n",
      "\n",
      " 14  Burton, Neel, MD. 'Our Hierarchy of Needs.' Psychology Today, 2012. https://www.psychologytoday.com/us/blog/hide-and-seek/201205/ourhierarchy-needs\n",
      "\n",
      " 15  Jacobson, Sheri. 'Inner Child Work: What Is It, And How Can You Benefit?' Harley Therapy, 2017. https://www.harleytherapy.co.uk/counselling/inner-childwork-can-benefit.htm\n",
      "\n",
      " 16  Henriques, Martha. 'Can the legacy of trauma be passed down the generations?' BBC, 2019. https://www.bbc.com/future/article/20190326-whatis-epigenetics\n",
      "\n",
      " 17  Covey, Stephen. The 7 Habits of Highly Effective People . Mango Media, Inc. 1989.\n",
      "\n",
      "18, 19  Hardy, Benjamin, Ph.D. 'Y ou Don't Control The Outcomes Of Y our Life, Principles Do.'\n",
      "\n",
      "LinkedIn, 2017. https://www.linkedin.com/pulse/you-dont-control-outcomesyour-life-principles-do-benjamin-hardy-3\n",
      "\n",
      " 20  Lopez, Donald S. 'Eightfold Path: Buddhism.' Britannica , undated. https:// www.britannica.com/topic/Eightfold-Path\n",
      "\n",
      " 21  Diamond, Stephen, Ph.D. 'Essential Secrets of Psychotherapy: The Inner Child.' Psychology Today, 2008. https://www.psychologytoday.com/us/blog/evildeeds/200806/essential-secrets-psychotherapy-the-inner-child\n",
      "\n",
      " 22  Brenner, Gail, Ph.D. 'The Warrior's Way to Inner Peace: What Is Inner Peace?' gailbrenner.com. https://gailbrenner.com/2009/11/the-warriors-way-1inner-peace/\n",
      "\n",
      " 23  Wiking, Miek. The Little Book of Hygge: Danish Secrets to Happy Living. New York: HarperCollins, 2016.\n",
      "\n",
      " 24  Pomeroy, Claire. 'Loneliness Is Harmful to Our Nation's Health.' Scientific American, 2019. https://blogs.scientificamerican.com/observations/loneliness-isharmful-to-our-nations-health/\n",
      "\n",
      "BRIANNA WIEST is an author living in Pennsylvania.\n",
      "\n",
      "briannawiest.com instagram.com/briannawiest facebook.com/briannawiestauthor pinterest.com/briannawiestwords\n",
      "\n",
      "101 Essays That Will Change The Way You Think\n",
      "\n",
      "This compilation of Wiest's published work features pieces on why you should pursue purpose over passion, embrace negative thinking, see the wisdom in daily routine, and become aware of the cognitive biases that are creating the way you see your life. Each will leave you thinking: this idea changed my life.\n",
      "\n",
      "'A must-read for everyone. This seminal book literally changed the way I think and ultimately changed my life.' HELEN - GOODREADS\n",
      "\n",
      "Salt Water:\n",
      "\n",
      "Poems On Healing &amp; Wholeness\n",
      "\n",
      "Salt Water is  Brianna  Wiest's  debut  poetry  book  which gracefully touches on the issues of self-awareness, wholeness and what it takes to reconcile with yourself.\n",
      "\n",
      "Brianna's prose artfully illustrates how healing helps us to actualize  our  latent  potential  and  bring  us  into  a  greater awe for the universe that we are so irrevocably connected to.\n",
      "\n",
      "I Am The Hero Of My Own Life\n",
      "\n",
      "This is the guide to getting out of your own way. I  Am the Hero of My Own Life is a guided journal that will will help you envision your ideal life and identify the unconscious  attachments  that  are  preventing  you  from  living it. Through a series of writing prompts and exercises, you will  sort  through  the  conflicting  thoughts,  feelings,  and fears that are preventing you from becoming the person you want and need to be.\n",
      "\n",
      "S A M P L E   E X E R C I S E S   I N C L U D E :\n",
      "\n",
      " - Tracing your judgments of other people back to your own personal insecurities.\n",
      "\n",
      " - Drafting a Venn diagram of your skills, your passions, and the world's needs.\n",
      "\n",
      " -  Outlining  exactly  the  type  of  partner  you  always dreamed of marrying, then strategizing how you're going to embody those traits yourself.\n",
      "\n",
      " - Envisioning what you'd think about and what you'd do all day if you were already healed, whole and happy.\n",
      "\n",
      "\n",
      "\n",
      "THOUGHT  CATALOG  BOOKS is  a  publishing  imprint  of Thought Catalog, a digital magazine for thoughtful storytelling.  Thought  Catalog  is  owned  by  The  Thought &amp;  Expression  Company,  an  independent  media  group based  in  Brooklyn,  NY,  which  also  owns  and  operates Shop  Catalog,  a  curated  shopping  experience  featuring our best-selling  books  and  one-of-a-kind products, and Collective World, a global creative community network. Founded in 2010,  we  are  committed  to  helping  people become better communicators and listeners to engender a more exciting, attentive, and imaginative world. As a publisher and media platform, we help creatives all over the world realize their artistic vision and share it in print and digital form with audiences across the globe.\n",
      "\n",
      "ThoughtCatalog.com | Thoughtful Storytelling\n",
      "\n",
      "ShopCatalog.com | Boutique Books + Curated Products\n",
      "\n",
      "Collective.world | Creative Community Network\n"
     ]
    }
   ],
   "source": [
    "source = \"../data/The Mountain Is You.pdf\"\n",
    "print(convert_pdf_to_text(source))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
